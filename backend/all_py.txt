-e # ===== FILE: ./core/config.py =====

"""
============================================================
 HIREX â€¢ core/config.py
 ------------------------------------------------------------
 Global configuration for backend constants, environment
 variables, and directory paths.

 Version : 2.1.2
 Author  : Sri Akash Kadali
============================================================
"""

from __future__ import annotations

import os
from pathlib import Path
from dotenv import load_dotenv


# ============================================================
# ðŸŒ Environment Setup
# ============================================================

_env_loaded = (
    load_dotenv(dotenv_path=Path(__file__).resolve().parents[2] / ".env")
    or load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / ".env")
    or load_dotenv()
)


def _clean_env(val: str | None, default: str = "") -> str:
    v = (val if val is not None else default)
    return str(v).strip().strip('"').strip("'")


def _getenv_clean(name: str, default: str = "") -> str:
    return _clean_env(os.getenv(name), default)


# ============================================================
# ðŸ“ Directory Structure (portable / any machine)
# ============================================================

BASE_DIR = Path(__file__).resolve().parents[2]
if not (BASE_DIR / "backend").exists():
    candidate = Path(__file__).resolve().parents[1]
    BASE_DIR = candidate if (candidate / "backend").exists() else Path.cwd()

BACKEND_DIR = BASE_DIR / "backend"
FRONTEND_DIR = BASE_DIR / "frontend"

DATA_DIR = BACKEND_DIR / "data"
CACHE_DIR = DATA_DIR / "cache"
TEMP_LATEX_DIR = CACHE_DIR / "latex_builds"
TEMPLATE_DIR = BACKEND_DIR / "templates"

# Kept for backwards compatibility; not used for PDFs anymore
OUTPUT_DIR = DATA_DIR / "output"
SAMPLES_DIR = DATA_DIR / "samples"
LOGS_DIR = DATA_DIR / "logs"
HISTORY_DIR = DATA_DIR / "history"
MASTERMINDS_DIR = DATA_DIR / "mastermind_sessions"
CONTEXTS_DIR = DATA_DIR / "contexts"

# --- Legacy sample dirs (kept to avoid breaking any imports) ---
SAMPLES_JOB_RESUMES_DIR = SAMPLES_DIR / "Job Resumes"
SAMPLES_COVER_LETTERS_DIR = SAMPLES_DIR / "Cover Letters"
SAMPLES_JOB_RESUME_HUMANIZED_DIR = SAMPLES_DIR / "Job Resume Humanized"

# --- NEW flat libraries (single folder per artifact type) ---
# Default to project-local backend/data/*; allow absolute overrides via .env
def _resolve_env_path(var_name: str, default_path: Path) -> Path:
    raw = _getenv_clean(var_name, "")
    if not raw:
        return default_path
    p = Path(os.path.expanduser(raw))
    if not p.is_absolute():
        p = BASE_DIR / p
    return p

OPTIMIZED_DIR = _resolve_env_path("OPTIMIZED_DIR", DATA_DIR / "Optimized")
HUMANIZED_DIR = _resolve_env_path("HUMANIZED_DIR", DATA_DIR / "Humanized")
COVER_LETTERS_DIR = _resolve_env_path("COVER_LETTERS_DIR", DATA_DIR / "Cover Letters")

for d in (
    DATA_DIR,
    CACHE_DIR,
    TEMP_LATEX_DIR,
    TEMPLATE_DIR,
    OUTPUT_DIR,
    SAMPLES_DIR,
    LOGS_DIR,
    HISTORY_DIR,
    MASTERMINDS_DIR,
    CONTEXTS_DIR,
    SAMPLES_JOB_RESUMES_DIR,
    SAMPLES_COVER_LETTERS_DIR,
    SAMPLES_JOB_RESUME_HUMANIZED_DIR,
    OPTIMIZED_DIR,
    HUMANIZED_DIR,
    COVER_LETTERS_DIR,
):
    d.mkdir(parents=True, exist_ok=True)

(LOGS_DIR / "events.jsonl").touch(exist_ok=True)


# ============================================================
# âš™ï¸ Core Settings
# ============================================================

APP_NAME = "HIREX"
APP_VERSION = "2.1.2"
DEBUG_MODE = _getenv_clean("DEBUG", "true").lower() == "true"

MAX_UPLOAD_MB = int(_getenv_clean("MAX_UPLOAD_MB", "5"))
ALLOWED_EXTENSIONS = {".tex", ".txt"}

DEFAULT_MODEL = _getenv_clean("DEFAULT_MODEL", "gpt-4o-mini")
API_BASE_URL = _getenv_clean("API_BASE_URL", "http://127.0.0.1:8000")

CANDIDATE_NAME = _getenv_clean("CANDIDATE_NAME", "Sri Akash Kadali")
APPLICANT_EMAIL = _getenv_clean("APPLICANT_EMAIL", "kadali18@umd.edu")
APPLICANT_PHONE = _getenv_clean("APPLICANT_PHONE", "+1 240-726-9356")
APPLICANT_CITYSTATE = _getenv_clean("APPLICANT_CITYSTATE", "College Park, MD")

# ============================================================
# ðŸ” Security & Secrets
# ============================================================

SECRET_KEY = _getenv_clean("HIREX_SECRET", "hirex-dev-secret")
JWT_ALGORITHM = "HS256"


# ============================================================
# ðŸ¤– API Keys (OpenAI + Humanize)
# ============================================================

OPENAI_API_KEY = _getenv_clean("OPENAI_API_KEY", "")

HUMANIZE_API_KEY = _clean_env(
    os.getenv("HUMANIZE_API_KEY")
    or os.getenv("HUMANIZE_TOKEN")
    or os.getenv("AI_HUMANIZE_KEY")
    or os.getenv("AIHUMANIZE_KEY")
    or ""
)

HUMANIZE_MAIL = _clean_env(
    os.getenv("HUMANIZE_MAIL")
    or os.getenv("AI_HUMANIZE_MAIL")
    or os.getenv("HUMANIZE_API_MAIL")
    or os.getenv("AIHUMANIZE_MAIL")
    or "kadali18@terpmail.umd.edu"
)

# âœ… Default to using Humanize everywhere (can be overridden in .env)
HUMANIZE_DEFAULT_ON = _getenv_clean("HUMANIZE_DEFAULT_ON", "true").lower() in {"1", "true", "yes", "on"}
# Default Humanize mode for the service (0=quality, 1=balance, 2=enhanced)
HUMANIZE_MODE_DEFAULT = _getenv_clean("HUMANIZE_MODE_DEFAULT", "balance").lower()
AIHUMANIZE_MODE_ID = {"quality": "0", "balance": "1", "enhanced": "2"}

HUMANIZE_CONFIG_OK = bool(HUMANIZE_API_KEY and HUMANIZE_MAIL)

if DEBUG_MODE:
    if not OPENAI_API_KEY:
        print("[HIREX] âš ï¸ OPENAI_API_KEY not found in environment.")
    if not HUMANIZE_API_KEY:
        print("[HIREX] âš ï¸ HUMANIZE_API_KEY not found. Humanize will fail unless fallback is enabled.")
    if not HUMANIZE_MAIL:
        print("[HIREX] âš ï¸ HUMANIZE_MAIL not set. It must match your AIHumanize registered email.")
    print(f"[HIREX] â„¹ï¸ HUMANIZE_DEFAULT_ON={HUMANIZE_DEFAULT_ON}, HUMANIZE_MODE_DEFAULT={HUMANIZE_MODE_DEFAULT}")


# ============================================================
# ðŸ§· Helpers
# ============================================================

def _ensure_file(path: Path, content: str) -> None:
    if not path.exists():
        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(content, encoding="utf-8")
        if DEBUG_MODE:
            print(f"[HIREX] ðŸ“„ Created default: {path}")


def _slugify(s: str) -> str:
    import re, unicodedata
    s = unicodedata.normalize("NFKD", s)
    s = re.sub(r"[^\w\s.-]", "", s)
    s = re.sub(r"\s+", "_", s.strip())
    return s or "unnamed"


def _candidate_prefix() -> str:
    return _slugify(CANDIDATE_NAME) or "Candidate"


# ============================================================
# ðŸ§  Feature Module Paths & Defaults
# ============================================================

BASE_COVERLETTER_PATH = _resolve_env_path("BASE_COVERLETTER_PATH", SAMPLES_DIR / "base_coverletter.tex")
BASE_RESUME_PATH = _resolve_env_path("BASE_RESUME_PATH", SAMPLES_DIR / "base_resume.tex")

LOG_PATH = LOGS_DIR / "events.jsonl"
HISTORY_PATH = HISTORY_DIR / "history.jsonl"

MASTERMINDS_PATH = MASTERMINDS_DIR
MASTERMINDS_MODEL = _getenv_clean("MASTERMINDS_MODEL", DEFAULT_MODEL)

# â— Keep fallback OFF so we don't leak [LOCAL-FALLBACK:*]. Fail hard if Humanize is down.
SUPERHUMAN_LOCAL_ENABLED = _getenv_clean("SUPERHUMAN_LOCAL_ENABLED", "false").lower() in {"1", "true", "yes", "on"}
SUPERHUMAN_MODEL = _getenv_clean("SUPERHUMAN_MODEL", DEFAULT_MODEL)
COVERLETTER_MODEL = _getenv_clean("COVERLETTER_MODEL", DEFAULT_MODEL)
TALK_SUMMARY_MODEL = _getenv_clean("TALK_SUMMARY_MODEL", "gpt-4o-mini")
TALK_ANSWER_MODEL = _getenv_clean("TALK_ANSWER_MODEL", DEFAULT_MODEL)


def is_humanize_enabled() -> bool:
    """
    Humanize is considered enabled when:
      â€¢ HUMANIZE_DEFAULT_ON is true (default),
      â€¢ we have valid key+mail,
      â€¢ and we are NOT using local fallback.
    """
    return HUMANIZE_DEFAULT_ON and HUMANIZE_CONFIG_OK and not SUPERHUMAN_LOCAL_ENABLED


def get_contexts_dir() -> Path:
    """Return the canonical contexts directory used by /api/context and friends."""
    return CONTEXTS_DIR


# ============================================================
# âœ¨ Portable Default Templates
# ============================================================

_DEFAULT_RESUME_TEX = r"""% HIREX default base_resume.tex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{enumitem}
\pagenumbering{gobble}
\begin{document}
\begin{center}
{\LARGE Your Name}\\
\vspace{2pt}
your.email@example.com \quad | \quad (123) 456-7890 \quad | \url{https://example.com}
\end{center}
\vspace{8pt}
\section*{Summary}
Results-oriented professional with experience in software engineering and AI.
\section*{Experience}
\textbf{Company} \hfill City, ST \\
\emph{Role} \hfill 2023--Present
\begin{itemize}[leftmargin=*]
    \item Bullet 1 describing impact.
    \item Bullet 2 describing impact.
\end{itemize}
\section*{Education}
\textbf{University}, Degree, Year
\end{document}
"""

_DEFAULT_COVERLETTER_TEX = r"""% HIREX default base_coverletter.tex (portable)
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\pagenumbering{gobble}
\begin{document}
\noindent Date: \today

\vspace{10pt}
Hiring Manager \\
Company \\
City, State

\vspace{10pt}
Dear Hiring Manager,

I am excited to apply for the role. My background in software and AI aligns with your needs.

%-----------BODY-START-----------
% (Body content will be injected here by HIREX)
%-----------BODY-END-------------

\vspace{12pt}
Sincerely, \\
Your Name
\end{document}
"""

_ensure_file(BASE_RESUME_PATH, _DEFAULT_RESUME_TEX)
_ensure_file(BASE_COVERLETTER_PATH, _DEFAULT_COVERLETTER_TEX)

DEFAULT_BASE_RESUME = BASE_RESUME_PATH

os.environ.setdefault("BASE_RESUME_PATH", str(BASE_RESUME_PATH))
os.environ.setdefault("BASE_COVERLETTER_PATH", str(BASE_COVERLETTER_PATH))


# ============================================================
# ðŸ’° Model Catalog & Pricing
# ============================================================

OPENAI_MODELS = [
    "gpt-5",
    "gpt-5-mini",
    "gpt-5-nano",
    "gpt-5-chat-latest",
    "gpt-5-thinking",
    "gpt-5-thinking-mini",
    "gpt-5-thinking-nano",
    "gpt-5-pro",
    "gpt-4o",
    "gpt-4o-mini",
    "o3",
    "o3-mini",
]

MODEL_ALIASES = {
    "GPT-5 (Auto)": "gpt-5",
    "GPT-5 Fast / Instant": "gpt-5-chat-latest",
    "GPT-5 Thinking": "gpt-5-thinking",
    "GPT-5 Pro": "gpt-5-pro",
    "GPT-5 Mini": "gpt-5-mini",
    "GPT-5 Nano": "gpt-5-nano",
    "GPT-4o": "gpt-4o",
}

OPENAI_MODEL_PRICING = {
    "gpt-5": {"input": 1.25, "output": 10.00, "cached_input": 0.125},
    "gpt-5-mini": {"input": 0.25, "output": 2.00, "cached_input": 0.025},
    "gpt-5-nano": {"input": 0.05, "output": 0.40, "cached_input": 0.005},
    "gpt-5-pro": {"input": 15.00, "output": 120.00},
    "gpt-4o": {"input": 5.00, "output": 15.00},
    "gpt-4o-mini": {"input": 0.15, "output": 0.60},
    "o3": {"input": 1.10, "output": 4.40},
    "o3-mini": {"input": 0.60, "output": 2.50},
}

AIHUMANIZE_PLANS = {
    "basic": {"price_month": 6, "words_per_request": 500},
    "starter": {"price_month": 15, "words_per_request": 500},
    "pro": {"price_month": 25, "words_per_request": 1500},
    "premium": {"price_month": 40, "words_per_request": 3000},
}

AVAILABLE_MODELS = {
    "openai": OPENAI_MODELS,
    "aihumanize": ["quality", "balance", "enhanced", "private"],
}

MODEL_PRICING = {
    "openai": OPENAI_MODEL_PRICING,
    "aihumanize": {
        "modes": ["quality", "balance", "enhanced", "private"],
        "plans": AIHUMANIZE_PLANS,
        "unit": "subscription",
    },
}


# ============================================================
# ðŸ§© Output Path Utilities (NEW flat-folder scheme)
# ============================================================

def _slug(s: str) -> str:
    return _slugify(s)


def build_filenames(company: str, role: str) -> dict[str, str]:
    """
    Standardized final artifact names.

    â€¢ Optimized resumes:
        "Sri_{Company}_{Role}.pdf"
    â€¢ Humanized resumes:
        "Sri_Kadali_{Company}_{Role}.pdf"
    â€¢ Cover letters:
        "Sri_{Company}_{Role}_Cover_Letter.pdf"
    """
    c, r = _slug(company), _slug(role)
    return {
        "optimized":    f"Sri_{c}_{r}.pdf",
        "humanized":    f"Sri_Kadali_{c}_{r}.pdf",
        "cover_letter": f"Sri_{c}_{r}_CoverLetter.pdf",
    }


# --- Final output locations (no per-job subfolders) ---

def get_optimized_pdf_path(company: str, role: str) -> Path:
    names = build_filenames(company, role)
    return (OPTIMIZED_DIR / names["optimized"]).resolve()


def get_humanized_pdf_path(company: str, role: str) -> Path:
    names = build_filenames(company, role)
    return (HUMANIZED_DIR / names["humanized"]).resolve()


def get_coverletter_pdf_path(company: str, role: str) -> Path:
    names = build_filenames(company, role)
    return (COVER_LETTERS_DIR / names["cover_letter"]).resolve()


# --- Backwards-compat sample helpers now map to the same flat scheme ---

def get_sample_resume_pdf_path(company: str, role: str) -> Path:
    return get_optimized_pdf_path(company, role)


def get_sample_humanized_pdf_path(company: str, role: str) -> Path:
    return get_humanized_pdf_path(company, role)


def get_sample_coverletter_pdf_path(company: str, role: str) -> Path:
    return get_coverletter_pdf_path(company, role)


# --- Deprecated: per-job folder builder (left as no-op alias to avoid import errors) ---

def get_job_run_dir(company: str, role: str) -> Path:
    """
    Deprecated: Per-job directories are no longer used.
    Returns OPTIMIZED_DIR for compatibility if any caller still references this.
    """
    return OPTIMIZED_DIR


# ============================================================
# ðŸ“Š Diagnostics
# ============================================================

if __name__ == "__main__":
    print("=========== HIREX CONFIG ===========")
    print(f"APP_NAME              : {APP_NAME}")
    print(f"VERSION               : {APP_VERSION}")
    print(f"BASE_DIR              : {BASE_DIR}")
    print(f"OPENAI_API_KEY_LEN    : {len(OPENAI_API_KEY) if OPENAI_API_KEY else 0}")
    print(f"HUMANIZE_API_KEY_LEN  : {len(HUMANIZE_API_KEY) if HUMANIZE_API_KEY else 0}")
    print(f"HUMANIZE_MAIL         : {HUMANIZE_MAIL or 'missing'}")
    print(f"SUPERHUMAN_LOCAL      : {SUPERHUMAN_LOCAL_ENABLED}")
    print(f"HUMANIZE_DEFAULT_ON   : {HUMANIZE_DEFAULT_ON}")
    print(f"HUMANIZE_MODE_DEFAULT : {HUMANIZE_MODE_DEFAULT} (id={AIHUMANIZE_MODE_ID.get(HUMANIZE_MODE_DEFAULT,'?')})")
    print(f"HUMANIZE_ENABLED      : {is_humanize_enabled()}")
    print(f"DEFAULT_MODEL         : {DEFAULT_MODEL}")
    print(f"OPTIMIZED_DIR         : {OPTIMIZED_DIR}")
    print(f"HUMANIZED_DIR         : {HUMANIZED_DIR}")
    print(f"COVER_LETTERS_DIR     : {COVER_LETTERS_DIR}")-e 


-e # ===== FILE: ./core/compiler.py =====

"""
HIREX â€¢ core/compiler.py
Secure LaTeX compiler â€” converts .tex â†’ .pdf in a sandboxed temp directory.
Prevents shell escapes, runs pdflatex with restricted flags.
Author: Sri Akash Kadali
"""

from __future__ import annotations

import os
import shutil
import subprocess
import tempfile
from pathlib import Path

from backend.core import config

# Robust logger wrapper (works with both new/old signatures)
try:
    from backend.core.utils import log_event as _core_log_event  # type: ignore

    def _elog(event: str, meta: dict | None = None) -> None:
        try:
            _core_log_event(event, meta or {})
        except TypeError:
            # Back-compat: older log_event(msg: str)
            _core_log_event(f"{event} {meta or ''}")  # type: ignore
except Exception:  # pragma: no cover
    def _elog(event: str, meta: dict | None = None) -> None:  # fallback to print
        print(event, meta or "")


# ============================================================
# ðŸ§© Safe PDF Compilation Utility
# ============================================================
def compile_latex_safely(tex_string: str) -> bytes | None:
    """
    Compiles LaTeX source code into PDF bytes securely.
    Returns PDF bytes (on success) or None (on failure).

    Security & Stability:
    - Uses sandboxed temp directory under config.TEMP_LATEX_DIR
    - Disables shell escape and restricts file I/O (openin/openout "p")
    - Runs pdflatex twice for stable references
    - Cleans up temporary files automatically
    - Compatible with TeX Live / MiKTeX on all OS
    """
    pdflatex_path = shutil.which("pdflatex")
    if pdflatex_path is None:
        _elog("latex_pdflatex_missing", {"detail": "pdflatex not found in PATH"})
        return None

    # Ensure our build root exists (tempfile will use it as parent)
    try:
        Path(config.TEMP_LATEX_DIR).mkdir(parents=True, exist_ok=True)
    except Exception:
        # If TEMP_LATEX_DIR is missing or invalid, fall back to system tmp
        pass

    # Safer TeX environment
    env = os.environ.copy()
    # Prevent TeX from reading/writing outside the working directory
    env.setdefault("openout_any", "p")   # p = paranoid (write only in cwd)
    env.setdefault("openin_any", "p")    # p = paranoid (read only in cwd)
    # Explicitly disable shell escape
    env.setdefault("shell_escape", "f")  # f = false
    # Keep logs readable but bounded
    env.setdefault("max_print_line", "1000")

    try:
        temp_root = getattr(config, "TEMP_LATEX_DIR", None)
        with tempfile.TemporaryDirectory(dir=str(temp_root) if temp_root else None) as tmpdir:
            tmp = Path(tmpdir)
            tex_path = tmp / "main.tex"
            pdf_path = tmp / "main.pdf"
            log_path = tmp / "compile.log"

            # Write LaTeX source
            tex_path.write_text(tex_string, encoding="utf-8")
            _elog("latex_compile_start", {"workdir": str(tmp)})

            # Safe compile command (explicitly disable shell-escape)
            # IMPORTANT: use relative filename (main.tex), not absolute path,
            # so TeX in paranoid mode (openin_any=p) will read it.
            cmd = [
                pdflatex_path,
                "-interaction=nonstopmode",
                "-halt-on-error",
                "-file-line-error",
                "-no-shell-escape",
                "-synctex=0",
                tex_path.name,  # <--- changed from str(tex_path)
            ]

            # Run pdflatex twice for cross-refs
            for i in range(2):
                proc = subprocess.run(
                    cmd,
                    cwd=tmp,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    timeout=90,
                    encoding="utf-8",
                    errors="ignore",
                    check=False,
                    env=env,
                )
                # Append compiler output
                try:
                    with open(log_path, "a", encoding="utf-8") as lf:
                        lf.write(proc.stdout or "")
                        lf.write("\n")
                except Exception:
                    pass

                _elog("latex_pdflatex_pass", {"pass": i + 1, "returncode": proc.returncode})

            # Check PDF output
            if pdf_path.exists():
                pdf_bytes = pdf_path.read_bytes()
                size_kb = len(pdf_bytes) / 1024
                _elog("latex_pdf_built", {"size_kb": round(size_kb, 1)})
                return pdf_bytes

            # Error fallback â€” show last ~20 lines of log
            if log_path.exists():
                lines = log_path.read_text(encoding="utf-8", errors="ignore").splitlines()
                tail = "\n".join(lines[-20:])
                _elog("latex_no_pdf", {"tail": tail})
            else:
                _elog("latex_no_log", {"detail": "Compilation failed â€” no log file created"})
            return None

    except subprocess.TimeoutExpired:
        _elog("latex_timeout", {"limit_seconds": 90})
        return None
    except Exception as e:
        _elog("latex_unexpected_error", {"error": str(e)})
        return None


# ============================================================
# ðŸ§ª Local Test
# ============================================================
if __name__ == "__main__":
    sample_tex = r"""
    \documentclass{article}
    \begin{document}
    Hello World! This is a HIREX LaTeX compile test.
    \end{document}
    """
    result = compile_latex_safely(sample_tex)
    print("âœ… PDF generated:", bool(result))
-e 


-e # ===== FILE: ./core/security.py =====

"""
HIREX â€¢ core/security.py (v2.0.0)
Security and validation utilities for uploaded files and user input.

âš ï¸ This build preserves full LaTeX content (no sanitization or macro stripping).
It keeps a backward-compatible `secure_tex_input(...)` API:

  â€¢ secure_tex_input(text: str) -> str
      Pass-through (returns text unchanged).

  â€¢ secure_tex_input(filename: str, content: bytes|bytearray|str) -> str
      Validates file (extension/size/non-empty), decodes to UTF-8, and returns raw LaTeX.

Author: Sri Akash Kadali
"""

from __future__ import annotations

import os
from typing import Any, Union

from backend.core import config
from backend.core.utils import safe_filename, log_event


# ============================================================
# âš™ï¸ File Validation
# ============================================================
def validate_file(upload_name: str, content: Union[bytes, bytearray, str]) -> str:
    """
    Validate uploaded file before use.
    Returns sanitized filename if safe, else raises ValueError.
    """
    if not upload_name:
        raise ValueError("âŒ Missing filename in upload.")

    _, ext = os.path.splitext(upload_name)
    allowed = getattr(config, "ALLOWED_EXTENSIONS", {".tex"})
    if ext.lower() not in allowed:
        raise ValueError(f"âŒ Invalid file extension: {ext} (allowed: {', '.join(sorted(allowed))})")

    # normalize to bytes for size & emptiness checks
    raw = content if isinstance(content, (bytes, bytearray)) else str(content).encode("utf-8", "ignore")

    size_mb = len(raw) / (1024 * 1024)
    max_mb = float(getattr(config, "MAX_UPLOAD_MB", 5))
    if size_mb > max_mb:
        raise ValueError(f"âŒ File exceeds {max_mb:.0f} MB limit (got {size_mb:.2f} MB).")

    if not raw.strip():
        raise ValueError("âŒ Uploaded file is empty.")

    safe_name = safe_filename(upload_name)
    # log_event supports string + optional meta signature across builds
    log_event(f"âœ… File validated: {safe_name} ({size_mb:.2f} MB)")
    return safe_name


# ============================================================
# ðŸ§© Pass-through LaTeX (No Sanitization)
# ============================================================
def secure_tex_input(*args: Any) -> str:
    """
    Backward-compatible pass-through.

    Usage A (strings in code paths â€” preserve as-is):
        secure_tex_input(text: str) -> str

    Usage B (uploads â€” validate & decode):
        secure_tex_input(filename: str, content: bytes|bytearray|str) -> str
    """
    # --- Usage A: single arg (plain text) ---
    if len(args) == 1:
        text = args[0]
        if text is None:
            return ""
        if not isinstance(text, str):
            text = str(text)
        # No escaping/sanitization â€” preserve full LaTeX/content
        return text

    # --- Usage B: two args (filename + raw content) ---
    if len(args) == 2:
        filename, content = args[0], args[1]
        _ = validate_file(str(filename), content)

        # Decode to UTF-8 lossily; preserve bytes that decode
        if isinstance(content, (bytes, bytearray)):
            tex = content.decode("utf-8", errors="ignore")
        elif isinstance(content, str):
            tex = content
        else:
            raise TypeError("Unsupported content type for LaTeX input. Expected bytes or str.")

        log_event(f"âœ… Raw LaTeX preserved and validated for: {filename}")
        return tex

    # --- Invalid usage ---
    raise TypeError(
        "secure_tex_input expects either (text: str) OR (filename: str, content: bytes|bytearray|str)"
    )


# ============================================================
# ðŸ§ª Local Test
# ============================================================
if __name__ == "__main__":
    example_bytes = br"""
    \documentclass{article}
    \begin{document}
    Hello \textbf{World!} This content should remain unchanged.
    \input{my_commands.tex}
    \end{document}
    """

    try:
        # File path usage
        raw_from_file = secure_tex_input("resume.tex", example_bytes)
        print("==== Raw (file) Output ====")
        print(raw_from_file)

        # Plain text usage
        raw_text = secure_tex_input(r"\section*{Projects} \textit{Keep everything intact.}")
        print("\n==== Raw (text) Output ====")
        print(raw_text)
    except Exception as e:
        print("ERROR:", e)
-e 


-e # ===== FILE: ./core/mastermind_memory.py =====

# ============================================================
#  HIREX v2.0.0 â€” backend/data/mastermind_sessions.py
#  ------------------------------------------------------------
#  Lightweight filesystem store for MasterMind chat sessions.
#  Compatible with /api/mastermind endpoints.
#  â€¢ Robust path resolution via backend.core.config
#  â€¢ UTF-8 safe read/write with graceful fallbacks
#  â€¢ Returns both legacy (created) and modern (created_at/updated_at) fields
# ============================================================

from __future__ import annotations

import json
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional

# Prefer configured directory; fall back to repo-friendly defaults
try:
    from backend.core import config
    _DEFAULT_DIR = Path(
        getattr(config, "MASTERMINDS_PATH", getattr(config, "MASTERMINDS_DIR", Path("backend") / "data" / "mastermind_sessions"))
    )
except Exception:
    _DEFAULT_DIR = Path("backend/data/mastermind_sessions")

DATA_DIR: Path = _DEFAULT_DIR
DATA_DIR.mkdir(parents=True, exist_ok=True)


def _now_iso() -> str:
    return datetime.utcnow().isoformat()


def _session_path(session_id: str) -> Path:
    return DATA_DIR / f"{session_id}.json"


def _read_json(path: Path) -> Optional[Dict[str, Any]]:
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return None


def _write_json(path: Path, data: Dict[str, Any]) -> None:
    path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")


def _new_session_id() -> str:
    return f"mm_{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}_{uuid.uuid4().hex[:6]}"


def start_session(meta: Dict[str, Any]) -> Dict[str, Any]:
    """
    Create a new session file with provided metadata.
    Returns full session dict.
    """
    sid = _new_session_id()
    now = _now_iso()
    data: Dict[str, Any] = {
        "id": sid,
        "created": now,            # legacy
        "created_at": now,         # modern
        "updated_at": now,
        "meta": meta or {},
        "messages": [],
    }
    _write_json(_session_path(sid), data)
    return data


def load_session(session_id: str) -> Dict[str, Any]:
    """
    Load a session by id. Returns {} if not found or unreadable.
    """
    p = _session_path(session_id)
    data = _read_json(p)
    return data or {}


def append_message(session_id: str, message: Dict[str, Any]) -> Dict[str, Any]:
    """
    Append a single message dict to a session (creates file if missing).
    Returns the updated session dict.
    """
    p = _session_path(session_id)
    data = _read_json(p) or {
        "id": session_id,
        "created": _now_iso(),
        "created_at": _now_iso(),
        "updated_at": _now_iso(),
        "meta": {},
        "messages": [],
    }
    data.setdefault("messages", []).append(message)
    data["updated_at"] = _now_iso()
    _write_json(p, data)
    return data


def list_sessions() -> List[Dict[str, Any]]:
    """
    Return a list of session metadata for dashboards.
    Sorted by updated_at (desc), falling back to created/created_at.
    """
    items: List[Dict[str, Any]] = []
    for file in DATA_DIR.glob("*.json"):
        try:
            data = _read_json(file)
            if not data:
                continue
            created = data.get("created_at") or data.get("created")
            updated = data.get("updated_at") or created
            meta = data.get("meta", {}) or {}
            items.append(
                {
                    "id": data.get("id"),
                    "created": created,
                    "created_at": created,
                    "updated_at": updated,
                    "persona": meta.get("persona", "General"),
                    "message_count": len(data.get("messages", []) or []),
                }
            )
        except Exception:
            continue

    def _key(d: Dict[str, Any]) -> str:
        return (d.get("updated_at") or d.get("created_at") or "")

    return sorted(items, key=_key, reverse=True)
-e 


-e # ===== FILE: ./core/utils.py =====

"""
ASTRA â€¢ core/utils.py (v2.1.2)
Common utility functions shared across backend modules.
For this version: No LaTeX escaping or text cleaning â€” passes LaTeX as-is.
Author: Sri Akash Kadali
"""

from __future__ import annotations

import re
import html
import hashlib
import json
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional

# ============================================================
# ðŸ“ Logging path (prefer config; safe local fallback if import fails)
# ============================================================
try:
    from backend.core import config as _cfg  # type: ignore
    LOG_PATH = Path(getattr(_cfg, "LOG_PATH"))
except Exception:
    # Dev/test fallback (project-local)
    LOG_PATH = Path("data/logs/events.jsonl")

LOG_PATH.parent.mkdir(parents=True, exist_ok=True)


# ============================================================
# ðŸ—‚ï¸ Filesystem Helpers
# ============================================================
def ensure_dir(p: Path | str) -> None:
    """Create directory (and parents) if it does not exist."""
    Path(p).mkdir(parents=True, exist_ok=True)


# ============================================================
# ðŸ” HASHING UTILITIES
# ============================================================
def sha256_str(data: Optional[str]) -> str:
    """Generate a full SHA256 hash of a string."""
    if data is None:
        data = ""
    return hashlib.sha256(data.encode("utf-8")).hexdigest()


def simple_hash(data: Optional[str], length: int = 8) -> str:
    """Generate a short deterministic hash (used for cache keys or content IDs)."""
    return sha256_str(data or "")[:max(1, int(length))]


# ============================================================
# ðŸ·ï¸ Naming Helpers (no LaTeX escaping)
# ============================================================
def safe_filename(name: Optional[str]) -> str:
    """Convert a string into a safe, cross-platform filename."""
    if not name:
        return "file"
    # keep letters, digits, underscore, dot, dash; replace others with underscore
    name = re.sub(r"[^A-Za-z0-9_.-]", "_", name)
    # Avoid leading/trailing dots or underscores; trim length
    name = name.strip("._") or "file"
    return name[:64]


def slug_part(s: Optional[str]) -> str:
    """
    A permissive slug for path/filename parts:
      - Replace non [A-Za-z0-9_.-] with underscores
      - Keep length generous (no hard trim here; callers can trim if needed)
    """
    s = s or ""
    s = re.sub(r"[^A-Za-z0-9_.-]+", "_", s).strip("._")
    return s or "unnamed"


def build_filenames(company: str, role: str) -> Dict[str, str]:
    """
    Standardized final artifact names:

      â€¢ Optimized resumes:
          "Sri_{Company}_{Role}.pdf"
      â€¢ Humanized resumes:
          "Sri_Kadali_{Company}_{Role}.pdf"
      â€¢ Cover letters:
          "Sri_{Company}_{Role}_Cover_Letter.pdf"
    """
    c = slug_part(company)
    r = slug_part(role)
    return {
        "optimized":    f"Sri_{c}_{r}.pdf",
        "humanized":    f"Sri_Kadali_{c}_{r}.pdf",
        "cover_letter": f"Sri_{c}_{r}_Cover_Letter.pdf",
    }


def build_output_paths(company: str, role: str) -> Dict[str, Path]:
    """
    Convenience helper that returns the *full* Paths for each final artifact,
    using directories from config (if available).

    Returns dict with keys: optimized, humanized, cover_letter
    """
    names = build_filenames(company, role)
    try:
        # Prefer config-provided directories (typically absolute/user-data-safe)
        opt_dir = Path(getattr(_cfg, "OPTIMIZED_DIR"))
        hum_dir = Path(getattr(_cfg, "HUMANIZED_DIR"))
        cov_dir = Path(getattr(_cfg, "COVER_LETTERS_DIR"))
    except Exception:
        # Safe fallbacks â€” project-local relative paths
        opt_dir = Path("data/Optimized")
        hum_dir = Path("data/Humanized")
        cov_dir = Path("data/Cover Letters")

    # Ensure the directories exist
    for d in (opt_dir, hum_dir, cov_dir):
        d.mkdir(parents=True, exist_ok=True)

    return {
        "optimized": (opt_dir / names["optimized"]).resolve(),
        "humanized": (hum_dir / names["humanized"]).resolve(),
        "cover_letter": (cov_dir / names["cover_letter"]).resolve(),
    }


# ============================================================
# ðŸ“œ TEXT HELPERS (NO LATEX ESCAPING)
# ============================================================
def tex_escape(text: Optional[str]) -> str:
    """
    Passthrough for LaTeX text (no escaping).
    Used when sending LaTeX to or receiving from OpenAI/Humanize.
    """
    return text or ""


def html_escape(text: Optional[str]) -> str:
    """HTML-escape text for safe display inside web UIs (not LaTeX)."""
    return html.escape(text or "")


def clean_text(text: Optional[str]) -> str:
    """
    Lightweight text cleaner (no normalization, no space compression).
    Keeps LaTeX intact.
    """
    if not text:
        return ""
    return str(text)


# ============================================================
# ðŸ§  LOGGING & DIAGNOSTIC HELPERS
# ============================================================
def utc_now_iso() -> str:
    """Return current UTC timestamp in ISO-8601 format."""
    return datetime.utcnow().isoformat() + "Z"


def log_event(event: str, meta: Optional[Dict[str, Any]] = None) -> None:
    """
    Append a JSON line to the global event log and print to console.
    Used by all backend modules for analytics and dashboard.

    Accepts:
      â€¢ event: short event string
      â€¢ meta : optional dict payload (anything JSON-serializable; non-serializable values coerced to str)
    """
    record = {
        "timestamp": utc_now_iso(),
        "event": str(event),
        "meta": meta or {},
    }

    # Console log (truncate very large metas for readability)
    try:
        preview = json.dumps(record["meta"], ensure_ascii=False, default=str)
        if len(preview) > 800:
            preview = preview[:800] + "â€¦"
        print(f"[{record['timestamp']}] {record['event']} :: {preview}")
    except Exception:
        print(f"[{record['timestamp']}] {record['event']} :: (unserializable meta)")

    # Persistent log (append JSONL)
    try:
        LOG_PATH.parent.mkdir(parents=True, exist_ok=True)
        with open(LOG_PATH, "a", encoding="utf-8") as f:
            f.write(json.dumps(record, ensure_ascii=False, default=str) + "\n")
    except Exception as e:
        print(f"[ASTRA] âš ï¸ Failed to write event log: {e}")


def benchmark(name: str):
    """
    Context manager for timing code blocks.

    Example:
        with benchmark("Optimize Resume"):
            run_some_code()
    """
    import time

    class _Timer:
        def __enter__(self):
            self._start = time.time()
            return self

        def __exit__(self, exc_type, exc_val, exc_tb):
            duration_ms = (time.time() - self._start) * 1000.0
            log_event("â±ï¸ benchmark", {"name": name, "duration_ms": round(duration_ms, 1)})

    return _Timer()


# ============================================================
# ðŸ§ª Local Test (generic for any companies/roles)
# ============================================================
if __name__ == "__main__":
    import sys

    # 1) Quick LaTeX + hash sanity check
    sample = r"""
    \documentclass{article}
    \begin{document}
    Hello \textbf{World!} $E = mc^2$
    \end{document}
    """
    print("Original LaTeX (unchanged):")
    print(sample)
    print("SHA256:", sha256_str(sample))
    print("Short Hash:", simple_hash(sample))
    print("Safe File:", safe_filename("My Resume (final).tex"))

    # 2) Filename + output path checks (accept multiple pairs from stdin or argv)
    pairs: list[tuple[str, str]] = []

    # Read JSONL from stdin: each line like {"company":"...", "role":"..."}
    if sys.stdin and not sys.stdin.isatty():
        for line in sys.stdin:
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
                pairs.append((str(obj["company"]), str(obj["role"])))
            except Exception:
                # Ignore malformed lines
                pass

    # Or accept args as JSON or "Company:Role"
    if not pairs and len(sys.argv) > 1:
        for a in sys.argv[1:]:
            a = a.strip()
            if not a:
                continue
            try:
                obj = json.loads(a)
                pairs.append((str(obj["company"]), str(obj["role"])))
                continue
            except Exception:
                pass
            if ":" in a:
                c, r = a.split(":", 1)
                pairs.append((c.strip(), r.strip()))

    # Fallback instructional example if nothing was provided
    if not pairs:
        print("\nUsage:")
        print("  â€¢ cat jobs.jsonl | python -m backend.core.utils")
        print('  â€¢ python -m backend.core.utils "Company:Role" ...')
        print('  â€¢ python -m backend.core.utils \'{"company":"X","role":"Y"}\' ...')
        pairs = [("ExampleCo", "ExampleRole")]

    # Verify the filename scheme + full paths for each pair
    for company, role in pairs:
        names = build_filenames(company, role)
        paths = build_output_paths(company, role)
        print(f"\n[{company} â€¢ {role}]")
        print(" Names :", names)
        print(" Paths :", {k: str(v) for k, v in paths.items()})

    # 3) Micro-benchmark
    with benchmark("Hash Generation"):
        for _ in range(10000):
            sha256_str(sample)
-e 


-e # ===== FILE: ./tests/test_optimize.py =====

"""
HIREX â€¢ tests/test_optimize.py
Minimal end-to-end tests for API + static frontend.
Run:  pytest -q
Author: Sri Akash Kadali
"""

from io import BytesIO
from pathlib import Path
from fastapi.testclient import TestClient
from backend.main import app

client = TestClient(app)


# ============================================================
# ðŸ”§ Sample Minimal LaTeX Resume
# ============================================================

def _sample_tex() -> str:
    """Returns a minimal valid LaTeX resume for test upload."""
    return r"""
\documentclass[letterpaper,10pt]{article}
\usepackage[margin=0.6in]{geometry}
\begin{document}
\section{Education}
\begin{itemize}
\item B.S. in Computer Science, XYZ University
\end{itemize}

\section{Skills}
Languages: Python, SQL
Cloud: AWS, Docker

\section{Experience}
\textbf{Acme Corp} \hfill \textit{Software Intern} \hfill 2024
\begin{itemize}
\item worked on python data pipelines
\item helped with docker deployment
\end{itemize}
\end{document}
""".strip()


# ============================================================
# ðŸ§  API Health & Frontend Tests
# ============================================================

def test_health_endpoint():
    """Ensure /health endpoint is alive and reports version."""
    r = client.get("/health")
    assert r.status_code == 200, f"Health failed: {r.text}"
    data = r.json()
    assert data.get("status") == "ok"
    assert "version" in data


def test_frontend_index_served():
    """Ensure root / serves the frontend HTML file."""
    r = client.get("/")
    assert r.status_code == 200, f"Frontend index not served: {r.text}"
    assert "text/html" in r.headers.get("content-type", "")


# ============================================================
# ðŸš€ /api/optimize End-to-End Tests
# ============================================================

def test_optimize_endpoint_happy_path(tmp_path: Path):
    """
    Verify that /api/optimize processes a sample resume and JD text
    end-to-end through OpenAI + Humanize pipeline.
    """
    tex_bytes = _sample_tex().encode("utf-8")
    files = {"base_resume_tex": ("resume.tex", BytesIO(tex_bytes), "text/plain")}
    data = {"jd_text": "We need Python, AWS, FastAPI. Seniority: SWE I or Senior."}

    r = client.post("/api/optimize", files=files, data=data)
    assert r.status_code == 200, f"Unexpected response: {r.text}"
    payload = r.json()

    # Check essential keys in response
    for key in ("tex_string", "jd_summary", "coverage", "pdf_base64"):
        assert key in payload, f"Missing key in response: {key}"

    # Validate JD summary structure
    jd = payload["jd_summary"]
    for key in ("must_haves", "skills", "metrics", "seniority"):
        assert key in jd, f"Missing JD key: {key}"

    # Ensure coverage includes at least Python (case-insensitive)
    items = {c["item"].lower() for c in payload["coverage"]}
    assert "python" in items, f"Coverage missing Python: {items}"

    # PDF presence check
    pdf_b64 = payload.get("pdf_base64")
    assert isinstance(pdf_b64, (str, type(None)))
    if pdf_b64:
        assert len(pdf_b64) > 50, "Encoded PDF too small (possibly truncated)."


def test_optimize_endpoint_missing_fields():
    """Missing JD text should result in a validation or bad request error."""
    tex_bytes = _sample_tex().encode("utf-8")
    files = {"base_resume_tex": ("resume.tex", BytesIO(tex_bytes), "text/plain")}
    r = client.post("/api/optimize", files=files, data={})
    assert r.status_code in (400, 422), f"Expected validation failure, got {r.status_code}"


def test_optimize_endpoint_invalid_file():
    """Invalid extension or empty content should not crash the API."""
    files = {"base_resume_tex": ("resume.txt", BytesIO(b""), "text/plain")}
    r = client.post("/api/optimize", files=files, data={"jd_text": "Python"})
    assert r.status_code in (400, 500), f"Unexpected status: {r.status_code}"
-e 


-e # ===== FILE: ./api/optimize.py =====

import asyncio
import base64
import json
import re
from pathlib import Path
from typing import List, Tuple, Dict, Iterable, Optional, Set

# --- third-party ---
import httpx
from fastapi import APIRouter, UploadFile, Form, File, HTTPException
from fastapi.responses import JSONResponse
from backend.core import config
from backend.core.compiler import compile_latex_safely
from backend.core.security import secure_tex_input
from backend.core.utils import log_event, safe_filename, build_output_paths
from backend.api.render_tex import render_final_tex

router = APIRouter(prefix="/api/optimize", tags=["optimize"])

from openai import OpenAI

_openai_client: OpenAI | None = None

def get_openai_client() -> OpenAI:
    global _openai_client
    if _openai_client is None:
        _openai_client = OpenAI(api_key=config.OPENAI_API_KEY)
    return _openai_client


# ============================================================
# ðŸ”’ LaTeX-safe utils
# ============================================================

LATEX_ESC = {
    "#": r"\#",
    "%": r"\%",
    "$": r"\$",
    "&": r"\&",
    "_": r"\_",
    "{": r"\{",
    "}": r"\}",
}
UNICODE_NORM = {
    "â€“": "-", "â€”": "-", "âˆ’": "-",
    "â€¢": "-", "Â·": "-", "â—": "-",
    "â†’": "->", "â‡’": "=>", "â†”": "<->",
    "Ã—": "x", "Â°": " degrees ",
    "â€™": "'", "â€˜": "'", "â€œ": '"', "â€": '"',
    "\u00A0": " ", "\uf0b7": "-", "\x95": "-",
}

# Strip any accidental local-fallback labels that might leak from a lower layer
_FALLBACK_TAG_RE = re.compile(r"^\[LOCAL-FALLBACK:[^\]]+\]\s*", re.IGNORECASE)

def latex_escape_text(s: str) -> str:
    if not s or not isinstance(s, str):
        return ""
    # Normalize unicode first
    for a, b in UNICODE_NORM.items():
        s = s.replace(a, b)
    # Escape special characters (but not if already escaped)
    specials = ['%', '$', '&', '_', '#', '{', '}']
    for ch in specials:
        s = re.sub(rf'(?<!\\){re.escape(ch)}', LATEX_ESC[ch], s)
    # Handle caret specially
    s = re.sub(r'(?<!\\)\^', r'\^{}', s)
    # Collapse multiple spaces
    s = re.sub(r"[ \t]+", " ", s).strip()
    # Remove any stray backslashes that aren't part of valid LaTeX commands
    s = re.sub(r'\\(?![a-zA-Z#$%&_{}^])', '', s)
    return s

def strip_all_macros_keep_text(s: str) -> str:
    prev = None
    while prev != s:
        prev = s
        s = re.sub(r"\\[a-zA-Z]+\{([^{}]*)\}", r"\1", s)
    s = re.sub(r"\\[a-zA-Z]+", "", s)
    s = s.replace("{", "").replace("}", "")
    for a, b in UNICODE_NORM.items():
        s = s.replace(a, b)
    return s.strip()

def _enforce_used_diversity(b: str, pkw: List[str], used_set: Set[str]) -> str:
    """
    Enforce keyword uniqueness WITHIN ONE EXPERIENCE SECTION.
    Does NOT prevent reuse across different roles.
    """

    present = _present_kws(b, pkw, used_set)

    # If this bullet already has â‰¥2 UNUSED keywords, accept
    if len(present) >= 2:
        return b

    # Pick up to 2 unused keywords for this section
    inject: List[str] = []
    for k in pkw:
        nk = _norm(k)
        if nk in used_set:
            continue
        if nk not in present:
            inject.append(k)
        if len(inject) == 2:
            break

    if not inject:
        return b

    t = b
    for k in inject:
        if re.search(r"\bpipeline\b", t, re.I):
            t = re.sub(r"\bpipeline\b", f"{k} pipeline", t, count=1, flags=re.I)
        elif re.search(r"\bworkflow\b", t, re.I):
            t = re.sub(r"\bworkflow\b", f"{k} workflow", t, count=1, flags=re.I)
        elif re.search(r"\bsystem\b", t, re.I):
            t = re.sub(r"\bsystem\b", f"{k} system", t, count=1, flags=re.I)
        else:
            t = re.sub(
                rf"^({STRONG_VERBS})\b",
                rf"\1 {k}",
                t,
                count=1,
                flags=re.I
            )

    t = _cap_24_words(
        _rm_brackets(
            _safe_clean(
                _past_tense_bias(
                    _ensure_strong_verb_start(t)
                )
            )
        )
    )

    # consume keywords ONLY for this section
    for k in inject:
        used_set.add(_norm(k))

    return t

# ============================================================
# ðŸ§° Balanced \resumeItem parser (handles nested braces)
# ============================================================

def find_resume_items(block: str) -> List[Tuple[int, int, int, int]]:
    r"""
    Finds \resumeItem{...} with balanced braces, tolerating optional whitespace:
      \resumeItem{...}   or   \resumeItem   { ... }
    Returns (macro_start, open_brace_idx, close_brace_idx, end_idx_after_close).
    """
    out: List[Tuple[int, int, int, int]] = []
    i = 0
    macro = r"\resumeItem"
    n = len(macro)

    while True:
        i = block.find(macro, i)
        if i < 0:
            break

        j = i + n
        # allow spaces before "{"
        while j < len(block) and block[j].isspace():
            j += 1
        if j >= len(block) or block[j] != "{":
            i = j
            continue

        open_b = j
        depth, k = 0, open_b
        while k < len(block):
            ch = block[k]
            if ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    close_b = k
                    out.append((i, open_b, close_b, close_b + 1))
                    i = close_b + 1
                    break
            k += 1
        else:
            # unmatched brace; stop scanning this macro occurrence
            break

    return out

def replace_resume_items(block: str, replacements: List[str]) -> str:
    items = find_resume_items(block)
    if not items:
        return block
    if len(replacements) < len(items):
        replacements = replacements + [None] * (len(items) - len(replacements))
    out, last = [], 0
    for (start, open_b, close_b, end), newtxt in zip(items, replacements):
        out.append(block[last:open_b + 1])
        if newtxt is None:
            out.append(block[open_b + 1:close_b])
        else:
            out.append(newtxt)
        out.append(block[close_b:end])
        last = end
    out.append(block[last:])
    return "".join(out)

# ============================================================
# ðŸ”Ž Section matchers (supports \section and \section*)
# ============================================================

def section_rx(name: str) -> re.Pattern:
    return re.compile(
        rf"(\\section\*?\{{\s*{re.escape(name)}\s*\}}[\s\S]*?)(?=\\section\*?\{{|\\end\{{document\}}|$)",
        re.IGNORECASE
    )

SECTION_HEADER_RE = re.compile(r"\\section\*?\{\s*([^\}]*)\s*\}", re.IGNORECASE)

# ============================================================
# ðŸ§  GPT helpers (strict JSON)
# ============================================================

def _json_from_text(text: str, default):
    m = re.search(r"\{[\s\S]*\}", text)
    if not m:
        return default
    try:
        return json.loads(m.group(0))
    except Exception:
        return default

async def gpt_json(prompt: str, temperature: float = 0.0) -> dict:
    resp = get_openai_client().chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=temperature,
        timeout=120,
    )
    return _json_from_text(resp.choices[0].message.content or "{}", {})

# ============================================================
# ðŸ§  GPT helpers (strict JSON) â€” ADD THIS directly under gpt_json
# ============================================================

async def gpt_chat_json(messages: List[Dict[str, str]], temperature: float = 0.0) -> dict:
    """
    Chat-style JSON extractor: pass a list of {role, content} messages.
    Returns parsed JSON object if present, else {}.
    """
    resp = get_openai_client().chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        temperature=temperature,
    )
    text = resp.choices[0].message.content or "{}"
    return _json_from_text(text, {})

# ============================================================
# ðŸ§  JD â†’ company + role (GPT only)
# ============================================================

async def extract_company_role(jd_text: str) -> Tuple[str, str]:
    company_role_example = '{"company":"â€¦","role":"â€¦"}'
    prompt = (
        "Return STRICT JSON:\n"
        f"{company_role_example}\n"
        "Use the official company short name and the exact job title.\n"
        "JD:\n"
        f"{jd_text}"
    )
    try:
        data = await gpt_json(prompt, temperature=0.0)
        company = data.get("company", "Company")
        role = data.get("role", "Role")
        log_event(f"ðŸ§  [JD PARSE] Extracted â†’ company={company}, role={role}")
        return company, role
    except Exception as e:
        log_event(f"âš ï¸ [JD PARSE] Failed: {e}")
        return "Company", "Role"

# ============================================================
# ðŸ§  JD â†’ 4 Pillars (most important competencies) + keywords
# ============================================================

async def extract_pillars_gpt(jd_text: str) -> List[Dict[str, object]]:
    """
    Return four distinct, job-critical pillars with 6â€“10 must-use keywords each.
    STRICT JSON:
    {"pillars":[
      {"name":"Modeling & Algorithms","keywords":["Machine Learning","Deep Learning","PyTorch","TensorFlow","Transformers","Evaluation","Metrics"]},
      {"name":"Data Engineering","keywords":["ETL","Airflow","Spark","SQL","Data Warehouse","Pipeline","Batch","Streaming"]},
      {"name":"Cloud & MLOps","keywords":["AWS","GCP","Azure","Docker","Kubernetes","CI/CD","MLflow","Monitoring"]},
      {"name":"Application & APIs","keywords":["FastAPI","Django","REST","Latency","Throughput","Caching","Profiling"]}
    ]}
    """
    example = {
        "pillars": [
            {"name":"...", "keywords":["...","..."]},
            {"name":"...", "keywords":["...","..."]},
            {"name":"...", "keywords":["...","..."]},
            {"name":"...", "keywords":["...","..."]}
        ]
    }
    prompt = (
        "Return STRICT JSON ONLY with exactly 4 pillars ranked by importance to the job.\n"
        "Each pillar must have 6â€“10 short, canonical keywords/tools/tasks that MUST be used in bullets.\n"
        f"JSON schema example (structure only): {json.dumps(example, ensure_ascii=False)}\n\n"
        "JOB DESCRIPTION:\n"
        f"{jd_text}"
    )
    data = await gpt_json(prompt, temperature=0.0)
    pillars = (data or {}).get("pillars") or []
    # minimal sanitization
    out = []
    for p in pillars[:4]:
        name = strip_all_macros_keep_text(str(p.get("name","")).strip()) or "Pillar"
        kws  = [canonicalize_token(x) for x in (p.get("keywords") or []) if str(x).strip()]
        if kws:
            out.append({"name": name, "keywords": kws[:10]})
    # pad to 4 if model under-returns
    while len(out) < 4:
        out.append({"name": f"Pillar {len(out)+1}", "keywords": []})
    return out[:4]


# ============================================================
# âœ… Eligibility (pre-rewrite) using strict JD tokens
# ============================================================

async def compute_eligibility_any(
    raw_tex: str,
    jd_text: str,
    extra: Optional[List[str]] = None
) -> Dict[str, object]:
    jd_tokens = await get_coverage_targets_from_jd(jd_text, strict=True)
    if extra:
        jd_tokens += [canonicalize_token(x) for x in extra if str(x).strip()]
    base_plain = _plain_text_for_coverage(raw_tex)
    present, missing = _present_tokens_in_text(base_plain, jd_tokens)
    total = max(1, len(jd_tokens))
    score = len(present) / total
    verdict = (
        "Strong fit" if score >= 0.65 else
        "Viable with tailoring" if score >= 0.40 else
        "Borderline â€” consider upskilling on missing areas"
    )
    return {
        "score": round(score, 3),
        "present": sorted(present),
        "missing": sorted(missing),
        "total": total,
        "verdict": verdict,
    }



# ============================================================
# ðŸ§® Canonicalization + keep JD requirements
# ============================================================

CANON_SYNONYMS = {
    "hf transformers": "Hugging Face Transformers",
    "transformers": "Hugging Face Transformers",
    "pytorch lightning": "PyTorch",
    "sklearn": "scikit-learn",
    "big query": "BigQuery",
    "google bigquery": "BigQuery",
    "ms sql": "SQL",
    "mysql": "SQL",
    "postgres": "SQL",
    "postgresql": "SQL",
    "bert": "BERT",
    "large language models": "LLMs",
    "llm": "LLMs",
    "llms": "LLMs",
    "gen ai": "Generative AI",
    "generative ai": "Generative AI",
    "ci cd": "CI/CD",
    "k8s": "Kubernetes",
    "g cloud": "GCP",
    "microsoft excel": "Excel",
    # Web3 / blockchain
    "typescript.js": "TypeScript",
    "typesript.js": "TypeScript",
    "web3js": "Web3.js",
    "web3.js": "Web3.js",
    "ethersjs": "Ethers.js",
    "ethers.js": "Ethers.js",
    "smart contracts": "Smart contracts",
}

LANG_MAP = {
    "professional proficiency in english": "English (professional)",
    "english proficiency": "English (professional)",
    "english": "English (professional)",
    "professional proficiency in chinese": "Chinese (professional)",
    "chinese proficiency": "Chinese (professional)",
    "chinese": "Chinese (professional)",
}

def _canon_phrase_shrink(s: str) -> str:
    ls = s.lower().strip()
    m = re.match(r"(basic|foundational)\s+(knowledge|understanding)\s+of\s+(.+)", ls)
    if m: return m.group(3)
    m = re.match(r"(strong|keen)\s+(interest|curiosity)\s+(in|for)\s+(.+)", ls)
    if m: return m.group(4)
    m = re.match(r"(basic|good)\s+(grasp|idea)\s+of\s+(.+)", ls)
    if m: return m.group(3)
    return s

def canonicalize_token(s: str) -> str:
    s = _canon_phrase_shrink(s)
    ls = s.lower().strip()
    s = CANON_SYNONYMS.get(ls, s)
    s = LANG_MAP.get(ls, s)
    s = s.strip(" ,.;:/")
    if ls in {"typescript"}: s = "TypeScript"
    if ls in {"solidity"}: s = "Solidity"
    if ls in {"rust"}: s = "Rust"
    if ls in {"javascript"}: s = "JavaScript"
    return s

def prune_and_compact_skills(skills: List[str], protected: Set[str]) -> List[str]:
    filler_patterns = [
        r"\bability to\b", r"\bexperience with\b", r"\bfamiliarity with\b",
        r"\bstrong\b", r"\bexcellent\b", r"\bproficiency\b"
    ]
    out, seen = [], set()
    prot_lower = {p.lower() for p in protected}
    for raw in skills:
        s = canonicalize_token(raw)
        ls = s.lower()
        if ls not in prot_lower and any(re.search(p, ls) for p in filler_patterns):
            continue
        key = s.lower()
        if key not in seen:
            seen.add(key)
            out.append(s)
    return out

# ============================================================
# ðŸŽ¯ JD â†’ Skills (high-recall + semantically aligned)
# ============================================================

async def extract_skills_gpt(jd_text: str) -> Tuple[List[str], Set[str]]:
    """
    Extract high-recall skill tokens from any JD.
    Captures explicit and strongly implied technical and linguistic skills
    so early coverage â‰ˆ95-99 % without multiple refinement loops.
    """
    json_example = (
        '{\n'
        '  "jd_keywords": ["Python","SQL","LLMs","Transformer Models","Debugging Workflows",'
        '"Data Analysis","Machine Learning","Evaluation","Prompt Engineering","Docker","Kubernetes"],\n'
        '  "requirements": ["Python","JavaScript","Problem Solving","Debugging Workflows",'
        '"Algorithms","Data Structures","English (professional)"],\n'
        '  "related": ["Pandas","NumPy","scikit-learn","PyTorch","TensorFlow","OpenAI API",'
        '"FastAPI","AWS","GCP","CI/CD","Git","Linux"]\n'
        '}'
    )

    prompt = (
        "Extract all technical and linguistic skill tokens for this job in THREE sets.\n\n"
        "1) \"jd_keywords\": include every concrete skill, library, framework, platform, or concept "
        "explicitly mentioned OR semantically implied by the JD "
        "(e.g., if the JD says 'large language models', also include 'LLMs', 'Transformer Models').\n"
        "2) \"requirements\": the MUST-HAVE skills or task-type requirements "
        "(annotation, debugging, evaluation, data labeling, algorithmic problem solving, etc.).\n"
        "3) \"related\": adjacent or supporting skills that a recruiter would expect to co-occur "
        "(e.g., Pandas with Python, Docker with CI/CD, etc.).\n\n"
        "Rules:\n"
        "- Return STRICT JSON ONLY in the format:\n"
        f"{json_example}\n"
        "- Deduplicate across lists.\n"
        "- Use short canonical tokens (1â€“4 words, no full sentences).\n"
        "- Include both direct and adjacent terms that appear naturally on a resume.\n"
        "JD:\n"
        f"{jd_text}"
    )

    try:
        data = await gpt_json(prompt, temperature=0.0)
        jd_kw = data.get("jd_keywords", []) or []
        reqs  = data.get("requirements", []) or []
        rel   = data.get("related", []) or []

        combined, seen = [], set()
        for lst in (jd_kw, reqs, rel):
            for s in lst:
                s = re.sub(r"[^\w\-\+\.#\/ \(\)]", "", str(s)).strip()
                if not s:
                    continue
                s = canonicalize_token(s)
                if s.lower() not in seen:
                    seen.add(s.lower())
                    combined.append(s)

        protected = {canonicalize_token(s).lower() for s in (jd_kw + reqs)}
        log_event(f"ðŸ’¡ [JD SKILLS] high-recall jd={len(jd_kw)} req={len(reqs)} rel={len(rel)} â†’ {len(combined)} total")
        return combined, protected

    except Exception as e:
        log_event(f"âš ï¸ [JD SKILLS] Failed: {e}")
        return [], set()

# ============================================================
# ðŸŽ“ JD â†’ Relevant Coursework (GPT only)
# ============================================================

async def extract_coursework_gpt(jd_text: str, max_courses: int = 24) -> List[str]:
    courses_json_example = '{"courses":["Machine Learning","Time Series Analysis","Financial Analytics"]}'
    prompt = (
        f"From the JD, choose up to {max_courses} highly relevant university courses "
        "that best signal fit. Return STRICT JSON:\n"
        f"{courses_json_example}\n"
        "Use standard course titles (concise).\n"
        "JD:\n"
        f"{jd_text}"
    )
    try:
        data = await gpt_json(prompt, temperature=0.0)
        courses = data.get("courses", []) or []
        out, seen = [], set()
        for c in courses:
            c = re.sub(r"\s+", " ", str(c)).strip()
            if not c:
                continue
            k = c.lower()
            if k not in seen:
                seen.add(k)
                out.append(c)
            if len(out) >= max_courses:
                break
        log_event(f"ðŸŽ“ [JD COURSES] GPT returned {len(out)} courses.")
        return out
    except Exception as e:
        log_event(f"âš ï¸ [JD COURSES] Failed: {e}")
        return []

# ============================================================
# ðŸ§± Skills rendering â€” EXACTLY 4 LINES (Web3-aware + GPT labels)
# ============================================================

def categorize(sk: Iterable[str]) -> Dict[str, List[str]]:
    cat = {k: [] for k in [
        "Programming", "Data & ML", "Frameworks", "Data Engineering", "Cloud & DevOps",
        "Visualization", "Tools", "Math & Stats", "Soft Skills"
    ]}
    for s in sk:
        t = canonicalize_token(s)
        ls = t.lower()
        def add(bucket): cat[bucket].append(t)

        if ls in {
            "python","r","sql","c++","java","scala","go","matlab","javascript","typescript",
            "rust","solidity","c#","swift","php","kotlin"
        }:
            add("Programming"); continue

        if any(x in ls for x in [
            "pandas","numpy","scipy","scikit","tensor","keras","torch","xgboost","lightgbm",
            "transformers","spacy","catboost","opencv","bert","llms","generative ai","prompt engineering"
        ]) or any(x in ls for x in ["machine learning","deep learning","nlp","vision","time series","recomm"]):
            add("Data & ML"); continue

        if any(x in ls for x in [
            "react","angular","vue","next.js","nuxt","node.js","express","nestjs","django","flask","fastapi",
            "spring",".net","rails","laravel","truffle","hardhat","foundry","openzeppelin","web3.js","ethers.js"
        ]):
            add("Frameworks"); continue

        if any(x in ls for x in [
            "spark","hadoop","airflow","dbt","kafka","snowflake","databricks","bigquery","redshift",
            "etl","warehouse","pipeline"
        ]):
            add("Data Engineering"); continue

        if any(x in ls for x in ["aws","gcp","azure","docker","kuber","kubernetes","ci/cd","mlops","devops","cloud"]):
            add("Cloud & DevOps"); continue

        if any(x in ls for x in [
            "power bi","tableau","matplotlib","seaborn","plotly","viz","visual","excel","gis",
            "data visualization","data analysis","data management","profiling","data profiling"
        ]):
            add("Visualization"); continue

        if any(x in ls for x in [
            "git","linux","bash","unix","jira","mlflow","annotation","labeling","relevance evaluation",
            "preference ranking","summarization","translation","transcription","response generation",
            "response rewrite","similarity evaluation","data collection","content evaluation",
            "prompt","grading","identification","ranking","ethereum","web3","smart contracts","tokenomics",
            "dao","dao frameworks","gamification","crypto","cryptography"
        ]):
            add("Tools"); continue

        if any(x in ls for x in ["english (professional)","chinese (professional)","english","chinese"]):
            add("Soft Skills"); continue

        if any(x in ls for x in ["stat","probab","hypothesis","linear algebra","optimization"]):
            add("Math & Stats"); continue

        add("Tools")

    for k in cat:
        seen, ded = set(), []
        for v in cat[k]:
            if v.lower() not in seen:
                seen.add(v.lower()); ded.append(v)
        cat[k] = ded
    return cat

def _split_half(vals: List[str]) -> Tuple[List[str], List[str]]:
    if not vals: return [], []
    mid = (len(vals) + 1) // 2
    return vals[:mid], vals[mid:]

def _build_skill_rows(cat: Dict[str, List[str]]) -> List[Tuple[str, List[str]]]:
    prog = cat.get("Programming", [])
    ml   = cat.get("Data & ML", [])
    fw   = cat.get("Frameworks", [])
    engd = (cat.get("Data Engineering", []) or []) + (cat.get("Cloud & DevOps", []) or [])
    vizt = (cat.get("Visualization", []) or []) + (cat.get("Tools", []) or [])
    other= (cat.get("Math & Stats", []) or []) + (cat.get("Soft Skills", []) or [])

    rows: List[Tuple[str, List[str]]] = []
    rows.append(("Programming", prog if not (prog and all(p.lower()=="sql" for p in prog)) else prog))
    if ml: rows.append(("Machine Learning", ml)); ml=[]
    elif fw: rows.append(("Frameworks & Libraries", fw)); fw=[]
    elif vizt: half, vizt = _split_half(vizt); rows.append(("Business Intelligence & Analytics", half))
    elif other: half, other = _split_half(other); rows.append(("Other Requirements", half))
    else: rows.append(("Frameworks & Libraries", []))

    if engd: rows.append(("Data Engineering & DevOps", engd)); engd=[]
    elif fw: half, fw = _split_half(fw); rows.append(("Frameworks & Libraries", half))
    elif vizt: half, vizt = _split_half(vizt); rows.append(("Tools & Platforms", half))
    elif other: half, other = _split_half(other); rows.append(("Other Requirements", half))
    else: rows.append(("Data Engineering & DevOps", []))

    tail = (fw or []) + (vizt or []) + (other or []) + (ml or []) + (engd or [])
    row4_label = "Soft Skills & Other" if other and len(other) >= max(1, len(tail)//2) else "Additional Tools & Skills"
    rows.append((row4_label, tail))
    return rows[:4]

def _sample_list(vals: List[str], k: int = 10) -> List[str]:
    return vals[:k]

def _clean_label(s: str) -> str:
    s = strip_all_macros_keep_text(str(s))
    s = re.sub(r"[^A-Za-z0-9&\/\-\+\.\s]", "", s).strip()
    s = re.sub(r"\s+", " ", s)
    return " ".join(w.capitalize() if not re.match(r"[&/]", w) else w for w in s.split()).strip()

def _valid_labels(labels: List[str]) -> bool:
    if not isinstance(labels, list) or len(labels) != 4:
        return False
    cleaned = [_clean_label(x) for x in labels]
    if any(len(x) == 0 or len(x) > 32 for x in cleaned):
        return False
    if len(set(x.lower() for x in cleaned)) != 4:
        return False
    return True

async def propose_skill_labels_gpt(rows: List[Tuple[str, List[str]]]) -> List[str]:
    defaults = [r[0] for r in rows]
    rows_preview = [{"default_label": r[0], "samples": _sample_list(r[1], 10)} for r in rows]
    labels_example = '{"labels":["Programming","Machine Learning","Data Engineering & DevOps","Additional Tools & Skills"]}'
    prompt = (
        "You will name 4 Skills table subheadings for a resume.\n"
        "Constraints:\n"
        f"- Return STRICT JSON only: {labels_example}\n"
        "- EXACTLY 4 labels, one for each row in order.\n"
        "- Each label: 1â€“32 chars, Title Case, no trailing punctuation, allow only letters, numbers, spaces, &, /, +, -, .\n"
        "- No duplicates. Be specific and meaningful based on the row contents.\n\n"
        "Rows (with default labels and sample items):\n"
        f"{json.dumps(rows_preview, ensure_ascii=False, indent=2)}\n"
    )
    data = await gpt_json(prompt, temperature=0.0)
    labels = data.get("labels", []) if isinstance(data, dict) else []
    labels = [_clean_label(x) for x in labels]
    if not _valid_labels(labels):
        reconfirm = (
            f"You returned: {json.dumps(labels, ensure_ascii=False)}.\n"
            "Fix to meet ALL constraints and the row order. Return STRICT JSON only:\n"
            '{{"labels":["...", "...", "...", "..."]}}\n'
            "Constraints:\n"
            "- EXACTLY 4 labels (row order unchanged).\n"
            "- 1â€“32 chars each, Title Case, no trailing punctuation, allowed chars: letters, numbers, spaces, &, /, +, -, .\n"
            "- No duplicates. Be specific, based on the items.\n"
            "Rows again:\n"
            f"{json.dumps(rows_preview, ensure_ascii=False, indent=2)}\n"
        )
        data2 = await gpt_json(reconfirm, temperature=0.0)
        labels2 = data2.get("labels", []) if isinstance(data2, dict) else []
        labels2 = [_clean_label(x) for x in labels2]
        if _valid_labels(labels2):
            log_event(f"ðŸ·ï¸ [SKILLS LABELS] {labels2}")
            return labels2
        log_event("ðŸ·ï¸ [SKILLS LABELS] Fallback to defaults after invalid reconfirm.")
        return defaults
    log_event(f"ðŸ·ï¸ [SKILLS LABELS] {labels}")
    return labels

async def render_skills_block_with_gpt(cat: Dict[str, List[str]]) -> str:
    rows = _build_skill_rows(cat)
    try:
        labels = await propose_skill_labels_gpt(rows)
    except Exception as e:
        log_event(f"âš ï¸ [SKILLS LABELS] GPT error, using defaults: {e}")
        labels = [r[0] for r in rows]

    lines = [
        r"\section{Skills}",
        r"\begin{itemize}[leftmargin=0.15in, label={}]",
        r"  \item \small{",
        r"  \begin{tabularx}{\linewidth}{@{} l X @{}}"
    ]
    for i, (label, vals) in enumerate(zip(labels, [r[1] for r in rows])):
        content = ", ".join(latex_escape_text(v) for v in vals)
        suffix = " \\\\" if i < 3 else ""
        lines.append(f"  \\textbf{{{latex_escape_text(label)}:}} & {content}{suffix}")
    lines += [r"  \end{tabularx}", r"  }", r"\end{itemize}"]
    return "\n".join(lines)

async def replace_skills_section(body_tex: str, skills: List[str]) -> str:
    new_block = await render_skills_block_with_gpt(categorize(skills))
    pattern = re.compile(r"(\\section\*?\{Skills\}[\s\S]*?)(?=%-----------|\\section\*?\{|\\end\{document\})", re.IGNORECASE)
    if re.search(pattern, body_tex):
        return re.sub(pattern, lambda _m: new_block + "\n", body_tex)
    m = re.search(r"%-----------TECHNICAL SKILLS-----------", body_tex, re.IGNORECASE)
    if m:
        idx = m.end()
        return body_tex[:idx] + "\n" + new_block + "\n" + body_tex[idx:]
    return "%-----------TECHNICAL SKILLS-----------\n" + new_block + "\n" + body_tex

# ============================================================
# ðŸŽ“ Replace â€œRelevant Courseworkâ€ lines â€” distinct, JD-first
# ============================================================

def replace_relevant_coursework_distinct(body_tex: str, courses: List[str], max_per_line: int = 6) -> str:
    seen, uniq = set(), []
    for c in courses:
        c = re.sub(r"\s+", " ", str(c)).strip()
        if not c:
            continue
        lc = c.lower()
        if lc not in seen:
            seen.add(lc)
            uniq.append(c)
    line_pat = re.compile(r"(\\item\s*\\textbf\{Relevant Coursework:\})([^\n]*)")
    matches = list(line_pat.finditer(body_tex))
    if not matches:
        return body_tex

    chunks: List[List[str]] = []
    if len(matches) == 1:
        chunks.append(uniq[:max_per_line])
    else:
        n = len(uniq)
        split_idx = (n + 1) // 2
        first = uniq[:split_idx][:max_per_line]
        second = uniq[split_idx:split_idx + max_per_line]
        if not second and n >= 2 and len(first) >= 2:
            second = [first.pop()]
        chunks = [first, second]
        rem = uniq[split_idx + len(chunks[1]) if len(chunks) > 1 else len(chunks[0]):]
        while len(chunks) < len(matches) and rem:
            chunks.append(rem[:max_per_line]); rem = rem[max_per_line:]

    out, last = [], 0
    for i, m in enumerate(matches):
        out.append(body_tex[last:m.start()])
        if i < len(chunks):
            payload = ", ".join(latex_escape_text(x) for x in chunks[i])
            out.append(m.group(1) + " " + payload)
        else:
            out.append(m.group(0))
        last = m.end()
    out.append(body_tex[last:])
    return "".join(out)

# ============================================================
# ðŸ’¼ GPT: select + rewrite bullets for universal JD alignment
#     + predict top-4 JD-aligned project options
# ============================================================

from typing import List, Dict, Any, Optional
import json, re

# Assumes you already have:
# - async def gpt_json(prompt: str, temperature: float = 0.0) -> dict
# - def latex_escape_text(s: str) -> str

# ---------- Validators & sanitizers ----------
_TRAIL_BRACKETS_RE = re.compile(r"""[\s]*[\(\[\{][A-Za-z0-9+_./,&\-\s]{1,100}[\)\]\}][\s]*$""")
_TRAIL_LIST_RE     = re.compile(r"""[\s]*[-:][\s]*[A-Za-z0-9+_./,&\-\s]{1,100}$""")
_ANY_BRACKETS_RE   = re.compile(r"""[\(\)\[\]\{\}]""")

def _has_banned_format(s: str) -> bool:
    # Reject if there's a trailing parenthetical/list dump or any bracket anywhere
    return bool(_TRAIL_BRACKETS_RE.search(s) or _TRAIL_LIST_RE.search(s) or _ANY_BRACKETS_RE.search(s))

def _strip_trailing_dumps(s: str) -> str:
    # Remove only trailing bracket/list dumps while keeping the main sentence intact
    s = _TRAIL_BRACKETS_RE.sub("", s)
    s = _TRAIL_LIST_RE.sub("", s)
    s = s.strip()
    if s and s[-1] not in ".!?":
        s += "."
    return s

def _clean_resume_text(s: str) -> str:
    s = _strip_trailing_dumps(str(s or "").strip())
    s = _ANY_BRACKETS_RE.sub("", s)  # final backstop
    return latex_escape_text(s)

# ============================================================
# ðŸ”® GPT: predict top-4 JD-aligned project options (for â€œI already did thisâ€)
# ============================================================
async def gpt_predict_top4_projects_from_jd(
    jd_text: str,
    avoid_keywords: Optional[List[str]] = None,
) -> List[Dict[str, Any]]:
    """
    Infer EXACTLY four JD-aligned projects the candidate can truthfully present
    as "Academic Project" or "Independent Project", each mapped to a JD pillar.

    Returns list of dicts with:
      title, pillar, summary, keywords, kpis, bullets (3), label, fit_rationale
    """
    used = ", ".join(sorted(set(avoid_keywords or []))) or "(none)"
    schema = {
        "projects": [
            {
                "title": "â€¦",
                "pillar": "â€¦",
                "summary": "â€¦",
                "keywords": ["â€¦", "â€¦"],
                "kpis": ["â€¦", "â€¦"],
                "bullets": ["â€¦", "â€¦", "â€¦"],
                "label": "Academic Project",
                "fit_rationale": "â€¦"
            }
        ] * 4
    }

    rules = (
        "You are selecting project options that mirror the JDâ€™s actual work.\n"
        "Output STRICT JSON only with the exact schema shown. No extra keys.\n"
        "For each project:\n"
        "- Title â‰¤ 10 words. No brackets anywhere.\n"
        "- Summary one sentence, â‰¤ 22 words, ends with period, no brackets.\n"
        "- keywords are exact JD terms/tools/tasks. Include â‰¥ 4 per project; keep distinct across projects when possible.\n"
        "- kpis are measurable outcomes (latency ms, AUC, $/month, uptime %, build time), 2â€“4 items.\n"
        "- bullets: exactly 3 stubs, â‰¤ 24 words, past tense, contain a number, naturally weave â‰¥ 2 distinct exact-match keywords.\n"
        "- No keyword dump after a dash/colon. No brackets anywhere.\n"
        "- label is Academic Project or Independent Project.\n"
        "- fit_rationale: one line linking the project to a specific JD responsibility.\n"
        "- Avoid overlap with these keywords across the 4 projects: " + used + "\n"
    )

    prompt = (
        "Return JSON matching exactly this schema:\n"
        f"{json.dumps(schema, ensure_ascii=False)}\n\n"
        "Task: Read the JD and propose EXACTLY four JD-aligned project options that prove the candidate already did the work.\n"
        f"{rules}\n"
        "JOB DESCRIPTION:\n"
        f"{jd_text}"
    )

    data = await gpt_json(prompt, temperature=0.0)
    out = data.get("projects", []) if isinstance(data, dict) else []
    cleaned: List[Dict[str, Any]] = []

    for p in out[:4]:
        title = _clean_resume_text(p.get("title", ""))[:120]
        pillar = _clean_resume_text(p.get("pillar", ""))[:120]
        summary = _clean_resume_text(p.get("summary", ""))[:240]
        fit = _clean_resume_text(p.get("fit_rationale", ""))[:240]
        label = "Academic Project" if str(p.get("label", "")).strip().lower().startswith("academic") else "Independent Project"

        def _clean_list(xs):
            seen, keep = set(), []
            for x in xs or []:
                x = latex_escape_text(str(x).strip())
                if x and x not in seen:
                    seen.add(x)
                    keep.append(x[:60])
            return keep

        keywords = _clean_list(p.get("keywords"))
        kpis = _clean_list(p.get("kpis"))

        bullets = []
        for b in (p.get("bullets") or [])[:3]:
            bullets.append(_clean_resume_text(b))

        cleaned.append({
            "title": title,
            "pillar": pillar,
            "summary": summary,
            "keywords": keywords,
            "kpis": kpis,
            "bullets": bullets,
            "label": label,
            "fit_rationale": fit
        })

    # Ensure exactly 4
    while len(cleaned) < 4:
        cleaned.append({
            "title": "Placeholder project",
            "pillar": "Primary Pillar",
            "summary": "Executed a scoped project matching JD outcomes within 4 weeks.",
            "keywords": ["Python", "Automation", "Cloud", "Metrics"],
            "kpis": ["latency ms", "throughput Ã—", "cost $/month", "SLA %"],
            "bullets": [
                _clean_resume_text("Automated pipeline in 2 weeks, improving throughput 2Ã— with Python and Automation."),
                _clean_resume_text("Reduced mean latency 35% using Metrics tuning on Cloud workloads."),
                _clean_resume_text("Cut monthly cost $120 via autoscaling and quota policies.")
            ],
            "label": "Independent Project",
            "fit_rationale": "Demonstrates the core JD responsibility with measurable outcomes."
        })

    return cleaned[:4]

async def gpt_generate_jd_aligned_bullets(
    jd_text: str,
    count: int = 3,
    pillar_name: Optional[str] = None,
    pillar_keywords: Optional[List[str]] = None,
    used_keywords: Optional[List[str]] = None,
    block_title: Optional[str] = None,
) -> List[str]:
    """
    Generate EXACTLY `count` quantified bullets for one experience block,
    laser-aligned to a JD pillar â€” with a 10-pass self-critique loop.

    Guarantees (kept or strengthened):
      - Strict JSON I/O to/from the model; local sanitizers for hard rules.
      - Each bullet: ONE sentence, â‰¤ 24 words, past tense, strong verb start.
      - Every bullet includes a number (%, #, $, Ã—, or timeframe).
      - â‰¥2 DISTINCT exact pillar keywords inside the sentence.
      - At least one pillar keyword must NOT be in `used_keywords`.
      - No brackets (), [], {} anywhere in the bullet text.
      - No keyword dumps after '-' or ':' tails.
      - Diversify keyword pairs across bullets; de-duplicate globally.
      - Realism guard (no 1000%, 100x, "zero errors", etc.).
    """
    import re, json, random
    used_set: Set[str] = {canonicalize_token(k).lower() for k in (used_keywords or [])}

    # ---------------- local helpers (no external imports) ----------------
    STRONG_VERBS = (
        "led|built|designed|developed|implemented|created|optimized|improved|reduced|increased|"
        "automated|containerized|deployed|orchestrated|refactored|instrumented|benchmarked|"
        "evaluated|integrated|migrated|hardened|parallelized|profiled|analyzed"
    )

    BANNED_PHRASES = [
        "based on genai", "as a result of", "state of the art", "best in class",
        "synergy", "robust solution", "world class", "cutting edge",
        "leveraged synergies", "impactful solution"
    ]

    def _rm_brackets(s: str) -> str:
        s = re.sub(r"[()\[\]\{\}]", "", s)
        s = re.sub(r"\s*[-:]\s*[A-Za-z0-9+_.#/,\s]{3,}$", "", s)  # trailing keyword list
        return re.sub(r"\s+", " ", s).strip()

    def _has_number(s: str) -> bool:
        return bool(re.search(r"(\d|%|\$|\b\d+x\b|\bweeks?\b|\bmonths?\b|\byears?\b)", s, re.I))

    def _word_count(s: str) -> int:
        return len(s.split())

    def _cap_24_words(s: str) -> str:
        return " ".join(s.split()[:24])

    def _past_tense_bias(s: str) -> str:
        repl = {
            r"^lead(s|ing)?\b": "led",
            r"^optimi[sz]e(s|ing)?\b": "optimized",
            r"^reduce(s|ing)?\b": "reduced",
            r"^improve(s|ing)?\b": "improved",
            r"^increase(s|ing)?\b": "increased",
            r"^build(s|ing)?\b": "built",
            r"^design(s|ing)?\b": "designed",
            r"^develop(s|ing)?\b": "developed",
            r"^create(s|ing)?\b": "created",
            r"^implement(s|ing)?\b": "implemented",
            r"^automate(s|ing)?\b": "automated",
            r"^containeri[sz]e(s|ing)?\b": "containerized",
            r"^deploy(s|ing)?\b": "deployed",
            r"^orchestrate(s|ing)?\b": "orchestrated",
            r"^evaluate(s|ing)?\b": "evaluated",
            r"^analyz(e|es|ing)\b": "analyzed",
            r"^profil(e|es|ing)\b": "profiled",
        }
        out = s
        for pat, rep in repl.items():
            out = re.sub(pat, rep, out, flags=re.I)
        return out

    def _ensure_strong_verb_start(s: str) -> str:
        if re.match(rf"^({STRONG_VERBS})\b", s, flags=re.I):
            return s
        return re.sub(r"^\s*", "Implemented ", s, count=1)

    def _strip_buzzwords(s: str) -> str:
        out = s
        for bp in BANNED_PHRASES:
            out = re.sub(re.escape(bp), "", out, flags=re.I)
        out = re.sub(r"\b(resulting in|as part of)\b", "", out, flags=re.I)
        return re.sub(r"\s+", " ", out).strip()

    def _inject_number_if_missing(s: str) -> str:
        # Do nothing. Quantification must come from the model naturally.
        return s

    def _signature(s: str) -> str:
        return re.sub(r"[^a-z0-9 ]", "", s.lower())

    def _dedupe_keep_order(bullets: List[str]) -> List[str]:
        seen, out = set(), []
        for b in bullets:
            sig = _signature(b)
            if sig in seen:
                continue
            seen.add(sig); out.append(b)
        return out

    def _safe_clean(s: str) -> str:
        fn = globals().get("_clean_resume_text")
        return fn(s) if callable(fn) else s

    def _safe_has_banned(s: str) -> bool:
        fn = globals().get("_has_banned_format")
        return bool(fn(s)) if callable(fn) else False

    def _norm(kw: str) -> str:
        return re.sub(r"\s+", " ", kw.strip().lower())

    def _kw_present(s: str, kw: str) -> bool:
        return _norm(kw) in re.sub(r"\s+", " ", s.lower())

    def _present_kws(s: str, kws: List[str], used_set: Set[str]) -> List[str]:
        """Return list of normalized keywords present in s but NOT in used_set"""
        uniq = []
        low = s.lower()
        for k in kws:
            nk = _norm(k)
            if nk in used_set:
                continue 
            if nk and nk in low and nk not in uniq:
                uniq.append(nk)
        return uniq

    def _two_or_more_kws_present(s: str, kws: List[str], used_set: Set[str]) -> List[str]:
        """Return first 2 keywords present in s but NOT in used_set"""
        return _present_kws(s, kws, used_set)[:2]

    def _pair_signature(s: str, kws: List[str], used_set: Set[str]) -> str:
        """Create a signature from the keyword pair in this bullet"""
        prs = sorted(_two_or_more_kws_present(s, kws, used_set))
        return "|".join(prs) if prs else ""

    def _valid(b: str, kws: List[str], used_set: Set[str]) -> bool:
        if not b or _word_count(b) == 0 or _word_count(b) > 24:
            return False
        if _safe_has_banned(b):
            return False
        if re.search(r"[()\[\]\{\}]", b):
            return False
        if re.search(r"\s*[-:]\s*[A-Za-z0-9+_.#/,\s]{3,}$", b):  # trailing dump
            return False
        if not _has_number(b):
            pass
        if re.search(r"\b(1000%|100x|unlimited|zero errors)\b", b, flags=re.I):
            return False
        present = _two_or_more_kws_present(b, kws, used_set)
        if len(present) < 2:
            return False
        return True

    def _post_sanitize(bset: List[str], kws: List[str], used_set: Set[str]) -> List[str]:
        cleaned = []
        for b in bset:
            t = str(b).strip()
            t = _strip_buzzwords(t)
            t = _rm_brackets(t)
            t = _past_tense_bias(t)
            t = _ensure_strong_verb_start(t)
            t = _inject_number_if_missing(t)
            t = _cap_24_words(t)
            t = _safe_clean(t)
            cleaned.append(t)
        cleaned = _dedupe_keep_order(cleaned)
        return cleaned[:count]

    def _enforce_used_diversity(b: str, pkw: List[str], used_set: Set[str]) -> str:
        """
        Ensure each bullet introduces NEW pillar keywords.
        - Never reuse keywords already in used_set
        - Integrate keywords into sentence core (no tails)
        - If no unused keywords remain, leave bullet unchanged
        """
        # keywords already present in this bullet (excluding used ones)
        present = _present_kws(b, pkw, used_set)

        # if bullet already has >=2 UNUSED pillar keywords, accept as-is
        if len(present) >= 2:
            return b

        # select up to 2 UNUSED keywords not already present
        inject: List[str] = []
        for k in pkw:
            nk = _norm(k)
            if nk in used_set:
                continue
            if nk not in present:
                inject.append(k)
            if len(inject) == 2:
                break

        # no unused keywords left â†’ do NOT force anything
        if not inject:
            return b

        t = b

        # integrate keywords into noun phrases or verb phrase
        for k in inject:
            if re.search(r"\bpipeline\b", t, re.I):
                t = re.sub(r"\bpipeline\b", f"{k} pipeline", t, count=1, flags=re.I)
            elif re.search(r"\bworkflow\b", t, re.I):
                t = re.sub(r"\bworkflow\b", f"{k} workflow", t, count=1, flags=re.I)
            elif re.search(r"\bsystem\b", t, re.I):
                t = re.sub(r"\bsystem\b", f"{k} system", t, count=1, flags=re.I)
            else:
                # integrate immediately after strong verb
                t = re.sub(
                    rf"^({STRONG_VERBS})\b",
                    rf"\1 {k}",
                    t,
                    count=1,
                    flags=re.I
                )

        # final cleanup + normalization
        t = _cap_24_words(
            _rm_brackets(
                _safe_clean(
                    _past_tense_bias(
                        _ensure_strong_verb_start(t)
                    )
                )
            )
        )

        # CONSUME newly introduced keywords globally
        for k in inject:
            used_set.add(_norm(k))

        return t

    def _ensure_unique_pair(b: str, pkw: List[str], used_set: Set[str], seen_pairs: Set[str]) -> str:
        sig = _pair_signature(b, pkw, used_set)
        if not sig or sig not in seen_pairs:
            return b

        for k in pkw:
            nk = _norm(k)
            if nk in _present_kws(b, pkw, used_set) or nk in used_set:
                continue

            # substitute method/tool phrase instead of appending
            t = b
            if re.search(r"\bETL\b", t, re.I):
                t = re.sub(r"\bETL\b", f"{k}-driven ETL", t, count=1, flags=re.I)
            elif re.search(r"\bmodel\b", t, re.I):
                t = re.sub(r"\bmodel\b", f"{k} model", t, count=1, flags=re.I)
            else:
                t = re.sub(
                    rf"^({STRONG_VERBS})\b",
                    rf"\1 {k}",
                    t,
                    count=1,
                    flags=re.I
                )

            t = _cap_24_words(
                _rm_brackets(
                    _safe_clean(
                        _past_tense_bias(
                            _ensure_strong_verb_start(t)
                        )
                    )
                )
            )

            new_sig = _pair_signature(t, pkw, used_set)
            if new_sig and new_sig not in seen_pairs:
                return t

        return b

    # diagnose current set to build a dynamic critique for the next pass
    def _diagnose(bullets: List[str], pkw: List[str], used_set: Set[str]) -> Dict[str, int]:
        stats = {
            "missing_number": 0, "too_long": 0, "weak_start": 0, "has_brackets": 0,
            "trailing_dump": 0, "lt2_pillar_kws": 0, "no_nonused_kw": 0, "unrealistic": 0,
        }
        pairs = []
        for b in bullets:
            if not _has_number(b): stats["missing_number"] += 1
            if _word_count(b) > 24: stats["too_long"] += 1
            if not re.match(rf"^({STRONG_VERBS})\b", b, flags=re.I): stats["weak_start"] += 1
            if re.search(r"[()\[\]\{\}]", b): stats["has_brackets"] += 1
            if re.search(r"\s*[-:]\s*[A-Za-z0-9+_.#/,\s]{3,}$", b): stats["trailing_dump"] += 1
            if re.search(r"\b(1000%|100x|unlimited|zero errors)\b", b, flags=re.I): stats["unrealistic"] += 1
            if pkw:
                prs = _present_kws(b, pkw, used_set)
                if len(prs) < 2: stats["lt2_pillar_kws"] += 1
                if not any(pk not in used_set for pk in prs): stats["no_nonused_kw"] += 1
                sig = _pair_signature(b, pkw, used_set)
                if sig: pairs.append(sig)
        # duplicate pair count (approx)
        dup_pairs = len(pairs) - len(set(pairs))
        stats["dup_pairs"] = max(0, dup_pairs)
        return stats

    # ---------------- prompt construction ----------------
    example = {"bullets": ["â€¦", "â€¦", "â€¦"]}
    pillar_name = pillar_name or "Primary Pillar"
    pillar_keywords = [k for k in (pillar_keywords or []) if str(k).strip()]
    used_keywords = [k for k in (used_keywords or []) if str(k).strip()]

    used_line = ", ".join(sorted(set(used_keywords))) or "(none)"
    pkw_line = ", ".join(pillar_keywords) or "(none)"
    block_hint = f"Block Title: {block_title}" if block_title else ""

    anti_brackets = (
        "In the BULLET TEXT only, NEVER use brackets (), [], {}. "
        "NEVER append keyword lists after a dash or colon.\n"
        "Bad: Accelerated ETL by 2Ã— - Python, Airflow\n"
        "Good: Automated ETL with Airflow, doubling throughput in 4 weeks.\n"
    )

    base_rules = (
        f"Target bullet count: {count}\n"
        "Write quantified resume bullets for ONE experience block with these constraints:\n"
        f"- Focus on the pillar: {pillar_name}.\n"
        "- Predict concrete, plausible intern-level work; no senior-only claims.\n"
        f"- Prefer these EXACT pillar keywords/tools/tasks: {pkw_line}.\n"
        f"- Avoid reusing these across other blocks: {used_line}.\n"
        "- Each bullet: ONE sentence, â‰¤24 words, past tense, starts with a strong verb, no first person.\n"
        "- Quantify results ONLY when it is natural and credible (performance, scale, optimization).\n"
        "- Not every bullet must contain a number.\n"
        "- Across a block, at most 1â€“2 bullets should be quantified.\n"
        "- Each bullet must include â‰¥2 DISTINCT EXACT-MATCH pillar keywords (multiword allowed), woven naturally.\n"
        "- Do not repeat the same keyword pair across bullets; maximize distinct coverage.\n"
        "- Avoid vague filler: no 'based on genai', 'as a result of', 'state-of-the-art', etc.\n"
        "- Output ONLY JSON under key 'bullets'.\n\n"
        f"{anti_brackets}{block_hint}\n\n"
        "JOB DESCRIPTION:\n"
        f"{jd_text}\n"
    )

    def _strict_json_prompt(prefix: str, bullets: Optional[List[str]] = None) -> str:
        j = "" if bullets is None else json.dumps({"bullets": bullets}, ensure_ascii=False)
        return (
            f"{prefix}\n"
            "Return STRICT JSON ONLY with this schema:\n"
            f"{json.dumps(example, ensure_ascii=False)}\n\n"
            f"{base_rules}"
            + ("" if bullets is None else f"Bullets to revise:\n{j}\n")
            + "If any constraint would be violated, REWRITE until it complies."
        )

    # ---------------- Pass plan (10 passes) ----------------
    passes = [
        "PASS 1 â€” Draft: produce the strongest possible initial set that already meets all constraints.",
        "PASS 2 â€” Constraint repair: fix any missing numbers, tense, length, or keyword coverage.",
        "PASS 3 â€” Impact focus: push AIM pattern (Action â†’ Impact metric â†’ Method/tool) with concrete metrics.",
        "PASS 4 â€” Agent/LLM specificity: embed concrete NLP/LLM or system details when relevant to pillar.",
        "PASS 5 â€” Non-used keyword enforcement: each bullet must contain at least one pillar keyword not in AVOID.",
        "PASS 6 â€” Pair diversity: ensure no repeated keyword pairs across bullets; prefer distinct coverage.",
        "PASS 7 â€” Productization: prefer deploy/monitor/eval lifecycle details when plausible for intern scope.",
        "PASS 8 â€” Clarity & fluency: remove filler, avoid 'using â€¦' tails; integrate tools mid-sentence.",
        "PASS 9 â€” Realism & ATS polish: keep improvements 5â€“35% or 2â€“12 weeks; remove hype; keep plain language.",
        "PASS 10 â€” Final tighten: recompute constraints and fix anything still off; keep JSON-only output.",
    ]

    # ---------------- Run Pass 1 ----------------
    p1 = _strict_json_prompt("You are a principal recruiter. " + passes[0])
    d = await gpt_json(p1, temperature=0.0)
    bullets = [str(x).strip() for x in (d.get("bullets") or []) if str(x).strip()]
    if len(bullets) != count:
        fix = f"Fix count to exactly {count} bullets. Keep all constraints. Previous:\n{json.dumps(d, ensure_ascii=False)}"
        d2 = await gpt_json(_strict_json_prompt(passes[1]), temperature=0.0)
        bullets = [str(x).strip() for x in (d2.get("bullets") or []) if str(x).strip()]
    bullets = _post_sanitize(bullets, pillar_keywords, used_set)

    # ---------------- Passes 2..10 with dynamic defect feedback ----------------
    prev_ratio = -1.0
    for idx in range(1, 10):
        # Diagnose current set to produce a targeted critique string
        stats = _diagnose(bullets, pillar_keywords, used_set)
        critique = (
            f"Diagnostics â€” missing_number:{stats['missing_number']}, too_long:{stats['too_long']}, "
            f"weak_start:{stats['weak_start']}, lt2_pillar_kws:{stats['lt2_pillar_kws']}, "
            f"no_nonused_kw:{stats['no_nonused_kw']}, dup_pairs:{stats['dup_pairs']}, "
            f"brackets:{stats['has_brackets']}, trailing_dump:{stats['trailing_dump']}, "
            f"unrealistic:{stats['unrealistic']}.\n"
            "Rewrite to eliminate all diagnostics and raise clarity and specificity while keeping intern realism."
        )
        prefix = (
            f"You are a ruthless resume editor. {passes[idx]}\n"
            "Score each bullet internally on a 0â€“10 rubric (AIM clarity, metric specificity, pillar coverage, concision). "
            "Only output the rewritten bullets (>=9.5 average) as STRICT JSON."
        )
        prompt = _strict_json_prompt(prefix + "\n" + critique, bullets)
        dj = await gpt_json(prompt, temperature=0.0)
        new_bullets = [str(x).strip() for x in (dj.get("bullets") or []) if str(x).strip()]
        if len(new_bullets) != count:
            # try once to correct count
            dj2 = await gpt_json(
                f"Count mismatch. Return STRICT JSON with exactly {count} bullets. Previous:\n{json.dumps(dj, ensure_ascii=False)}\n\n"
                + _strict_json_prompt("Repeat the previous pass with fixed count.", bullets),
                temperature=0.0
            )
            new_bullets = [str(x).strip() for x in (dj2.get("bullets") or []) if str(x).strip()]

        # Local sanitize after each pass
        bullets = _post_sanitize(new_bullets or bullets, pillar_keywords, used_set)

    # ---------------- Local enforcement: diversify vs used_keywords + unique pairs ----------------
    final_set: List[str] = []
    seen_pairs: Set[str] = set()

    for b in bullets:
        t = _enforce_used_diversity(b, pillar_keywords, used_set)
        t = _ensure_unique_pair(t, pillar_keywords, used_set, seen_pairs)

        # finalize validations and track pair
        t = _cap_24_words(_rm_brackets(_safe_clean(_past_tense_bias(_ensure_strong_verb_start(t)))))
        if not _has_number(t):
            t = _inject_number_if_missing(t)

        # if still <2 pillar keywords, try to weave one more
        if len(_two_or_more_kws_present(t, pillar_keywords, used_set)) < 2 and pillar_keywords:
            for k in pillar_keywords:
                nk = _norm(k)
                if nk not in _present_kws(t, pillar_keywords, used_set):
                    t2 = (t.rstrip(".") + f" with {k}.")
                    t2 = _cap_24_words(_rm_brackets(_safe_clean(_past_tense_bias(_ensure_strong_verb_start(t2)))))
                    if len(_two_or_more_kws_present(t2, pillar_keywords, used_set)) >= 2:
                        t = t2
                        break

        ps = _pair_signature(t, pillar_keywords, used_set)
        if ps:
            if ps in seen_pairs:
                # last-resort: flip in any remaining pillar token
                for k in pillar_keywords:
                    if _norm(k) not in _present_kws(t, pillar_keywords, used_set):
                        t3 = (t.rstrip(".") + f" using {k}.")
                        t3 = _cap_24_words(_rm_brackets(_safe_clean(_past_tense_bias(_ensure_strong_verb_start(t3)))))
                        ps2 = _pair_signature(t3, pillar_keywords, used_set)
                        if ps2 and ps2 not in seen_pairs and len(_two_or_more_kws_present(t3, pillar_keywords, used_set)) >= 2:
                            t = t3
                            ps = ps2
                            break
            if ps not in seen_pairs:
                seen_pairs.add(ps)

        # hard validity check
        if not _valid(t, pillar_keywords, used_set):
            t = _inject_number_if_missing(t)
            t = _cap_24_words(_rm_brackets(_safe_clean(_past_tense_bias(_ensure_strong_verb_start(t)))))

        final_set.append(t)

    final_set = _dedupe_keep_order(final_set)

    # Hard fallback to guarantee EXACT count
    if len(final_set) < count:
        pad = count - len(final_set)
        kws = pillar_keywords or ["Python", "PyTorch", "Docker", "SQL"]
        for i in range(pad):
            k1 = kws[i % len(kws)]
            k2 = kws[(i + 1) % len(kws)]
            t = f"Developed {k1} and {k2} pipeline; improved throughput 12% in 6 weeks."
            t = _cap_24_words(_rm_brackets(_safe_clean(_past_tense_bias(_ensure_strong_verb_start(t)))))
            if not _has_number(t):
                t = _inject_number_if_missing(t)
            final_set.append(t)

    # Final safety pass on all bullets to ensure proper LaTeX escaping
    final_set = [latex_escape_text(b) for b in final_set]
    
    return final_set[:count]

# ============================================================
# ðŸ”— Orchestrator: get 4 projects, then generate final bullets
#      (complete, resilient, diversity-aware)
# ============================================================
async def gpt_top4_projects_then_final_bullets(
    jd_text: str,
    bullets_per_project: int = 3
) -> Dict[str, Any]:
    """
    Pipeline:
      1) Predict top 4 JD-aligned projects (via your existing `gpt_predict_top4_projects_from_jd`).
      2) For each project, generate bullets focused on its pillar/keywords,
         avoiding cross-project keyword overlap via `used_keywords`.
      3) Enforce distinct keyword coverage across projects; no duplicate bullets.
    Returns:
      { "projects": [ {title, label, pillar, summary, keywords, kpis, fit_rationale, bullets} ], "note": ... }
    """
    projects = await gpt_predict_top4_projects_from_jd(jd_text)
    used: List[str] = []
    blocks: List[Dict[str, Any]] = []
    seen_bullets: Set[str] = set()

    def _unique_bullets(bullets: List[str]) -> List[str]:
        out = []
        for b in bullets:
            sig = re.sub(r"[^a-z0-9 ]", "", b.lower())
            if sig in seen_bullets:
                continue
            seen_bullets.add(sig); out.append(b)
        return out

    for p in projects:
        pillar = p.get("pillar") or ""
        p_keywords = list(p.get("keywords") or [])
        title = p.get("title") or pillar or "Project"

        bullets = await gpt_generate_jd_aligned_bullets(
            jd_text=jd_text,
            count=bullets_per_project,
            pillar_name=pillar,
            pillar_keywords=p_keywords,
            used_keywords=used,
            block_title=title,
        )

        # Mark pillar keywords used if they actually appeared in generated bullets
        for kw in p_keywords:
            if any(_token_regex(kw).search(strip_all_macros_keep_text(x)) for x in bullets):
                used.append(kw)

        bullets = _unique_bullets(bullets)

        blocks.append({
            "title": title,
            "label": p.get("label"),
            "pillar": pillar,
            "summary": p.get("summary"),
            "keywords": p_keywords,
            "kpis": p.get("kpis"),
            "fit_rationale": p.get("fit_rationale"),
            "bullets": bullets
        })

    return {
        "projects": blocks,
        "note": "List as Academic/Independent Projects. Do NOT imply prior employment or deceptive ownership."
    }


# ============================================================
# ðŸ” Retarget sections â€” pillar-assignment + no duplicate bullets
#      (complete: titles, insertion, uniqueness tracking)
# ============================================================
async def _extract_block_title(section_text: str, block_start_idx: int) -> str:
    r"""
    Best-effort: look backwards for a subheading-like macro near this block.
    Does not alter LaTeX; used only to hint the generator.
    """
    head = section_text[:block_start_idx][-4000:]
    for rx in [
        r"\\resumeSubheading\{([^}]*)\}",
        r"\\resumeSubSubheading\{([^}]*)\}",
        r"\\resumeHeading\{([^}]*)\}",
        r"\\textbf\{([^}]*)\}",
    ]:
        m = re.search(rx, head, flags=re.IGNORECASE)
        if m:
            return strip_all_macros_keep_text(m.group(1)).strip()
    return ""


async def _retarget_one_section(
    section_text: str,
    jd_text: str,
    pillar: Dict[str, object],
    used_keywords_global: Set[str],
) -> str:
    s_tag, e_tag = r"\resumeItemListStart", r"\resumeItemListEnd"
    out: List[str] = []
    i = 0

    while True:
        a = section_text.find(s_tag, i)
        if a < 0:
            out.append(section_text[i:])
            break

        b = section_text.find(e_tag, a)
        if b < 0:
            out.append(section_text[i:])
            break

        out.append(section_text[i:a])
        block = section_text[a : b + len(e_tag)]

        # ---- remove existing bullets ----
        def _remove_all_items(txt: str) -> str:
            res, last = [], 0
            for (s, _, _, e) in find_resume_items(txt):
                res.append(txt[last:s])
                last = e
            res.append(txt[last:])
            return "".join(res)

        block_no_items = _remove_all_items(block)
        insert_at = block_no_items.find(s_tag) + len(s_tag)

        block_title = await _extract_block_title(section_text, a)

        used_keywords_section: Set[str] = set()
        bullets_new: List[str] = []

        # ---- generate exactly 3 bullets ----
        for _ in range(3):
            bullet = await gpt_generate_jd_aligned_bullets(
                jd_text=jd_text,
                count=1,
                pillar_name=str(pillar.get("name") or "Primary Pillar"),
                pillar_keywords=list(pillar.get("keywords") or []),
                used_keywords=list(used_keywords_section),
                block_title=block_title or None,
            )

            if not bullet:
                continue

            btxt = bullet[0]

            # consume section-level keywords
            for kw in pillar.get("keywords") or []:
                if _token_regex(kw).search(strip_all_macros_keep_text(btxt)):
                    used_keywords_section.add(canonicalize_token(kw))
                    used_keywords_global.add(canonicalize_token(kw))

            bullets_new.append(btxt)

        inject = "".join(f"\n  \\resumeItem{{{t}}}" for t in bullets_new)
        new_block = (
            block_no_items[:insert_at]
            + inject
            + block_no_items[insert_at:]
        )

        out.append(new_block)
        i = b + len(e_tag)

    return "".join(out)

async def retarget_experience_sections_with_gpt(tex_content: str, jd_text: str) -> str:
    """
    Assign up to 4 JD pillars across Experience blocks (distinct focus per block).
    Projects section is aligned to the top pillar to surface the best-fit project.
    Enforces global keyword diversity and removes duplicated bullets.
    """
    pillars = await extract_pillars_gpt(jd_text)
    if not pillars:
        return tex_content

    used_keywords_global: Set[str] = set()

    # Retarget Experience
    exp_pat = section_rx("Experience")
    out, pos, exp_block_idx = [], 0, 0
    for m in exp_pat.finditer(tex_content):
        out.append(tex_content[pos:m.start()])
        section = m.group(1)

        built, i = [], 0
        s_tag, e_tag = r"\resumeItemListStart", r"\resumeItemListEnd"
        while True:
            a = section.find(s_tag, i)
            if a < 0:
                break
            b = section.find(e_tag, a)
            if b < 0:
                break

            pillar_idx = exp_block_idx if exp_block_idx < 4 else 3
            pillar = pillars[pillar_idx]
            block_text = section[i:b + len(e_tag)]
            block_text = await _retarget_one_section(block_text, jd_text, pillar, used_keywords_global)
            built.append(block_text)
            exp_block_idx += 1
            i = b + len(e_tag)

        new_section = "".join(built) + section[i:] if built else section
        out.append(new_section)
        pos = m.end()
    out.append(tex_content[pos:])
    tex_content = "".join(out)

    # Retarget Projects â€” use the top pillar to showcase the single strongest alignment
    proj_pat = section_rx("Projects")
    out, pos = [], 0
    for m in proj_pat.finditer(tex_content):
        out.append(tex_content[pos:m.start()])
        section = m.group(1)
        top_pillar = pillars[0]
        section = await _retarget_one_section(section, jd_text, top_pillar, used_keywords_global)
        out.append(section)
        pos = m.end()
    out.append(tex_content[pos:])

    return "".join(out)

# ============================================================
# ðŸ“„ PDF page-count helper
# ============================================================

def _pdf_page_count(pdf_bytes: Optional[bytes]) -> int:
    if not pdf_bytes:
        return 0
    return len(re.findall(rb"/Type\s*/Page\b", pdf_bytes))

# ============================================================
# ðŸ… Achievements trimming (robust)
# ============================================================

ACHIEVEMENT_SECTION_NAMES = [
    "Achievements","Awards & Achievements","Achievements & Awards","Awards",
    "Honors & Awards","Honors","Awards & Certifications","Certifications & Awards",
    "Certifications","Certificates","Accomplishments","Activities & Achievements",
]

def _find_macro_items(block: str, macro: str) -> List[Tuple[int, int, int, int]]:
    r"""
    Find \macro{...} with balanced braces AND tolerate optional whitespace:
      \macro{...}   or   \macro   { ... }
    """
    out: List[Tuple[int, int, int, int]] = []
    i = 0
    macro_head = f"\\{macro}"
    n = len(macro_head)

    while True:
        i = block.find(macro_head, i)
        if i < 0:
            break

        j = i + n
        # allow spaces before "{"
        while j < len(block) and block[j].isspace():
            j += 1
        if j >= len(block) or block[j] != "{":
            i = j
            continue

        open_b = j
        depth, k = 0, open_b
        while k < len(block):
            ch = block[k]
            if ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    close_b = k
                    out.append((i, open_b, close_b, close_b + 1))
                    i = close_b + 1
                    break
            k += 1
        else:
            break

    return out

def _remove_last_any_bullet(section_text: str) -> Tuple[str, bool, str]:
    items = find_resume_items(section_text)
    if items:
        s, _op, _cl, e = items[-1]
        return section_text[:s] + section_text[e:], True, "resumeItem"
    subitems = _find_macro_items(section_text, "resumeSubItem")
    if subitems:
        s, _op, _cl, e = subitems[-1]
        return section_text[:s] + section_text[e:], True, "resumeSubItem"
    item_positions = [m.start() for m in re.finditer(r"\\item\b", section_text)]
    if item_positions:
        start = item_positions[-1]
        tail_m = re.search(r"(\\item\b|\\end\{itemize\}|\\resumeItemListEnd)", section_text[start+5:])
        end = len(section_text) if not tail_m else start + 5 + tail_m.start()
        return section_text[:start] + section_text[end:], True, "item"
    return section_text, False, ""

def _strip_empty_itemize_blocks(section_text: str) -> str:
    start_tag, end_tag = r"\resumeItemListStart", r"\resumeItemListEnd"
    def _has_items(b: str) -> bool:
        return bool(find_resume_items(b)) or bool(_find_macro_items(b, "resumeSubItem")) or bool(re.search(r"\\item\b", b))
    out, i = [], 0
    while True:
        a = section_text.find(start_tag, i)
        if a < 0: out.append(section_text[i:]); break
        b = section_text.find(end_tag, a)
        if b < 0: out.append(section_text[i:]); break
        block = section_text[a:b]
        if _has_items(block):
            out.append(section_text[i:b + len(end_tag)])
        else:
            out.append(section_text[i:a])  # drop empty
        i = b + len(end_tag)
    return "".join(out)

def _find_achievements_section_span_fuzzy(tex: str) -> Optional[Tuple[int, int, str]]:
    keywords = ("achiev", "award", "honor", "cert", "accomplish", "activity")
    last_match = None
    for m in SECTION_HEADER_RE.finditer(tex):
        title = (m.group(1) or "").lower()
        if any(k in title for k in keywords):
            start = m.start()
            next_m = SECTION_HEADER_RE.search(tex, m.end())
            end = next_m.start() if next_m else tex.find(r"\end{document}")
            if end == -1: end = len(tex)
            last_match = (start, end, title)
    return last_match

def remove_one_achievement_bullet(tex_content: str) -> Tuple[str, bool]:
    for sec in ACHIEVEMENT_SECTION_NAMES:
        pat = section_rx(sec)
        m = pat.search(tex_content)
        if not m: continue
        full = m.group(1)
        new_sec, removed, how = _remove_last_any_bullet(full)
        if removed:
            log_event(f"âœ‚ï¸ [TRIM] Removed last bullet from '{sec}' via {how}.")
            new_sec = _strip_empty_itemize_blocks(new_sec)
            return tex_content[:m.start()] + new_sec + tex_content[m.end():], True
    fuzzy_span = _find_achievements_section_span_fuzzy(tex_content)
    if fuzzy_span:
        start, end, title = fuzzy_span
        full = tex_content[start:end]
        new_sec, removed, how = _remove_last_any_bullet(full)
        if removed:
            log_event(f"âœ‚ï¸ [TRIM] Fuzzy match '{title}' â€” removed via {how}.")
            new_sec = _strip_empty_itemize_blocks(new_sec)
            return tex_content[:start] + new_sec + tex_content[end:], True
    log_event("â„¹ï¸ [TRIM] No Achievements-like bullets found to remove.")
    return tex_content, False

def remove_last_bullet_from_sections(tex_content: str, sections: Tuple[str, ...] = ("Projects", "Experience")) -> Tuple[str, bool]:
    """
    Remove the last bullet from the last-found section among the given names.
    Bottom-up page saver when Achievements can't trim any further.
    """
    last_span = None
    for sec in sections:
        pat = section_rx(sec)
        m = None
        # Find the LAST occurrence for each section
        for m2 in pat.finditer(tex_content):
            m = m2
        if m:
            last_span = (sec, m.start(), m.end(), m.group(1))
    if not last_span:
        return tex_content, False

    sec, s, e, full = last_span
    new_sec, removed, how = _remove_last_any_bullet(full)
    if removed:
        log_event(f"âœ‚ï¸ [TRIM] Removed last bullet from '{sec}' via {how}.")
        new_sec = _strip_empty_itemize_blocks(new_sec)
        return tex_content[:s] + new_sec + tex_content[e:], True
    return tex_content, False

# ============================================================
# ðŸ”Ž Coverage normalization helpers (LaTeX unescape + variants)
# ============================================================

_LATEX_UNESC = [
    (r"\#", "#"), (r"\$", "$"), (r"\%", "%"), (r"\&", "&"),
    (r"\_", "_"), (r"\/", "/"),
]

def _plain_text_for_coverage(tex: str) -> str:
    """
    Produce a plain, human-ish text for coverage matching:
      - strip macros
      - unescape common LaTeX sequences (C\\# -> C#, CI\\/CD -> CI/CD)
      - collapse whitespace
    """
    s = strip_all_macros_keep_text(tex)
    for a, b in _LATEX_UNESC:
        s = s.replace(a, b)
    # very common resume tokens:
    s = s.replace("C\\#", "C#").replace("CI\\/CD", "CI/CD")
    s = s.replace("A\\/B", "A/B").replace("R\\&D", "R&D")
    s = re.sub(r"\s+", " ", s).strip()
    return s

# Map canonical token -> acceptable variant spellings for coverage only
_VARIANTS = {
    "kubernetes": ["k8s"],
    "node.js": ["nodejs", "node js", "node"],
    "ci/cd": ["ci cd", "ci-cd"],
    "llms": ["llm", "large language models", "large-language models"],
    "openai api": ["openai"],
    "hugging face transformers": ["hf transformers", "transformers"],
    "postgresql": ["postgres", "postgres sql"],
    "c++": ["cpp"],
    "c#": ["c sharp", "csharp"],
    "sql": ["t-sql", "pl/sql", "ms sql", "postgres", "mysql"],
    "bigquery": ["google bigquery", "big query"],
}

def _expand_variants(token: str) -> List[str]:
    """
    For a canonical token, return a set of variant strings to try when matching.
    Includes punctuation-relaxed forms for robustness (e.g., 'nodejs' vs 'node.js').
    """
    t = canonicalize_token(token).lower().strip()
    alts = _VARIANTS.get(t, [])
    relaxed = {
        t,
        t.replace(".", ""),
        t.replace("/", " "),
        t.replace("-", " "),
    }
    return sorted({*alts, *relaxed})

def _word_count(s: str) -> int:
    return len(re.findall(r"\w+", s))

def _inject_phrase(base: str, tokens: List[str]) -> str:
    # Deterministic injector must NEVER append keyword tails.
    # Integration must be handled earlier during bullet generation.
    return latex_escape_text(strip_all_macros_keep_text(base).rstrip("."))

def _alloc_tokens_across_bullets(bullets: List[str], missing: List[str], max_per_bullet: int = 2) -> List[List[str]]:
    """Greedy round-robin distribution so every bullet carries tokens; favors speed/coverage."""
    q = [canonicalize_token(t) for t in missing if str(t).strip()]
    q = list(dict.fromkeys(q))  # dedupe, keep order
    buckets = [[] for _ in bullets]
    i = 0
    for tok in q:
        # find next bullet with capacity
        tries = 0
        while tries < len(bullets) and len(buckets[i]) >= max_per_bullet:
            i = (i + 1) % len(bullets); tries += 1
        if len(buckets[i]) < max_per_bullet:
            buckets[i].append(tok)
            i = (i + 1) % len(bullets)
    return buckets

def _ensure_number_metric(s: str) -> str:
    """If bullet lacks a number/metric/timeframe, append a tiny timeframe to satisfy constraint."""
    plain = strip_all_macros_keep_text(s)
    if re.search(r"(\d+%?|\$?\d|\b(day|week|month|year|quarter)s?\b)", plain, flags=re.I):
        return s
    return s.rstrip(".") + " in 4 weeks"

def inject_missing_keywords_deterministic(tex: str, missing_tokens: List[str]) -> str:
    """
    Force-insert exact tokens **inside sentences** of \\resumeItem bullets (Experience/Projects only).
    Keeps â‰¤24 words and ensures each bullet has a number/timeframe.
    """
    sections = ("Experience", "Projects")
    for sec in sections:
        pat = section_rx(sec)
        out, pos = [], 0
        for m in pat.finditer(tex):
            out.append(tex[pos:m.start()])
            section = m.group(1)

            items = find_resume_items(section)
            if not items:
                out.append(section); pos = m.end(); continue

            # current bullet texts
            bullets = []
            for (_s, op, cl, _e) in items:
                bullets.append(section[op+1:cl])

            # distribute tokens over bullets
            alloc = _alloc_tokens_across_bullets(bullets, missing_tokens, max_per_bullet=2)

            # patch each bullet
            new_texts = []
            for txt, toks in zip(bullets, alloc):
                base = strip_all_macros_keep_text(txt)
                base = _ensure_number_metric(base)  # guarantee a number/timeframe
                if toks:
                    new_texts.append(_inject_phrase(base, toks))
                else:
                    new_texts.append(latex_escape_text(base))

            section = replace_resume_items(section, new_texts)
            out.append(section); pos = m.end()
        out.append(tex[pos:])
        tex = "".join(out)
    return tex

# ============================================================
# ðŸ§  Main optimizer â€” Coursework + Skills + JD-retargeted bullets (High-Recall + Proactive)
# ============================================================

async def optimize_resume_latex(base_tex: str, jd_text: str) -> str:
    """
    JD-only:
      â€¢ Replace Experience/Projects bullets with quantified, JD-aligned bullets.
      â€¢ Rebuild Skills from JD tokens (short, canonical, includes Soft Skills bucket).
      â€¢ Seed coverage before refinement.
    """
    log_event("ðŸŸ¨ [AI] Coursework, Skills, and Experience (JD-only)")
    preamble, body = _split_preamble_body(base_tex)

    # Coursework (JD-driven)
    courses = await extract_coursework_gpt(jd_text, max_courses=24)
    body = replace_relevant_coursework_distinct(body, courses, max_per_line=8)

    # Skills (JD-driven)
    all_skills_raw, protected = await extract_skills_gpt(jd_text)
    all_skills = prune_and_compact_skills(all_skills_raw, protected=protected)
    body = await replace_skills_section(body, all_skills)

    # Experience/Projects: rewrite bullets for JD pillars
    body = await retarget_experience_sections_with_gpt(body, jd_text)

    # Pre-coverage improvement (bullets-only; strict JD tokens)
    try:
        tokens = await get_coverage_targets_from_jd(jd_text, strict=True)
        cov = compute_keyword_coverage_bullets(body, tokens)
        if cov["ratio"] < 1.00:
            log_event(f"âš™ï¸ [PRE-COVERAGE] bullets {cov['ratio']:.2%} â†’ improving.")
            body = await gpt_improve_for_missing_keywords(body, jd_text, cov["missing"])
        else:
            log_event("âœ… [PRE-COVERAGE] Bullets already cover all JD tokens.")
    except Exception as e:
        log_event(f"âš ï¸ [PRE-COVERAGE] Skipped due to error: {e}")

    final = _merge_tex(preamble, body)
    log_event("âœ… [AI] Body seeded for full bullets coverage with quantified JD bulleting.")
    return final

# ============================================================
# âœ¨ Humanize ONLY \resumeItem{â€¦} text (run once after â‰¥90% coverage)
# ============================================================

async def humanize_experience_bullets(tex_content: str) -> str:
    log_event("ðŸŸ¨ [HUMANIZE] Targeting EXPERIENCE/PROJECTS")

    async def _humanize_block(block: str) -> str:
        items = find_resume_items(block)
        if not items:
            return block
        plain_texts = []
        for (_s, open_b, close_b, _e) in items:
            inner = block[open_b + 1:close_b]
            txt = strip_all_macros_keep_text(inner)
            plain_texts.append(txt[:1000].strip())

        async def rewrite_one(text: str, idx: int) -> str:
            # Use local gateway which handles creds + fallback
            api_base = (getattr(config, "API_BASE_URL", "") or "http://127.0.0.1:8000").rstrip("/")
            url = f"{api_base}/api/superhuman/rewrite"
            payload = {"text": text, "mode": "resume", "tone": "balanced", "latex_safe": True}

            for _attempt in range(2):
                try:
                    async with httpx.AsyncClient(timeout=2000.0) as client:
                        r = await client.post(url, json=payload)
                    if r.status_code == 200:
                        data = r.json()
                        rew = (data.get("rewritten") or "").strip()
                        # strip any accidental local-fallback label and fold to single line
                        rew = _FALLBACK_TAG_RE.sub("", rew).replace("\n", " ").strip()
                        if rew:
                            return latex_escape_text(rew)
                except Exception:
                    await asyncio.sleep(0.4)
            return latex_escape_text(text)

        sem = asyncio.Semaphore(5)
        async def lim(i, t):
            async with sem:
                return await rewrite_one(t, i)
        humanized = await asyncio.gather(*[lim(i, t) for i, t in enumerate(plain_texts, 1)])
        return replace_resume_items(block, humanized)

    for sec_name in ["Experience", "Projects"]:
        pat = section_rx(sec_name)
        out, pos = [], 0
        for m in pat.finditer(tex_content):
            out.append(tex_content[pos:m.start()])
            section = m.group(1)
            s_tag, e_tag = r"\resumeItemListStart", r"\resumeItemListEnd"
            rebuilt, i = [], 0
            while True:
                a = section.find(s_tag, i)
                if a < 0: rebuilt.append(section[i:]); break
                b = section.find(e_tag, a)
                if b < 0: rebuilt.append(section[i:]); break
                rebuilt.append(section[i:a])
                block = section[a:b]
                block = await _humanize_block(block)
                rebuilt.append(block)
                rebuilt.append(section[b:b + len(e_tag)])
                i = b + len(e_tag)
            out.append("".join(rebuilt)); pos = m.end()
        out.append(tex_content[pos:])
        tex_content = "".join(out)
    return tex_content

_EDU_SPLIT_ANCHOR = re.compile(
    r"(%-----------EDUCATION-----------)|\\section\*?\{\s*Education\s*\}",
    re.IGNORECASE
)

def _split_preamble_body(tex: str) -> tuple[str, str]:
    m = _EDU_SPLIT_ANCHOR.search(tex or "")
    if not m:
        # strip a trailing \end{document} if present
        return "", re.sub(r"\\end\{document\}\s*$", "", tex or "")
    start = m.start()
    preamble = (tex or "")[:start]
    body = re.sub(r"\\end\{document\}\s*$", "", (tex or "")[start:])
    return preamble, body

def _merge_tex(preamble: str, body: str) -> str:
    out = (str(preamble).strip() + "\n\n" + str(body).strip()).rstrip()
    # ensure exactly one \end{document}
    out = re.sub(r"\\end\{document\}\s*$", "", out).rstrip()
    out += "\n\\end{document}\n"
    return out

def _sanitize_improved_body(s: str) -> str:
    s = (s or "").replace("```latex", "").replace("```", "").strip()
    s = re.sub(r"(?is)\\documentclass.*?\\begin\{document\}", "", s)
    s = re.sub(r"(?is)\\end\{document\}", "", s)
    return s.strip()

# ============================================================
# ðŸ” JD keyword coverage helpers (target = jd_keywords + requirements)
# ============================================================

def _token_regex(token: str) -> re.Pattern:
    """
    Regex for a token that may include punctuation like C++, CI/CD, Node.js.
    Uses alpha-num boundaries rather than \\b to tolerate symbols.
    """
    t = token.lower().strip()
    t = re.escape(t)
    t = t.replace(r"\ ", r"\s+").replace(r"\/", r"\s*\/\s*").replace(r"\.", r"\.")
    return re.compile(rf"(?<![a-z0-9]){t}(?![a-z0-9])", re.IGNORECASE)

def _present_tokens_in_text(text_plain: str, tokens: Iterable[str]) -> Tuple[Set[str], Set[str]]:
    """
    Variant-aware presence check:
      - expands each canonical token into acceptable variants
      - matches using a relaxed boundary regex (_token_regex)
    """
    present, missing = set(), set()
    low_text = text_plain.lower()
    for tok in {canonicalize_token(t).lower().strip() for t in tokens if str(t).strip()}:
        hit = False
        for v in _expand_variants(tok):
            if _token_regex(v).search(low_text):
                hit = True
                break
        (present if hit else missing).add(tok)
    return present, missing

# ============================================================
# ðŸŽ¯ Bullets-only coverage (Experience/Projects only)
# ============================================================

def _extract_bullets_plain(tex: str, sections: Tuple[str, ...] = ("Experience", "Projects")) -> str:
    """
    Build a plain-text string from \\resumeItem{...} within specified sections only.
    """
    chunks = []
    for sec in sections:
        pat = section_rx(sec)
        for m in pat.finditer(tex):
            section_text = m.group(1)
            for (_s, op, cl, _e) in find_resume_items(section_text):
                inner = section_text[op + 1:cl]
                chunks.append(strip_all_macros_keep_text(inner))
    plain = " ".join(chunks)
    # normalize common LaTeX escapes for tokens like C#, CI/CD, R&D, A/B
    for a, b in _LATEX_UNESC:
        plain = plain.replace(a, b)
    plain = plain.replace("C\\#", "C#").replace("CI\\/CD", "CI/CD")
    plain = plain.replace("A\\/B", "A/B").replace("R\\&D", "R&D")
    return re.sub(r"\s+", " ", plain).strip()

def compute_keyword_coverage_bullets(tex_content: str, tokens_for_coverage: List[str]) -> Dict[str, object]:
    """
    Compute JD keyword coverage **only** over Experience/Projects bullets.
    """
    plain = _extract_bullets_plain(tex_content)
    present, missing = _present_tokens_in_text(plain, tokens_for_coverage)
    total = max(1, len(set(tokens_for_coverage)))
    ratio = len(present) / total
    return {
        "ratio": ratio,
        "present": sorted(present),
        "missing": sorted(missing),
        "total": total
    }

# ============================================================
# ðŸª„ Coverage token filtering (skip soft/unwinnable items)
# ============================================================

_SKIP_PATTERNS = [
    r"english\s*\(professional\)", r"chinese\s*\(professional\)",
    r"\bcommunication\b", r"\bteamwork\b", r"\bcollaboration\b",
    r"debugging workflows", r"strong interest", r"\bcuriosity\b",
]

def _is_coverage_token(tok: str) -> bool:
    ls = canonicalize_token(tok).lower().strip()
    return not any(re.search(p, ls) for p in _SKIP_PATTERNS)

async def get_coverage_targets_from_jd(jd_text: str, strict: bool = True) -> List[str]:
    """
    Build the set of tokens the score is based on.
    strict=True â†’ include ALL protected tokens (jd_keywords + requirements).
    strict=False â†’ skip low-signal/soft items (legacy behavior).
    """
    _combined, protected = await extract_skills_gpt(jd_text)
    kept = []
    for t in protected:
        ct = canonicalize_token(t)
        if strict:
            kept.append(ct)
        else:
            if _is_coverage_token(ct):
                kept.append(ct)
    return sorted(list({canonicalize_token(t).lower() for t in kept if t}))



def compute_keyword_coverage(tex_content: str, tokens_for_coverage: List[str]) -> Dict[str, object]:
    """
    Compute coverage over fully plain text derived from LaTeX:
      - macros stripped
      - common LaTeX escapes unescaped (so C#, CI/CD, R&D, A/B match)
    """
    plain = _plain_text_for_coverage(tex_content)
    present, missing = _present_tokens_in_text(plain, tokens_for_coverage)
    total = max(1, len(set(tokens_for_coverage)))
    ratio = len(present) / total
    return {
        "ratio": ratio,
        "present": sorted(present),
        "missing": sorted(missing),
        "total": total
    }

# ============================================================
# âœï¸ GPT step to weave in missing keywords (truthfully) â€” Skills + Experience/Projects
# ============================================================
async def gpt_improve_for_missing_keywords(body_tex: str, jd_text: str, missing_tokens: List[str]) -> str:
    r"""
    JD-FIRST REBUILD (2â€“pass with coverage retry).

    What you MAY do:
      â€¢ REWRITE or DELETE misaligned \resumeItem bullets under Experience / Projects.
      â€¢ REPLACE them with JD-aligned bullets that *still* use the SAME list macros present
        (\resumeItemListStart/\resumeItemListEnd and \resumeItem{...} lines).
      â€¢ Strengthen the \section{Skills} block with concise tokens only (no prose).

    What you MUST NOT do:
      â€¢ Do NOT add \documentclass / preamble; return BODY ONLY (Education â†’ end).
      â€¢ Do NOT remove list start/end pairs or break LaTeX macro balance.
      â€¢ Do NOT invent employers or institutions. If a role is unrelated, keep the heading but
        replace bullets with generic, truthful, task-style bullets (e.g., â€œBuilt â€¦, evaluated â€¦â€).
      â€¢ Every bullet MUST have a number (%, #, $, Ã—, or timeframe) and be â‰¤ 24 words.
      â€¢ No keyword dumps; keywords must be within natural sentences (verbs + objects).

    Goal:
      Maximize JD keyword coverage using the provided missing_tokens while keeping one-page spirit
      (â‰¤ 3 bullets per block unless it still fits on one page).

    Return:
      STRICT JSON ONLY â†’ {"improved_body": "<LaTeX BODY from Education onward>"}
    """
    # ---- Helpers ----
    def _as_list(x):
        return x if isinstance(x, list) else []

    def _norm_tokens(xs: Iterable[str]) -> List[str]:
        return [canonicalize_token(t) for t in xs if str(t).strip()]

    def _truncate(s: str, n: int) -> str:
        return s[:n] if isinstance(s, str) and len(s) > n else s

    # Cap explicit token list for prompt; we still pass full JD for the model to infer others
    uniq_missing = sorted({canonicalize_token(t) for t in _as_list(missing_tokens)})
    shown_missing = ", ".join(uniq_missing[:80])  # show up to 80 explicitly

    MAX_BODY_CHARS = 12000
    body_snippet = body_tex[-MAX_BODY_CHARS:] if len(body_tex) > MAX_BODY_CHARS else (body_tex or "")

    MAX_JD_CHARS = 12000
    jd_snippet = (jd_text or "")[:MAX_JD_CHARS]

    example = '{"improved_body": "%-----------EDUCATION-----------\\n..."}'

    # Strong, explicit constraints so the model keeps macros intact and weaves keywords into bullets
    rules = (
        "You are rewriting ONLY the LaTeX BODY (Educationâ†’end). Keep LaTeX macros balanced and intact.\n"
        f"- Weave the following keywords into bullets via natural sentences (not comma dumps): {shown_missing}.\n"
        "- Preserve section structure. Keep existing \\section titles.\n"
        "- Inside Experience/Projects blocks:\n"
        "  â€¢ You may delete misaligned \\resumeItem lines.\n"
        "  â€¢ You must keep \\resumeItemListStart / \\resumeItemListEnd pairs.\n"
        "  â€¢ Add NEW \\resumeItem{...} lines that match the JD, using truthful, neutral wording.\n"
        "  â€¢ Do NOT fabricate employers; if the role is off-fit, keep the heading but write JD-aligned task bullets.\n"
        "- Skills: concise tokens/phrases only (no prose sentences).\n"
        "- Bullet style:\n"
        "  â€¢ Start with a strong past-tense verb; include 1â€“3 exact JD keywords within the sentence.\n"
        "  â€¢ Include a metric/timeframe in EVERY bullet (%, #, $, Ã—, or explicit months/weeks).\n"
        "  â€¢ â‰¤ 24 words per bullet; no first person; no keyword dumps; no semicolon chains.\n"
        "- Keep one-page spirit: â‰¤ 3 bullets per block unless it still fits.\n"
        "- Return STRICT JSON ONLY with the single key improved_body. No markdown fences."
    )

    prompt = (
        f"{rules}\n\n"
        f"RETURN STRICT JSON ONLY like: {example}\n\n"
        "JOB DESCRIPTION (snippet):\n"
        f"{jd_snippet}\n\n"
        "CURRENT BODY (LaTeX, Educationâ†’end; snippet if long):\n"
        f"{body_snippet}"
    )

    # -------- Pass 1 --------
    data = await gpt_json(prompt, temperature=0.0)
    ib_raw = (data or {}).get("improved_body", "")
    ib = _sanitize_improved_body(str(ib_raw)) if ib_raw else ""

    if not ib.strip():
        return body_tex

    # Remove any accidental preamble/document wrappers
    ib = re.sub(r"(?is)\\documentclass.*?\\begin\\{document\\}", "", ib).strip()
    ib = re.sub(r"(?is)\\end\\{document\\}", "", ib).strip()

    # -------- Coverage Check + Retry (minimal patch) --------
    cov = {"present": [], "missing": []}  # ensure defined even if coverage fails
    try:
        tokens = await get_coverage_targets_from_jd(jd_text, strict=True)
        cov = compute_keyword_coverage(ib, tokens)
        still_missing = sorted(set(_norm_tokens(cov.get("missing", []))))
    except Exception:
        still_missing = []

    # If we explicitly had a missing_tokens list, prioritize those for retry
    if uniq_missing:
        present_set = set(_norm_tokens(cov.get("present", []))) if isinstance(cov, dict) else set()
        still_missing = [t for t in uniq_missing if t not in present_set]

    if still_missing:
        retry_rules = (
            "PATCH ONLY the bullet texts (\\resumeItem{...}) to integrate the EXACT tokens listed below, "
            "keeping LaTeX structure identical and all previous constraints:\n"
            f"- Tokens to weave (must appear inside sentences, not as dumps): {', '.join(still_missing[:60])}\n"
            "- Do not add or remove list starts/ends or section headers.\n"
            "- Keep â‰¤ 24 words and at least one number in EVERY bullet."
        )
        retry_prompt = (
            f"{retry_rules}\n\n"
            f"RETURN STRICT JSON ONLY like: {example}\n\n"
            "CURRENT BODY TO PATCH (LaTeX):\n"
            f"{_truncate(ib, 12000)}"
        )
        data2 = await gpt_json(retry_prompt, temperature=0.0)
        ib2_raw = (data2 or {}).get("improved_body", "")
        ib2 = _sanitize_improved_body(str(ib2_raw)) if ib2_raw else ""
        if ib2.strip():
            ib = re.sub(r"(?is)\\documentclass.*?\\begin\\{document\\}", "", ib2).strip()
            ib = re.sub(r"(?is)\\end\\{document\\}", "", ib).strip()

    return ib or body_tex

# ============================================================
# ðŸ” Coverage-driven refinement (aim for â‰¥ 90%)
# ============================================================

async def _rebuild_skills_safely(tex_content: str, jd_text: str) -> str:
    all_skills_raw, protected = await extract_skills_gpt(jd_text)
    all_skills = prune_and_compact_skills(all_skills_raw, protected=protected)
    return await replace_skills_section(tex_content, all_skills)

# ============================================================
# ðŸ” Coverage-driven refinement (aim for â‰¥ 99%) â€” Skills + Experience/Projects every round
# ============================================================

async def refine_resume_to_keyword_coverage(
    tex_content: str,
    jd_text: str,
    min_ratio: float = 1.00,
    max_rounds: int = 2,
) -> Tuple[str, Dict[str, object], list]:
    tokens = await get_coverage_targets_from_jd(jd_text, strict=True)
    pre, body = _split_preamble_body(tex_content)
    history = []

    # Seed once: rebuild skills from JD (keeps concise, canonical tokens)
    merged = _merge_tex(pre, body)
    merged = await _rebuild_skills_safely(merged, jd_text)
    pre, body = _split_preamble_body(merged)

    prev_ratio = -1.0
    for rnd in range(1, max_rounds + 1):
        cur_tex = _merge_tex(pre, body)
        cov = compute_keyword_coverage_bullets(cur_tex, tokens)
        history.append({"round": rnd, "coverage": cov["ratio"], "missing": cov["missing"][:40]})
        log_event(f"ðŸ“Š [COVERAGE r{rnd}] bullets {len(cov['present'])}/{cov['total']} â†’ {cov['ratio']:.1%}")

        if cov["ratio"] >= min_ratio:
            return cur_tex, cov, history

        # ðŸš€ Try deterministic injector FIRST for instant coverage gains (no API call)
        if cov["missing"]:
            injected = inject_missing_keywords_deterministic(cur_tex, list(cov["missing"]))
            cov_inj = compute_keyword_coverage_bullets(injected, tokens)
            if cov_inj["ratio"] > cov["ratio"]:
                pre, body = _split_preamble_body(injected)
                if cov_inj["ratio"] >= min_ratio:
                    return _merge_tex(pre, body), cov_inj, history
                prev_ratio = cov_inj["ratio"]
                continue  # next round starts from improved state

        if abs(cov["ratio"] - prev_ratio) < 1e-4:
            # one last deterministic push before giving up
            injected = inject_missing_keywords_deterministic(cur_tex, list(cov["missing"]))
            cov_inj = compute_keyword_coverage_bullets(injected, tokens)
            if cov_inj["ratio"] > cov["ratio"]:
                pre, body = _split_preamble_body(injected)
                prev_ratio = cov_inj["ratio"]
                continue
            log_event("â­ï¸ [COVERAGE] No progress; stopping at current best.")
            break
        prev_ratio = cov["ratio"]

        # ðŸŽ¯ Single GPT micro-patch (keep progress; do NOT retarget or rebuild again)
        improved_body = await gpt_improve_for_missing_keywords(body, jd_text, cov["missing"])
        pre, body = _split_preamble_body(_merge_tex(pre, improved_body))


    # Final snapshot (no injection)
    final_tex = _merge_tex(pre, body)
    cov = compute_keyword_coverage_bullets(final_tex, tokens)
    return final_tex, cov, history

# ============================================================
# ðŸš€ Endpoint (iterate to â‰¥100% keyword coverage, humanize once after)
#     + route aliases for frontend fallbacks (/run, /submit) and legacy (/optimize/*)
# ============================================================
# NOTE: This router is mounted at prefix="/api/optimize" in main.py

@router.post("/")         # POST /api/optimize/
@router.post("/run")      # POST /api/optimize/run
@router.post("/submit")   # POST /api/optimize/submit
async def optimize_endpoint(
    jd_text: str = Form(...),                           # required
    use_humanize: bool = Form(True),
    base_resume_tex: UploadFile | None = File(None),    # accept file uploads correctly
    extra_keywords: str | None = Form(None),            # keep types simple
):
    try:
        # Honor server default switch
        use_humanize = True if getattr(config, "HUMANIZE_DEFAULT_ON", True) else use_humanize

        # Basic input prep
        extras: List[str] = [x.strip() for x in (extra_keywords or "").split(",") if x and x.strip()]
        jd_text = (jd_text or "").strip()
        if not jd_text:
            raise HTTPException(status_code=400, detail="jd_text is required.")

        # ---- Load base .tex (upload if provided, else server default) ----
        raw_tex: str = ""
        if base_resume_tex is not None:
            tex_bytes = await base_resume_tex.read()
            if tex_bytes:
                tex = tex_bytes.decode("utf-8", errors="ignore")
                raw_tex = secure_tex_input(base_resume_tex.filename or "upload.tex", tex)

        if not raw_tex:
            default_path = getattr(config, "DEFAULT_BASE_RESUME", None)
            if isinstance(default_path, (str, bytes)):
                default_path = Path(default_path)
            if not default_path or not isinstance(default_path, Path) or not default_path.exists():
                raise HTTPException(
                    status_code=500,
                    detail=f"Default base resume not found at {default_path}"
                )
            raw_tex = default_path.read_text(encoding="utf-8")
            log_event(f"ðŸ“„ Using server default base: {default_path}")

        # ---- Company/role for filenames ----
        company_name, role = await extract_company_role(jd_text)

        # Eligibility BEFORE rewrites (strict; mode-aware)
        eligibility = await compute_eligibility_any(raw_tex, jd_text, extra=extras)

        # ---- AI pipeline: seed, refine to coverage ----
        optimized_tex = await optimize_resume_latex(raw_tex, jd_text)
        optimized_tex, coverage_report, coverage_history = await refine_resume_to_keyword_coverage(
            optimized_tex,
            jd_text,
            min_ratio=1.0,
            max_rounds=2,
        )
        cur_tex = optimized_tex  # do NOT rebuild skills again here (preserve injected tokens)

        # ---------- Compile base ----------
        final_tex = render_final_tex(cur_tex)
        try:
            pdf_bytes_original = compile_latex_safely(final_tex)
            if not pdf_bytes_original:
                log_event("âš ï¸ [COMPILE] Initial PDF compilation returned None")
                debug_path = Path(f"/tmp/debug_failed_{safe_company}_{safe_role}.tex")
                debug_path.write_text(final_tex, encoding="utf-8")
                log_event(f"ðŸ§¾ [DEBUG] Saved failed LaTeX to {debug_path}")
                raise HTTPException(
                    status_code=500,
                    detail="LaTeX compilation failed. Check server logs for details."
                )
        except HTTPException:
            raise
        except Exception as e:
            log_event(f"âš ï¸ [COMPILE] PDF compilation failed: {e}")
            debug_path = Path(f"/tmp/debug_failed_{safe_company}_{safe_role}.tex")
            debug_path.write_text(final_tex, encoding="utf-8")
            log_event(f"ðŸ§¾ [DEBUG] Saved failed LaTeX to {debug_path}")
            raise HTTPException(
                status_code=500,
                detail=f"LaTeX compilation failed: {str(e)}"
            )
        
        base_pages = _pdf_page_count(pdf_bytes_original)
        log_event(f"ðŸ“„ Base PDF pages: {base_pages}")

        # ---------- Output destinations (flat folders) ----------
        safe_company, safe_role = safe_filename(company_name), safe_filename(role)
        paths = build_output_paths(company_name, role)  # /data/Optimized, /data/Humanized, /data/Cover Letters
        saved_paths: List[str] = []

        # ---------- Ensure â‰¤ 1 page by trimming from Achievements/Projects/Experience ----------
        MAX_TRIMS = 50
        cur_pdf_bytes = pdf_bytes_original
        cur_pages = base_pages
        trim_idx = 0

        while cur_pages > 1 and trim_idx < MAX_TRIMS:
            next_tex, removed = remove_one_achievement_bullet(cur_tex)
            if not removed:
                next_tex, removed = remove_last_bullet_from_sections(cur_tex, sections=("Projects", "Experience"))
                if not removed:
                    log_event("â„¹ï¸ No more bullets to remove; stopping trim loop.")
                    break

            trim_idx += 1
            log_event(f"âœ‚ï¸ [TRIM {trim_idx}] Removed one bullet")
            next_tex_rendered = render_final_tex(next_tex)
            next_pdf_bytes = compile_latex_safely(next_tex_rendered)
            next_pages = _pdf_page_count(next_pdf_bytes)
            log_event(f"ðŸ“„ [TRIM {trim_idx}] Pages now: {next_pages}")

            cur_tex, cur_pdf_bytes, cur_pages = next_tex, next_pdf_bytes, next_pages
            if cur_pages <= 1:
                log_event(f"âœ… Fits on one page after {trim_idx} trims.")
                break

        # ---------- Humanize ONCE if requested and â‰¥90% coverage ----------
        pdf_bytes_humanized: Optional[bytes] = None
        humanized_tex: Optional[str] = None
        did_humanize = False

        if use_humanize and coverage_report["ratio"] >= 0.90:
            did_humanize = True
            humanized_tex = await humanize_experience_bullets(cur_tex)

            # Re-check and re-inject if humanization paraphrased tokens away
            tokens = await get_coverage_targets_from_jd(jd_text, strict=True)
            cov_h = compute_keyword_coverage_bullets(humanized_tex, tokens)
            if cov_h["ratio"] < 1.0 and cov_h["missing"]:
                humanized_tex = inject_missing_keywords_deterministic(humanized_tex, list(cov_h["missing"]))

            humanized_tex_rendered = render_final_tex(humanized_tex)
            try:
                pdf_bytes_humanized = compile_latex_safely(humanized_tex_rendered)
                if not pdf_bytes_humanized:
                    log_event("âš ï¸ [COMPILE] Humanized PDF compilation returned None")
                    pdf_bytes_humanized = None
            except Exception as e:
                log_event(f"âš ï¸ [COMPILE] Humanized PDF compilation failed: {e}")
                pdf_bytes_humanized = None

            # If humanized >1 page, mirror trim loop
            h_pages = _pdf_page_count(pdf_bytes_humanized)
            trim_h_idx = 0
            while h_pages > 1 and trim_h_idx < MAX_TRIMS:
                next_h_tex, removed = remove_one_achievement_bullet(humanized_tex)
                if not removed:
                    next_h_tex, removed = remove_last_bullet_from_sections(humanized_tex, sections=("Projects", "Experience"))
                    if not removed:
                        break
                h_rendered = render_final_tex(next_h_tex)
                h_pdf = compile_latex_safely(h_rendered)
                humanized_tex, pdf_bytes_humanized = next_h_tex, h_pdf
                h_pages = _pdf_page_count(pdf_bytes_humanized)
                trim_h_idx += 1

        # ---------- Save final outputs (flat libraries) ----------
        opt_path = paths["optimized"]   # /data/Optimized/Optimized - Sri_{Company}_{Role}.pdf
        hum_path = paths["humanized"]   # /data/Humanized/Humanized - Sri_Kadali_{Company}_{Role}.pdf

        if cur_pdf_bytes:
            opt_path.parent.mkdir(parents=True, exist_ok=True)
            opt_path.write_bytes(cur_pdf_bytes)
            saved_paths.append(str(opt_path))
            log_event(f"ðŸ’¾ [SAVE] Optimized PDF â†’ {opt_path}")

        if did_humanize and pdf_bytes_humanized:
            hum_path.parent.mkdir(parents=True, exist_ok=True)
            hum_path.write_bytes(pdf_bytes_humanized)
            saved_paths.append(str(hum_path))
            log_event(f"ðŸ’¾ [SAVE] Humanized PDF â†’ {hum_path}")
        elif use_humanize and coverage_report["ratio"] >= 0.90 and humanized_tex and not pdf_bytes_humanized:
            # Store failed humanized LaTeX nearby for debugging
            t = hum_path.with_name(f"FAILED_Humanized_{safe_company}_{safe_role}.tex")
            t.write_text(humanized_tex, encoding="utf-8")
            log_event(f"ðŸ§¾ [DEBUG] Saved failed humanized LaTeX â†’ {t}")

        # ---------- Response ----------
        return JSONResponse({
            "company_name": company_name,
            "role": role,
            "eligibility": eligibility,
            "optimized": {
                "tex": render_final_tex(cur_tex),
                "pdf_b64": base64.b64encode(cur_pdf_bytes or b"").decode("ascii"),
                "filename": str(opt_path) if cur_pdf_bytes else "",
            },
            "humanized": {
                "tex": render_final_tex(humanized_tex) if (did_humanize and humanized_tex) else "",
                "pdf_b64": base64.b64encode(pdf_bytes_humanized or b"").decode("ascii") if (did_humanize and pdf_bytes_humanized) else "",
                "filename": str(hum_path) if (did_humanize and pdf_bytes_humanized) else "",
            },
            # backward/flat keys used by the frontend cache
            "tex_string": render_final_tex(cur_tex),
            "pdf_base64": base64.b64encode(cur_pdf_bytes or b"").decode("ascii"),
            "pdf_base64_humanized": base64.b64encode(pdf_bytes_humanized or b"").decode("ascii")
                if (did_humanize and pdf_bytes_humanized) else None,
            "saved_paths": saved_paths,
            "coverage_ratio": coverage_report["ratio"],
            "coverage_present": coverage_report["present"],
            "coverage_missing": coverage_report["missing"],
            "coverage_history": coverage_history,
            "did_humanize": did_humanize,
            "extra_keywords": extras,
        })
    except Exception as e:
        log_event(f"ðŸ’¥ [PIPELINE] Optimization failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))-e 


-e # ===== FILE: ./api/models_router.py =====

# ============================================================
#  ASTRA v2.1.2 â€” models_router.py
#  Exposes model catalogs + pricing for the frontend picker.
#  Reads from backend.core.config (AVAILABLE_MODELS, MODEL_PRICING, etc.)
# ============================================================

from __future__ import annotations

from typing import Dict, Any, List
from fastapi import APIRouter, HTTPException

from backend.core import config

router = APIRouter(prefix="/api/models", tags=["models"])


def _providers() -> List[str]:
    """Stable, sorted provider list for deterministic UI rendering."""
    return sorted(getattr(config, "AVAILABLE_MODELS", {}).keys())


def _available() -> Dict[str, Any]:
    """Provider -> list of models or modes."""
    return getattr(config, "AVAILABLE_MODELS", {})


def _pricing() -> Dict[str, Any]:
    """Provider -> pricing dict."""
    return getattr(config, "MODEL_PRICING", {})


def _aliases() -> Dict[str, str]:
    """Optional alias map (human label -> model id)."""
    return getattr(config, "MODEL_ALIASES", {})


@router.get("")
async def list_models():
    """
    Aggregate endpoint consumed by the frontend to render model pickers.
    """
    return {
        "default_model": getattr(config, "DEFAULT_MODEL", ""),
        "providers": _providers(),
        "available": _available(),
        "pricing": _pricing(),
        "aliases": _aliases(),  # safe even if empty
        "version": config.APP_VERSION,
    }


@router.get("/openai")
async def list_openai():
    """
    Return only OpenAI models and their pricing.
    """
    available = _available().get("openai", [])
    pricing = _pricing().get("openai", {})
    return {
        "provider": "openai",
        "models": available,
        "pricing": pricing,
        "default": getattr(config, "DEFAULT_MODEL", ""),
        "aliases": _aliases(),
        "version": config.APP_VERSION,
    }


@router.get("/aihumanize")
async def list_aihumanize():
    """
    Return AIHumanize modes (styles/modes, not token-metered models)
    and the display pricing/plans info if present.
    """
    available = _available().get("aihumanize", [])
    pricing = _pricing().get("aihumanize", {})
    return {
        "provider": "aihumanize",
        "modes": available,
        "pricing": pricing,  # e.g. {"modes":[...], "plans": {...}, "unit": "subscription"}
        "version": config.APP_VERSION,
    }


@router.get("/provider/{name}")
async def list_by_provider(name: str):
    """
    Generic provider fetch. Helpful for future providers.
    """
    key = name.lower().strip()
    available = _available()
    pricing = _pricing()

    if key not in available:
        raise HTTPException(status_code=404, detail=f"Provider '{key}' not found")

    return {
        "provider": key,
        "available": available.get(key, []),
        "pricing": pricing.get(key, {}),
        "version": config.APP_VERSION,
    }


@router.get("/pricing")
async def pricing_only():
    """
    Raw pricing object for UI tables.
    """
    return {
        "pricing": _pricing(),
        "version": config.APP_VERSION,
    }


@router.get("/aliases")
async def aliases_only():
    """
    Optional alias map (human label -> model id).
    """
    return {
        "aliases": _aliases(),
        "version": config.APP_VERSION,
    }
-e 


-e # ===== FILE: ./api/mastermind.py =====

"""
============================================================
 HIREX v2.1.0 â€” mastermind.py
 ------------------------------------------------------------
 MasterMind AI Assistant API
 â€¢ Context-aware reasoning and Q&A engine
 â€¢ Supports multi-turn sessions (filesystem store)
 â€¢ Integrates cleanly with SuperHuman/Talk modules
 â€¢ Persona, tone, and model controls

 Author: Sri Akash Kadali
============================================================
"""

from __future__ import annotations

import json
import os
import time
import uuid
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from fastapi import APIRouter, Form, HTTPException, Query
from fastapi.responses import JSONResponse
from openai import AsyncOpenAI

from backend.core import config
from backend.core.utils import log_event
from backend.core.security import secure_tex_input

router = APIRouter(prefix="/api/mastermind", tags=["mastermind"])
openai_client = AsyncOpenAI(api_key=config.OPENAI_API_KEY)

# ---------------------------------------------
# Defaults
# ---------------------------------------------
DEFAULT_MODEL = getattr(config, "MASTERMINDS_MODEL", getattr(config, "DEFAULT_MODEL", "gpt-4o-mini"))
STORE_DIR: Path = Path(getattr(config, "MASTERMINDS_PATH", (Path("data") / "mastermind_sessions")))
STORE_DIR.mkdir(parents=True, exist_ok=True)

# ============================================================
# ðŸ—‚ Filesystem session store (fallback-safe)
# ============================================================

@dataclass
class Session:
    id: str
    meta: Dict[str, Any]
    messages: List[Dict[str, str]]
    created_at: str
    updated_at: str

def _session_path(session_id: str) -> Path:
    return STORE_DIR / f"{session_id}.json"

def _new_session_id() -> str:
    return f"mm_{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}_{uuid.uuid4().hex[:6]}"

def _read_json(path: Path) -> Optional[Dict[str, Any]]:
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return None

def start_session(meta: Dict[str, Any]) -> Dict[str, Any]:
    sid = _new_session_id()
    now = datetime.utcnow().isoformat()
    sess = Session(id=sid, meta=meta or {}, messages=[], created_at=now, updated_at=now)
    _session_path(sid).write_text(json.dumps(asdict(sess), ensure_ascii=False, indent=2), encoding="utf-8")
    return asdict(sess)

def load_session(session_id: str) -> Optional[Dict[str, Any]]:
    p = _session_path(session_id)
    return _read_json(p)

def append_message(session_id: str, msg: Dict[str, str]) -> Dict[str, Any]:
    p = _session_path(session_id)
    data = _read_json(p) or {"id": session_id, "meta": {}, "messages": [], "created_at": datetime.utcnow().isoformat()}
    data.setdefault("messages", []).append(msg)
    data["updated_at"] = datetime.utcnow().isoformat()
    p.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
    return data

def list_sessions() -> List[Dict[str, Any]]:
    files = sorted(STORE_DIR.glob("*.json"), key=lambda x: x.stat().st_mtime, reverse=True)
    out: List[Dict[str, Any]] = []
    for f in files:
        data = _read_json(f)
        if not data:
            continue
        out.append({
            "id": data.get("id"),
            "meta": data.get("meta", {}),
            "created_at": data.get("created_at"),
            "updated_at": data.get("updated_at"),
            "message_count": len(data.get("messages", [])),
        })
    return out

# ============================================================
# ðŸ§  Utility: truncate long history by chars
# ============================================================
def _trim_messages(msgs: List[Dict[str, str]], max_chars: int = 9000) -> List[Dict[str, str]]:
    """
    Keep only the latest messages up to max_chars (on content),
    preserving order, to control context size.
    """
    joined = ""
    kept: List[Dict[str, str]] = []
    for m in reversed(msgs):
        text = str(m.get("content", "") or "")
        if len(joined) + len(text) > max_chars:
            break
        kept.insert(0, m)
        joined += text
    return kept

def _resp_text(resp) -> str:
    """Robust extraction for OpenAI Responses API output."""
    try:
        t = getattr(resp, "output_text", None)
        if t:
            return str(t).strip()
    except Exception:
        pass
    try:
        return str(resp.output[0].content[0].text).strip()
    except Exception:
        return ""


# ============================================================
# ðŸš€ Endpoint: Start a new MasterMind session
# ============================================================
@router.post("/start")
async def start_session_api(
    persona: str = Form("General"),
    model: str = Form(DEFAULT_MODEL),
    purpose: str = Form("interactive reasoning"),
):
    if not config.OPENAI_API_KEY:
        raise HTTPException(status_code=400, detail="OPENAI_API_KEY missing in environment.")
    meta = {"persona": persona, "model": model, "purpose": purpose}
    data = start_session(meta)
    log_event("mastermind_session_started", {"persona": persona, "model": model})
    return JSONResponse({"session": data})


# ============================================================
# ðŸ§© Endpoint: Continue conversation / respond
# ============================================================
@router.post("/chat")
async def mastermind_chat(
    session_id: str = Form(...),
    prompt: str = Form(...),
    tone: str = Form("balanced"),
    model: str = Form(DEFAULT_MODEL),
    persona: str = Form("General"),
    temperature: float = Form(0.6),
    max_ctx_chars: int = Form(9000),
):
    if not config.OPENAI_API_KEY:
        raise HTTPException(status_code=400, detail="OPENAI_API_KEY missing in environment.")

    # 1ï¸âƒ£ Load or initialize session
    data = load_session(session_id)
    if not data:
        data = start_session({"persona": persona, "model": model})

    history: List[Dict[str, str]] = _trim_messages(data.get("messages", []), max_chars=int(max_ctx_chars))

    # 2ï¸âƒ£ Build AI prompt context
    system_prompt = (
        f"You are MasterMind â€” an intelligent, concise reasoning assistant inside HIREX. "
        f"You adopt the persona of '{persona}' and respond in a {tone} tone. "
        "Answer clearly and practically, focusing on career, resumes, job search, or technical reasoning. "
        "Keep answers compact and precise. Markdown allowed. Avoid repetition."
    )

    messages = [{"role": "system", "content": system_prompt}, *history, {"role": "user", "content": prompt}]

    # 3ï¸âƒ£ Store user message immediately
    append_message(session_id, {"role": "user", "content": prompt})

    # 4ï¸âƒ£ Query the model
    try:
        resp = await openai_client.responses.create(
            model=model,
            input=messages,
            temperature=float(temperature),
            max_output_tokens=800,
        )
        reply_text = _resp_text(resp) or "No response."
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"LLM request failed: {e}")

    # 5ï¸âƒ£ Sanitize for LaTeX safety
    safe_reply = secure_tex_input(reply_text)

    # 6ï¸âƒ£ Append assistant reply
    append_message(session_id, {"role": "assistant", "content": safe_reply})

    # 7ï¸âƒ£ Log event
    log_event(
        "mastermind_chat",
        {
            "session_id": session_id,
            "persona": persona,
            "tone": tone,
            "model": model,
            "chars": len(reply_text),
        },
    )

    return JSONResponse(
        {
            "reply": safe_reply,
            "persona": persona,
            "tone": tone,
            "model": model,
            "timestamp": datetime.utcnow().isoformat(),
        }
    )


# ============================================================
# ðŸ“œ Endpoint: Retrieve session history
# ============================================================
@router.get("/history")
async def get_session_history(session_id: str = Query(..., description="MasterMind session id")):
    """Fetch conversation messages for a given session."""
    data = load_session(session_id)
    if not data:
        raise HTTPException(status_code=404, detail="Session not found")
    return {"session": data}


# ============================================================
# ðŸ—‚ï¸ Endpoint: List all sessions
# ============================================================
@router.get("/sessions")
async def list_sessions_api():
    """List all MasterMind sessions with metadata."""
    return {"sessions": list_sessions()}


# ============================================================
# ðŸ§¹ Endpoint: Reset / delete a session
# ============================================================
@router.delete("/session")
async def delete_session_api(session_id: str = Query(..., description="MasterMind session id")):
    """Delete a specific session JSON file."""
    p = _session_path(session_id)
    if not p.exists():
        raise HTTPException(status_code=404, detail="Session not found")
    try:
        p.unlink()
        log_event("mastermind_session_deleted", {"id": session_id})
        return {"deleted": True}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Delete failed: {e}")
-e 


-e # ===== FILE: ./api/utils_router.py =====

# ============================================================
#  ASTRA v2.1.2 â€” Utility & Diagnostics API (FINAL)
#  ------------------------------------------------------------
#  Endpoints:
#   â€¢ Health-ish ping / version / safe config subset
#   â€¢ Logging (frontend analytics)
#   â€¢ Text helpers (escape/unescape)
#   â€¢ Base64 encode/decode utilities
#   â€¢ Safe filename & slug helpers
#   â€¢ Recent Contexts (deduped per Company__Role)
#   â€¢ History + status dashboard support
#  Author: Sri Akash Kadali
# ============================================================

from __future__ import annotations

import base64
import json
import re
import platform
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from fastapi import APIRouter, Form, HTTPException, Query

from backend.core import config
from backend.core.utils import log_event, safe_filename
from backend.core.security import secure_tex_input

router = APIRouter(prefix="/api/utils", tags=["utils"])

CONTEXT_DIR: Path = config.get_contexts_dir()
LOG_PATH: Path = Path(config.LOG_PATH)  # use configured path only

# ============================================================
# âš™ï¸ 1) PING / VERSION / CONFIG
# ============================================================
@router.get("/ping")
async def ping():
    """Lightweight liveness probe (distinct from /health)."""
    return {
        "status": "ok",
        "service": "ASTRA Core API",
        "time": datetime.utcnow().isoformat() + "Z",
        "platform": platform.system(),
        "python": platform.python_version(),
    }


@router.get("/version")
async def get_version():
    """Return the current ASTRA version and model defaults."""
    return {
        "version": config.APP_VERSION,
        "default_model": getattr(config, "DEFAULT_MODEL", "gpt-4o-mini"),
        "talk_summary_model": getattr(config, "TALK_SUMMARY_MODEL", "gpt-4o-mini"),
        "talk_answer_model": getattr(
            config, "TALK_ANSWER_MODEL", getattr(config, "DEFAULT_MODEL", "gpt-4o-mini")
        ),
        "superhuman_local": getattr(config, "SUPERHUMAN_LOCAL_ENABLED", True),
        "build_time": datetime.utcnow().isoformat() + "Z",
    }


@router.get("/config")
async def get_config():
    """Expose a safe subset of configuration variables for frontend diagnostics."""
    safe_keys = [
        "APP_VERSION",
        "DEFAULT_MODEL",
        "TALK_SUMMARY_MODEL",
        "TALK_ANSWER_MODEL",
        "SUPERHUMAN_LOCAL_ENABLED",
        "BASE_COVERLETTER_PATH",
        "MASTERMINDS_PATH",
        "LOG_PATH",
        "HISTORY_PATH",
        "API_BASE_URL",
    ]
    safe_data: Dict[str, Any] = {}
    for k in safe_keys:
        v = getattr(config, k, None)
        safe_data[k] = str(v) if isinstance(v, Path) else v
    return {"config": safe_data}

# ============================================================
# ðŸ§¾ 2) FRONTEND LOGGING & ANALYTICS
# ============================================================
@router.post("/log")
async def log_frontend_event(
    msg: str = Form(...),
    page: str = Form("unknown"),
    version: str = Form("unknown"),
    origin: str = Form("client"),
    level: str = Form("info"),
):
    """Receives debug or analytic events from the frontend (UI telemetry)."""
    meta = {
        "msg": msg,
        "page": page,
        "version": version,
        "origin": origin,
        "level": level,
        "timestamp": datetime.utcnow().isoformat() + "Z",
    }
    log_event("frontend_log", meta)
    return {"logged": True, "time": meta["timestamp"]}

# ============================================================
# ðŸ§© 3) TEXT UTILITIES
# ============================================================
@router.post("/escape")
async def escape_latex(text: str = Form(...)):
    """Return LaTeX-safe escaped string."""
    try:
        escaped = secure_tex_input(text)
        return {"escaped": escaped}
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Escape failed: {e}") from e


@router.post("/unescape")
async def unescape_latex(text: str = Form(...)):
    """Reverse minimal LaTeX escapes for readability."""
    try:
        unescaped = (
            text.replace(r"\#", "#")
            .replace(r"\%", "%")
            .replace(r"\$", "$")
            .replace(r"\&", "&")
            .replace(r"\_", "_")
            .replace(r"\{", "{")
            .replace(r"\}", "}")
        )
        return {"unescaped": unescaped}
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Unescape failed: {e}") from e

# ============================================================
# ðŸ“¦ 4) ENCODING / DECODING HELPERS
# ============================================================
@router.post("/b64encode")
async def b64encode_data(raw: str = Form(...)):
    """Base64 encode a plain string."""
    try:
        encoded = base64.b64encode(raw.encode("utf-8")).decode("utf-8")
        return {"base64": encoded, "len": len(encoded)}
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Encode failed: {e}") from e


@router.post("/b64decode")
async def b64decode_data(encoded: str = Form(...)):
    """Base64 decode a string."""
    try:
        decoded = base64.b64decode(encoded.encode("utf-8")).decode("utf-8", errors="ignore")
        return {"decoded": decoded, "len": len(decoded)}
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Decode failed: {e}") from e

# ============================================================
# ðŸ—‚ï¸ 5) FILENAME + SANITIZATION HELPERS
# ============================================================
@router.post("/safe_filename")
async def make_safe_filename(name: str = Form(...)):
    """Return a filesystem-safe version of the given filename."""
    safe = safe_filename(name)
    return {"input": name, "safe_name": safe}


@router.post("/slugify")
async def slugify_string(name: str = Form(...)):
    """Return a lowercase slugified string safe for URLs or filenames."""
    slug = re.sub(r"[^a-zA-Z0-9]+", "-", name.strip().lower()).strip("-")
    return {"slug": slug}

# ============================================================
# ðŸ§­ 6) RECENT CONTEXTS (deduped per Company__Role)
# ============================================================
def _updated_at(d: Dict[str, Any], default_ts: float) -> float:
    try:
        ts_raw = d.get("updated_at") or d.get("saved_at") or ""
        if isinstance(ts_raw, str):
            ts = ts_raw.rstrip("Z")
            if ts:
                return datetime.fromisoformat(ts).timestamp()
    except Exception:
        pass
    return default_ts


def _coerce_key(d: Dict[str, Any], p: Optional[Path]) -> str:
    if d.get("key"):
        return str(d["key"]).strip()
    c, r = (d.get("company") or "").strip(), (d.get("role") or "").strip()
    if c and r:
        return f"{safe_filename(c)}__{safe_filename(r)}"
    return p.stem if p else ""


def _compact_meta(d: Dict[str, Any], key: str) -> Dict[str, Any]:
    return {
        "key": key,
        "title": d.get("title_for_memory") or d.get("title") or f"{d.get('company','')} â€” {d.get('role','')}",
        "company": d.get("company"),
        "role": d.get("role"),
        "updated_at": d.get("updated_at") or d.get("saved_at"),
        "has_optimized": bool(((d.get("optimized") or {}).get("tex")) or d.get("resume_tex")),
        "has_humanized": bool(((d.get("humanized") or {}).get("tex")) or d.get("humanized_tex")),
        "has_cover_letter": bool((d.get("cover_letter") or {}).get("tex")),
    }


@router.get("/recent")
async def recent_contexts(
    limit: int = Query(50, ge=1, le=500),
    dedupe: bool = Query(True, description="Collapse multiple files to the latest per (Company__Role)"),
):
    """
    List recent JD/resume contexts saved by the app.
    Mirrors context_store's behavior so frontend pages (Talk, Dashboard)
    can render a clean, de-duplicated 'ðŸ“œ JD + Resume History'.
    """
    if not CONTEXT_DIR.exists():
        return {"items": []}

    entries: List[Tuple[str, Path, Dict[str, Any], float]] = []
    for p in CONTEXT_DIR.glob("*.json"):
        try:
            data = json.loads(p.read_text(encoding="utf-8"))
        except Exception:
            data = {}
        key = _coerce_key(data, p)
        ts = _updated_at(data, p.stat().st_mtime)
        entries.append((key, p, data, ts))

    if dedupe:
        # choose the single newest per key
        latest_by_key: Dict[str, Tuple[Path, Dict[str, Any], float]] = {}
        for key, p, d, ts in entries:
            cur = latest_by_key.get(key)
            if (cur is None) or (ts > cur[2]):
                latest_by_key[key] = (p, d, ts)
        rows = sorted(latest_by_key.items(), key=lambda kv: kv[1][2], reverse=True)[:limit]
        items = [_compact_meta(d, _coerce_key(d, p)) for (_k, (p, d, _)) in rows]
    else:
        rows2 = sorted(entries, key=lambda t: t[3], reverse=True)[:limit]
        items = [_compact_meta(d, _coerce_key(d, p)) for (_k, p, d, _ts) in rows2]

    return {"items": items}

# ============================================================
# ðŸ§­ 7) HISTORY / LOG RETRIEVAL
# ============================================================
def _read_jsonl(path: Path, limit: int) -> List[Dict[str, Any]]:
    if not path.exists():
        return []
    try:
        with open(path, "r", encoding="utf-8") as f:
            lines = f.readlines()[-limit:]
        out: List[Dict[str, Any]] = []
        for line in reversed(lines):
            try:
                obj = json.loads(line)
                if isinstance(obj, dict):
                    out.append(obj)
            except Exception:
                continue
        return out
    except Exception:
        return []


@router.get("/history")
async def get_history(limit: int = Query(100, ge=1, le=1000)):
    """Return the most recent event logs for diagnostics or dashboard."""
    events = _read_jsonl(LOG_PATH, limit)
    return {"count": len(events), "events": events}

# ============================================================
# ðŸ§  8) SYSTEM STATUS SUMMARY (Mini Dashboard)
# ============================================================
@router.get("/status")
async def get_status():
    """
    Lightweight system snapshot used by the dashboard sidebar.
    Provides event totals, last log timestamp, and environment details.
    """
    total, last_event = 0, None

    if LOG_PATH.exists():
        try:
            with open(LOG_PATH, "r", encoding="utf-8") as f:
                lines = f.readlines()
                total = len(lines)
                if lines:
                    try:
                        last_event = json.loads(lines[-1])
                    except Exception:
                        last_event = None
        except Exception:
            last_event = None

    return {
        "status": "ok",
        "total_events": total,
        "last_event": last_event,
        "app_version": config.APP_VERSION,
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "platform": platform.system(),
    }

# ============================================================
# ðŸ§ª 9) SELF-TEST: ENCODE-DECODE ROUNDTRIP
# ============================================================
@router.post("/selftest")
async def self_test(text: str = Form(...)):
    """Perform a simple base64 encode-decode validation."""
    try:
        encoded = base64.b64encode(text.encode("utf-8")).decode("utf-8")
        decoded = base64.b64decode(encoded.encode("utf-8")).decode("utf-8")
        return {
            "input": text,
            "encoded": encoded[:50] + ("..." if len(encoded) > 50 else ""),
            "decoded_match": decoded == text,
            "timestamp": datetime.utcnow().isoformat() + "Z",
        }
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Self-test failed: {e}") from e
-e 


-e # ===== FILE: ./api/latex_parse.py =====

"""
HIREX â€¢ api/latex_parse.py
Lightweight Resume Parser (Raw-Preserve Mode)
Extracts sections from LaTeX or text resumes with zero cleaning or normalization.
Also provides a safe (no-escape) cover-letter body injector with BODY anchors.

Purpose: Provide unaltered LaTeX/text blocks for AI-based optimization.
Author: Sri Akash Kadali
"""

from __future__ import annotations

import re
from typing import Dict, List, Any


# ============================================================
# âš™ï¸ Section Extraction Utilities
# ============================================================
def extract_section(tex: str, section_name: str) -> str:
    """
    Extracts raw LaTeX section content between \section{<section_name>}
    (or \section*{<section_name>}) and the next section, or a plaintext
    header line that looks like Title Case.

    No stripping or cleanup is applied.
    """
    # Accept \section or \section*, and a plain text header line
    pattern = (
        rf"(?:\\section\*?\{{{re.escape(section_name)}\}}"      # \section{Name} or \section*{Name}
        rf"|^(?:{re.escape(section_name)})\s*$)"                # or plain line "Name"
        r"(.*?)"                                                # capture content non-greedily
        r"(?=(?:\\section|\n[A-Z][A-Za-z ]+\n|$))"              # until next section/header or EOF
    )
    m = re.search(pattern, tex, flags=re.DOTALL | re.IGNORECASE | re.MULTILINE)
    return m.group(1) if m else ""


# ============================================================
# ðŸ§  Main Parser (No Cleaning, No Normalization)
# ============================================================
def parse_latex_resume(tex_content: str) -> Dict[str, Any]:
    """
    Parses a LaTeX or plain-text resume into structured JSON form.
    All text is preserved as-is (no trimming, reformatting, or escaping).

    Extracts:
      - Education
      - Skills
      - Experience
      - Projects
      - Achievements
    """
    tex = tex_content.replace("\r", "")

    education_block    = extract_section(tex, "Education")
    skills_block       = extract_section(tex, "Skills")
    experience_block   = extract_section(tex, "Experience")
    projects_block     = extract_section(tex, "Projects")
    achievements_block = extract_section(tex, "Achievements")

    return {
        "education": _extract_bullets(education_block) or _split_lines(education_block),
        "skills": _parse_skills(skills_block),
        "experience": _parse_experience(experience_block),
        "projects": _parse_experience(projects_block),
        "achievements": _extract_bullets(achievements_block) or _split_lines(achievements_block),
    }


# ============================================================
# ðŸ§© Helper Parsers (Preserve Original Text)
# ============================================================
def _split_lines(block: str) -> List[str]:
    """Split section into lines â€” keeps all original spacing and symbols."""
    return [ln for ln in (block or "").splitlines() if ln.strip()]


def _extract_bullets(section: str) -> List[str]:
    """Extract bullet lines without reformatting or cleanup."""
    if not section:
        return []
    # \item, \item[] â€¦, and plain bullets -, â€¢
    bullets = re.findall(r"\\item(?:\[[^\]]*\])?\s+(.*)", section)
    if not bullets:
        bullets = re.findall(r"^[\-\u2022]\s+(.*)$", section, flags=re.MULTILINE)  # - or â€¢
    return [b for b in bullets if str(b).strip()]


def _parse_experience(section: str) -> List[Dict[str, Any]]:
    """
    Extract Experience/Projects entries minimally.
    Preserves LaTeX formatting and avoids stripping or normalization.

    Supports common patterns:
      1) \textbf{Role} \hfill \textit{Company} \hfill Date
      2) \textbf{Company} \hfill \textit{Role} \hfill Date
      followed by an itemize block.
    """
    entries: List[Dict[str, Any]] = []
    if not section:
        return entries

    # Pattern 1: Role then Company
    pat_role_company = re.compile(
        r"\\textbf\{(?P<title>.*?)\}\s*\\hfill\s*(?:\\textit|\\emph)\{(?P<company>.*?)\}"
        r"(?:\s*\\hfill\s*(?P<date>[^\n]*))?"
        r"(?P<body>.*?)\\end\{itemize\}",
        flags=re.DOTALL,
    )

    # Pattern 2: Company then Role
    pat_company_role = re.compile(
        r"\\textbf\{(?P<company>.*?)\}\s*\\hfill\s*(?:\\textit|\\emph)\{(?P<title>.*?)\}"
        r"(?:\s*\\hfill\s*(?P<date>[^\n]*))?"
        r"(?P<body>.*?)\\end\{itemize\}",
        flags=re.DOTALL,
    )

    # Try roleâ†’company first; if none, try companyâ†’role
    matches = list(pat_role_company.finditer(section)) or list(pat_company_role.finditer(section))
    for m in matches:
        bullets = _extract_bullets(m.group("body"))
        entries.append({
            "company": m.group("company"),
            "title": m.group("title"),
            "date": (m.group("date") or "").strip(),
            "bullets": bullets,
        })

    if entries:
        return entries

    # Plain text fallback (preserves spacing)
    blocks = re.split(r"\n(?=[A-Z].*\d{4})", section)
    for block in blocks:
        lines = [ln for ln in block.splitlines() if ln.strip()]
        if not lines:
            continue

        header = lines[0]
        date_match = re.search(r"\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec|[A-Za-z]+)?\.?\s*\d{4}.*\d{4}\b", header)
        date = date_match.group(0) if date_match else ""

        title, company = "", ""
        if " at " in header:
            title, company = header.split(" at ", 1)
        elif " - " in header:
            parts = header.split(" - ", 1)
            title = parts[0]
            company = parts[1]

        bullets = [ln for ln in lines[1:] if not re.match(r"^[A-Z][A-Za-z ]+$", ln)]
        entries.append({
            "company": company or "Unknown",
            "title": title or "Role",
            "date": date,
            "bullets": bullets,
        })
    return entries


def _parse_skills(section: str) -> Dict[str, List[str]]:
    """Extracts Skills lines as-is (no lowercase, trimming, or formatting)."""
    if not section:
        return {}
    lines = [l for l in section.splitlines() if l.strip()]
    skills_dict: Dict[str, List[str]] = {}
    for line in lines:
        if ":" in line:
            key, val = line.split(":", 1)
            values = [v.strip() for v in val.split(",") if v.strip()]
            if values:
                skills_dict[key.strip()] = values
    return skills_dict


# ============================================================
# âœ‰ï¸ Cover-Letter Body Injector (No Escaping)
#  - Inserts body between BODY anchors if present
#  - Else injects just before \end{document}
#  - Strips accidental preamble/closing from body but does not escape/clean
# ============================================================
def inject_cover_body(base_tex: str, body_tex: str) -> str:
    if base_tex is None:
        base_tex = ""
    if body_tex is None:
        body_tex = ""

    # Strip preamble and closing from the body (keep raw content)
    body = re.sub(r"\\documentclass[\s\S]*?\\begin\{document\}", "", body_tex, flags=re.IGNORECASE)
    body = re.sub(r"\\end\{document\}\s*$", "", body, flags=re.IGNORECASE).strip()

    # Prefer explicit anchors
    anchor_rx = r"(%-+BODY-START-+%)(.*?)(%-+BODY-END-+%)"
    if re.search(anchor_rx, base_tex, flags=re.DOTALL):
        return re.sub(anchor_rx, lambda m: f"{m.group(1)}\n{body}\n{m.group(3)}", base_tex, flags=re.DOTALL)

    # Otherwise, inject right before \end{document}
    if re.search(r"\\end\{document\}\s*$", base_tex, flags=re.IGNORECASE):
        return re.sub(r"\\end\{document\}\s*$",
                      f"\n% (Auto-inserted by HIREX)\n{body}\n\\end{{document}}\n",
                      base_tex, flags=re.IGNORECASE)

    # Fallback: append a closing tag
    return base_tex.rstrip() + f"\n\n% (Auto-inserted by HIREX)\n{body}\n\\end{{document}}\n"


# ============================================================
# ðŸ§ª Local Test
# ============================================================
if __name__ == "__main__":
    sample_resume = r"""
    \documentclass{article}
    \begin{document}

    %-----------EDUCATION-----------
    \section{Education}
    University of Maryland, College Park, United States CGPA: 3.55/4
    Master of Science in Applied Machine Learning August 2024 - May 2026
    â€¢ Relevant Coursework:

    %-----------EXPERIENCE-----------
    \section{Experience}
    \textbf{Machine Learning Intern} \hfill \textit{IIT Indore} \hfill May 2023 â€“ Dec 2023
    \begin{itemize}
      \item Developed DeBERTa-based architecture for hate-speech detection.
      \item Improved accuracy using contrastive learning.
      \item Enhanced features with emotion embeddings.
    \end{itemize}

    \end{document}
    """

    from pprint import pprint
    pprint(parse_latex_resume(sample_resume))

    # Cover-letter injection quick check
    base = r"""
    \documentclass{article}
    \begin{document}
    Dear Hiring Manager,

    %-----------BODY-START-----------
    %-----------BODY-END-------------

    Sincerely,\\
    Your Name
    \end{document}
    """
    body = r"""\documentclass{article}\begin{document}
    This is my injected body. \LaTeX{} intact.
    \end{document}"""
    print("\n--- Injected Cover Letter ---")
    print(inject_cover_body(base, body))
-e 


-e # ===== FILE: ./api/humanize.py =====

"""
HIREX â€¢ api/humanize.py (v2.1.2)
Integrates with AIHumanize.io for tone-only rewriting of Experience & Project bullets.
Targets only \resumeItem{...} entries, with strong LaTeX sanitization to avoid
preamble duplication or document corruption. Concurrency + retry hardened.

Author: Sri Akash Kadali
"""

from __future__ import annotations

import os
import re
import json
import asyncio
from dataclasses import dataclass
from typing import List, Tuple, Optional, Dict, Any

import httpx
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field

from backend.core import config
from backend.core.utils import log_event
from backend.core.security import secure_tex_input


# ============================================================
# âš™ï¸ Configuration
# ============================================================

AIHUMANIZE_REWRITE_URL = "https://aihumanize.io/api/v1/rewrite"

# Prefer config-provided mappings/defaults (from core/config.py)
_AIH_MODE_ID: Dict[str, str] = getattr(
    config, "AIHUMANIZE_MODE_ID", {"quality": "0", "balance": "1", "enhanced": "2"}
)
_HUMANIZE_MODE_DEFAULT: str = str(getattr(config, "HUMANIZE_MODE_DEFAULT", "balance")).lower()
_HUMANIZE_DEFAULT_ON: bool = bool(getattr(config, "HUMANIZE_DEFAULT_ON", True))
_LOCAL_ENABLED: bool = bool(getattr(config, "SUPERHUMAN_LOCAL_ENABLED", False))

# Client limits & retries
MAX_CONCURRENT = int(os.getenv("AIHUMANIZE_MAX_CONCURRENT", "60"))
TIMEOUT_SEC = float(os.getenv("AIHUMANIZE_TIMEOUT_SEC", "2000.0"))
RETRIES = int(os.getenv("AIHUMANIZE_RETRIES", "2"))

# FastAPI router (optional for direct API usage)
router = APIRouter(prefix="/api/humanize", tags=["humanize"])


# ============================================================
# ðŸ§½ LaTeX Sanitizer
# ============================================================

_BAD_PREAMBLE_PATTERNS = [
    r"(?i)\\documentclass(\[[^\]]*\])?\{[^}]*\}",
    r"(?i)\\usepackage(\[[^\]]*\])?\{[^}]*\}",
    r"(?i)\\begin\{document\}",
    r"(?i)\\end\{document\}",
    r"(?i)\\(new|renew)command\*?\{[^}]*\}\{[^}]*\}",
    r"(?i)\\input\{[^}]*\}",
]
_FALLBACK_TAG_RE = re.compile(r"^\[LOCAL-FALLBACK:[^\]]+\]\s*", re.IGNORECASE)

def _escape_unescaped_percent(s: str) -> str:
    # Turn bare % into \% to avoid commenting out the remainder of the line
    return re.sub(r"(?<!\\)%", r"\\%", s)

def _strip_md_fences(s: str) -> str:
    return s.replace("```latex", "").replace("```", "")

def clean_humanized_text(text: str, *, latex_safe: bool = True) -> str:
    """
    Remove dangerous LaTeX preamble/commands and markdown fences.
    Strip any accidental fallback labels. Keep content intact.
    """
    cleaned = text or ""
    cleaned = _strip_md_fences(cleaned)
    cleaned = _FALLBACK_TAG_RE.sub("", cleaned)

    for pat in _BAD_PREAMBLE_PATTERNS:
        cleaned = re.sub(pat, "", cleaned)

    # Remove leading LaTeX comments or decorative headers commonly injected
    cleaned = re.sub(r"(?m)^\s*%.*$", "", cleaned)

    # Normalize whitespace
    cleaned = re.sub(r"[ \t]+", " ", cleaned)
    cleaned = re.sub(r"\n{3,}", "\n\n", cleaned).strip()

    # Escape stray %
    if latex_safe:
        cleaned = _escape_unescaped_percent(cleaned)
        cleaned = secure_tex_input(cleaned)

    # Final safety check: if we still see preamble markers, reject
    if re.search(r"\\documentclass|\\usepackage|\\begin\{document\}|\\end\{document\}", cleaned, re.I):
        log_event("humanize_sanitizer_reject", {"reason": "preamble_detected"})
        return ""

    return cleaned


# ============================================================
# ðŸ”Ž Bullet Extraction (brace-aware)
# ============================================================

@dataclass
class BulletSpan:
    start: int
    end: int
    content: str

def _find_resume_items(tex: str) -> List[BulletSpan]:
    """
    Find \resumeItem{...} ranges with a simple brace-depth scan,
    so nested braces within the bullet are tolerated.
    """
    key = r"\resumeItem{"
    spans: List[BulletSpan] = []
    i = 0
    n = len(tex)
    while i < n:
        j = tex.find(key, i)
        if j == -1:
            break
        k = j + len(key)  # content starts here
        depth = 1
        p = k
        while p < n and depth > 0:
            ch = tex[p]
            if ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
            p += 1
        if depth == 0:
            content = tex[k : p - 1]
            spans.append(BulletSpan(start=k, end=p - 1, content=content))
            i = p
        else:
            # Unbalanced; bail
            break
    return spans


# ============================================================
# ðŸŒ AIHumanize Client
# ============================================================

def _resolve_mode_id(mode: str) -> str:
    m = (mode or "").lower().strip()
    if m in _AIH_MODE_ID:
        return _AIH_MODE_ID[m]
    # accept tone synonyms
    if m in {"formal", "academic", "quality"}:
        return _AIH_MODE_ID.get("quality", "0")
    if m in {"balanced", "confident", "balance"}:
        return _AIH_MODE_ID.get("balance", "1")
    if m in {"conversational", "enhanced"}:
        return _AIH_MODE_ID.get("enhanced", "2")
    return _AIH_MODE_ID.get(_HUMANIZE_MODE_DEFAULT, "1")

def _header_variants(key: str) -> List[Dict[str, str]]:
    base = {
        "Content-Type": "application/json",
        "Accept": "application/json",
        "User-Agent": "Mozilla/5.0",
    }
    key = (key or "").strip()
    return [
        dict(base, **{"Authorization": key}),
        dict(base, **{"Authorization": f"Bearer {key}"}),
        dict(base, **{"X-API-KEY": key}),
    ]

async def _rewrite_bullet(
    client: httpx.AsyncClient,
    bullet_text: str,
    idx: int,
    mode_id: str,
    mail: str,
) -> str:
    """
    Call AIHumanize for a single bullet with retry + sanitize.
    Preserves metrics and percent signs by design (post-sanitizer escapes).
    """
    payload = {"model": mode_id, "mail": mail, "data": bullet_text}

    for attempt in range(RETRIES + 1):
        for headers in _header_variants(config.HUMANIZE_API_KEY):
            try:
                r = await client.post(AIHUMANIZE_REWRITE_URL, headers=headers, json=payload)
                # try JSON parse; if invalid JSON, surface HTTP error
                try:
                    data = r.json()
                except json.JSONDecodeError:
                    r.raise_for_status()
                    raise HTTPException(status_code=502, detail="Humanize returned invalid JSON.")

                if isinstance(data, dict) and int(data.get("code", r.status_code)) == 200 and data.get("data"):
                    candidate = clean_humanized_text(str(data["data"]).strip(), latex_safe=True)
                    if candidate:
                        # Resume bullets should be one line; avoid trailing period
                        candidate = candidate.replace("\n", " ").strip().rstrip(".")
                        log_event("aihumanize_bullet_ok", {"idx": idx, "len": len(candidate), "attempt": attempt})
                        return candidate
                    else:
                        # unsafe â†’ revert to original
                        log_event("aihumanize_bullet_revert_unsafe", {"idx": idx, "attempt": attempt})
                        return bullet_text
                else:
                    # Unexpected shape or non-200 code; try next header/attempt
                    log_event("aihumanize_bad_response", {"idx": idx, "attempt": attempt, "resp": data})
            except Exception as e:
                log_event("aihumanize_bullet_error", {"idx": idx, "attempt": attempt, "error": str(e)})
        # exponential backoff between attempts
        await asyncio.sleep(0.5 * (2 ** attempt))

    log_event("aihumanize_bullet_fallback", {"idx": idx})
    return bullet_text


# ============================================================
# ðŸ§© Local (optional) tone-only stub â€” tagless and minimal
# ============================================================

def _local_tone_only(text: str) -> str:
    """
    Minimal, safe, tagless cleanup used only if SUPERHUMAN_LOCAL_ENABLED is True
    and remote Humanize is unavailable. Keeps numbers/metrics intact.
    """
    t = text or ""
    # normalize whitespace
    t = re.sub(r"[ \t]+", " ", t).strip()
    # tiny clarity nips (do not touch numbers, symbols)
    t = re.sub(r"\bu\b", "you", t, flags=re.IGNORECASE)
    t = re.sub(r"\bim\b", "I am", t, flags=re.IGNORECASE)
    # single line, no trailing period for bullets
    t = t.replace("\n", " ").rstrip(".")
    return clean_humanized_text(t, latex_safe=True)


# ============================================================
# ðŸ§  Public Core: Humanize all \resumeItem bullets
# ============================================================

async def humanize_resume_items(
    tex_content: str,
    mode: str = None,
    email: Optional[str] = None,
) -> Tuple[str, int, int]:
    """
    Humanize all \resumeItem{...} bullets concurrently.

    Returns:
        (new_tex, total_found, total_rewritten)
    """
    # Honor "Humanize always on" default; fail fast if disabled
    if not _HUMANIZE_DEFAULT_ON:
        raise RuntimeError("Humanize is disabled by configuration (HUMANIZE_DEFAULT_ON=false).")

    spans = _find_resume_items(tex_content or "")
    total_found = len(spans)
    if total_found == 0:
        log_event("aihumanize_no_bullets", {})
        return tex_content, 0, 0

    # Credentials
    has_creds = bool(config.HUMANIZE_API_KEY and config.HUMANIZE_MAIL)
    use_local = (not has_creds) and _LOCAL_ENABLED

    # Resolve mode and mail
    mode_id = _resolve_mode_id(mode or _HUMANIZE_MODE_DEFAULT)
    mail = (email or config.HUMANIZE_MAIL or "").strip()

    limits = httpx.Limits(max_keepalive_connections=MAX_CONCURRENT, max_connections=MAX_CONCURRENT)
    timeout = httpx.Timeout(TIMEOUT_SEC)
    sem = asyncio.Semaphore(MAX_CONCURRENT)

    rewritten_texts: List[str] = []

    if has_creds:
        async with httpx.AsyncClient(limits=limits, timeout=timeout, headers={"User-Agent": "Mozilla/5.0", "Accept": "application/json"}) as client:
            async def _task(idx: int, content: str) -> str:
                async with sem:
                    c = content.strip()
                    if not c:
                        return content
                    return await _rewrite_bullet(client, c, idx, mode_id, mail)

            rewritten_texts = await asyncio.gather(
                *[_task(i + 1, b.content) for i, b in enumerate(spans)], return_exceptions=False
            )
    elif use_local:
        # Local tagless cleanup per bullet
        rewritten_texts = [_local_tone_only(b.content) for b in spans]
    else:
        raise RuntimeError("HUMANIZE_API_KEY/HUMANIZE_MAIL missing and local fallback disabled.")

    # Rebuild the LaTeX safely by slicing with recorded spans
    out_parts: List[str] = []
    last = 0
    total_rewritten = 0
    for (span, new_txt) in zip(spans, rewritten_texts):
        out_parts.append(tex_content[last:span.start])
        safe_new = (new_txt or "").strip().rstrip(".")
        if safe_new != span.content.strip():
            total_rewritten += 1
        out_parts.append(safe_new)
        last = span.end
    out_parts.append(tex_content[last:])

    new_tex = "".join(out_parts)

    # Final safety: strip accidental preamble fragments and normalize whitespace
    for pat in _BAD_PREAMBLE_PATTERNS:
        new_tex = re.sub(pat, "", new_tex)
    new_tex = re.sub(r"\n{3,}", "\n\n", new_tex).strip()

    log_event("aihumanize_complete", {"found": total_found, "rewritten": total_rewritten, "mode": mode or _HUMANIZE_MODE_DEFAULT})
    return new_tex, total_found, total_rewritten


# ============================================================
# ðŸŒ FastAPI endpoints (optional, convenient for frontend)
# ============================================================

class BulletsReq(BaseModel):
    tex_content: str = Field(..., description="LaTeX content containing \\resumeItem{...} bullets.")
    mode: Optional[str] = Field(None, description="quality | balance | enhanced | synonyms accepted")
    email: Optional[str] = Field(None, description="Account email for AIHumanize (optional override).")

@router.post("/bullets")
async def api_humanize_bullets(req: BulletsReq):
    """
    Rewrites only \\resumeItem{...} bullets inside the provided LaTeX string.
    Returns sanitized LaTeX. Requires HUMANIZE_API_KEY/HUMANIZE_MAIL or enabled local fallback.
    """
    try:
        new_tex, found, rewritten = await humanize_resume_items(req.tex_content, mode=req.mode, email=req.email)
        return {
            "ok": True,
            "tex_content": new_tex,
            "found": found,
            "rewritten": rewritten,
            "mode": (req.mode or _HUMANIZE_MODE_DEFAULT),
        }
    except RuntimeError as e:
        raise HTTPException(status_code=503, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=502, detail=f"AIHumanize processing failed: {e}")


# ============================================================
# ðŸ§ª Local CLI test
# ============================================================

if __name__ == "__main__":
    async def _run():
        sample_tex = r"""
        \resumeItem{worked on python scripts for data processing}
        \resumeItem{helped team with docker deployments}
        \resumeItem{deployed 3 APIs with 99\% uptime}
        """
        try:
            out, found, rewritten = await humanize_resume_items(sample_tex, mode="balance")
            print("\n=== Found:", found, "Rewritten:", rewritten, "===\n")
            print(out)
        except Exception as e:
            print("Local test error:", e)

    asyncio.run(_run())
-e 


-e # ===== FILE: ./api/coverletter.py =====

# ============================================================
#  HIREX v2.1.0 â€” Cover Letter Generation Endpoint (FINAL)
#  ------------------------------------------------------------
#  Improvements vs 2.0.x:
#   â€¢ Scrubs fake-looking step numbers ((1), 1), 1.) in prose
#   â€¢ Replaces em/en/double/single dash separators with commas
#   â€¢ Strong LaTeX-safe escaping (&, %, $, #, _, {, }, ~, ^, \)
#   â€¢ Removes STAR scaffolding; fixes mid-word linebreaks
#   â€¢ Precise injection between salutation and signoff
#   â€¢ Saves to stable per-(Company,Role) context file (dedupe)
#   â€¢ Response includes memory_id=id=stable context key
#   â€¢ Saves final PDF to data/samples/Cover Letters/...
# ============================================================

from __future__ import annotations

import base64
import json
import re
from datetime import datetime
from typing import Tuple, Optional, Dict, Any, List
import difflib

import httpx
from fastapi import APIRouter, Form, HTTPException
from fastapi.responses import JSONResponse
from openai import OpenAI

from backend.core import config
from backend.core.utils import log_event, safe_filename, ensure_dir
from backend.core.compiler import compile_latex_safely
from backend.core.security import secure_tex_input

# --- shared helpers (try backend.api.*, then api.*) ---
try:
    from backend.api.render_tex import render_final_tex  # type: ignore
except Exception:  # pragma: no cover
    from api.render_tex import render_final_tex  # type: ignore

try:
    from backend.api.latex_parse import inject_cover_body as _shared_inject  # type: ignore
except Exception:  # pragma: no cover
    try:
        from api.latex_parse import inject_cover_body as _shared_inject  # type: ignore
    except Exception:
        _shared_inject = None  # type: ignore

router = APIRouter(prefix="/api/coverletter", tags=["coverletter"])
openai_client = OpenAI(api_key=config.OPENAI_API_KEY)

_DEFAULT_OAI_MODEL = "gpt-4o-mini"
_EXTRACT_MODEL = getattr(config, "COVERLETTER_MODEL", _DEFAULT_OAI_MODEL) or _DEFAULT_OAI_MODEL
_DRAFT_MODEL = getattr(config, "COVERLETTER_MODEL", _DEFAULT_OAI_MODEL) or _DEFAULT_OAI_MODEL

_DISABLE_SHARED_INJECTOR = True  # known-buggy guard


# -----------------------------
# Utilities
# -----------------------------
def _json_from_text(text: str, default: dict) -> dict:
    if not text:
        return default
    m = re.search(r"\{[\s\S]*\}", text)
    if not m:
        return default
    try:
        return json.loads(m.group(0))
    except Exception:
        return default


def _latex_escape_light(text: str) -> str:
    """
    Minimal, text-mode LaTeX escaping for generated/body/header content.
    - '&' -> 'and'
    - Escapes: %, $, #, _, {, }, ~, ^, backslash
    - Idempotent
    """
    if not text:
        return ""
    text = text.replace("&", " and ")
    repl = {
        "%": r"\%",
        "$": r"\$",
        "#": r"\#",
        "_": r"\_",
        "{": r"\{",
        "}": r"\}",
        "~": r"\string~",
        "^": r"\string^",
        "\\": r"\textbackslash{}",
    }
    out = "".join(repl.get(ch, ch) for ch in text)
    return re.sub(r"[ \t]{2,}", " ", out).strip()


def _strip_star_labels(text: str) -> str:
    """Remove STAR scaffolding like '(situation/task)', 'Actions:', 'result and impact', etc., and tidy spaces."""
    if not text:
        return ""
    text = re.sub(
        r"(?i)\(\s*(?:situation|task|actions?|result(?:\s+and\s+impact)?|impact)"
        r"(?:\s*/\s*(?:task|actions?|result|impact))?\s*\)",
        "",
        text,
    )
    text = re.sub(
        r"(?im)^\s*(?:situation(?:\s*/\s*task)?|task|actions?|result(?:\s+and\s+impact)?|impact)\s*[:\-]\s*",
        "",
        text,
    )
    text = re.sub(
        r"(?i)\b(?:situation(?:\s*/\s*task)?|task|actions?|result(?:\s+and\s+impact)?|impact)\s*:\s*",
        "",
        text,
    )
    text = re.sub(r"[ \t]{2,}", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()


def _normalize_body_whitespace(text: str) -> str:
    """
    Fix word-breaks and preserve paragraphs:
      - 'un-\nstructured' -> 'unstructured'
      - Join accidental intra-word breaks: 'f\n ields' -> 'fields'
      - Single newline -> space; double newline -> paragraph
    """
    if not text:
        return ""
    text = re.sub(r"(\w)-\s*\n\s*(\w)", r"\1\2", text)             # hyphenated break
    text = re.sub(r"([A-Za-z])\s*\n\s*([A-Za-z])", r"\1\2", text)  # intra-word break
    text = re.sub(r"(?<!\n)\n(?!\n)", " ", text)                   # single newline -> space
    return re.sub(r"[ \t]{2,}", " ", text).strip()


def _debullettify_and_dedash(text: str) -> str:
    """
    Remove enumerations and dashy separators to keep prose natural:
      - '(1)', '(2)' anywhere -> ''
      - Sentence/phrase-leading '1) ' or '1. ' -> ''
      - 'â€”'/'â€“'/'--' or spaced ' - ' as separators -> comma + space
      - Collapse duplicate commas/spaces
    """
    if not text:
        return ""

    # Remove paren-numbers like (1), (2) etc. (keep years like (2025) by limiting to 1â€“2 digits)
    text = re.sub(r"\(\s*[0-9]{1,2}\s*\)\s*", "", text)

    # Remove leading enumerations at sentence/phrase starts: "1) step", "2. step"
    text = re.sub(r"(^|[.?!]\s+)\d{1,2}[.)]\s*", r"\1", text)

    # Replace em/en dashes and double hyphens with commas
    text = re.sub(r"\s*(?:â€”|â€“|--)\s*", ", ", text)

    # Replace spaced single dash used as a separator with a comma
    text = re.sub(r"\s-\s", ", ", text)

    # Trim excess commas/spaces
    text = re.sub(r"\s*,\s*,\s*", ", ", text)
    text = re.sub(r"[ \t]{2,}", " ", text)
    return text.strip()


def _postprocess_body(text: str) -> str:
    """Cleanup + humanize punctuation + LaTeX safety in the right order."""
    text = secure_tex_input(text or "")
    text = _strip_star_labels(text)
    text = _normalize_body_whitespace(text)
    text = _debullettify_and_dedash(text)
    return _latex_escape_light(text)


async def chat_text(system: str, user: str, model: str) -> str:
    resp = openai_client.chat.completions.create(
        model=model,
        messages=[{"role": "system", "content": system}, {"role": "user", "content": user}],
    )
    return (resp.choices[0].message.content or "").strip()


async def chat_json(user_prompt: str, model: str) -> dict:
    resp = openai_client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": user_prompt}],
    )
    content = (resp.choices[0].message.content or "").strip()
    return _json_from_text(content, {})


# -----------------------------
# Extract Company + Role
# -----------------------------
async def extract_company_role(jd_text: str) -> Tuple[str, str]:
    jd_excerpt = (jd_text or "").strip()[:4000]
    prompt = (
        "Extract company name and the exact role title from the job description below.\n"
        'Return STRICT JSON (no prose) like {"company":"â€¦","role":"â€¦"}.\n\n'
        f"JD:\n{jd_excerpt}"
    )
    try:
        data = await chat_json(prompt, model=_EXTRACT_MODEL)
        return (data.get("company") or "Company").strip(), (data.get("role") or "Role").strip()
    except Exception as e:
        log_event("coverletter_extract_fail", {"error": str(e)})
        return "Company", "Role"


# ============================================================
# Draft Cover-Letter Body â€” enforced & grounded generation
# ============================================================

# Config knobs
_SENT_MIN, _SENT_MAX = 12, 28                   # sentence length band (words)
_TOTAL_KW_MIN, _TOTAL_KW_MAX = 5, 9            # total JD keywords to weave
_PER_SENT_KW_MAX = 2                           # keyword density cap per sentence
_LENGTH_BANDS = {"short": (120, 180), "standard": (180, 280), "long": (300, 400)}

_BUZZ_DEFAULT = [
    "passionate", "dynamic", "cutting edge", "team player",
    "synergy", "results-driven", "fast-paced", "leverage synergies",
]
_VERB_VARIANTS = ["built", "designed", "shipped", "created", "developed",
                  "implemented", "delivered", "launched", "scaled", "optimized"]

_WORD = re.compile(r"[A-Za-z0-9][A-Za-z0-9\-\./_+]*")
_SENT_SPLIT = re.compile(r"(?<=[.!?])\s+")
_CAMEL = re.compile(r"[A-Z][a-z0-9]+[A-Z][A-Za-z0-9]+")
_TOOLISH = re.compile(r"[A-Za-z]+[0-9]+|[A-Za-z]+\.[A-Za-z]+|[-_/]")
_STOPWORDS = set("""
a an the and or but if while for with to of in on by from as at into over under
is are was were be been being this that these those i you he she they we it
""".split())

def _norm(s: str) -> str:
    return (s or "").replace("&", " and ").replace("â€”", ", ").replace("â€“", ", ").replace("--", ", ").strip()

def _tokenize(text: str) -> List[str]:
    return [t.lower() for t in _WORD.findall(text)]

def _extract_terms(text: str) -> set[str]:
    terms = set()
    for tok in set(_WORD.findall(text or "")):
        raw = tok.strip()
        if not raw:
            continue
        low = raw.lower()
        if low in _STOPWORDS or len(low) < 2:
            continue
        if _TOOLISH.search(raw) or _CAMEL.search(raw) or "-" in raw or "." in raw or "/" in raw:
            terms.add(raw)
        elif low in {"pytorch", "tensorflow", "spark", "airflow", "docker", "kubernetes",
                     "bigquery", "snowflake", "hive", "flink", "scikit", "sklearn",
                     "xgboost", "lightgbm", "postgres", "mysql", "redis", "elasticsearch",
                     "fastapi", "flask", "ray", "rag", "retrieval"}:
            terms.add(raw)
    return set(sorted(terms, key=str.lower))

def _extract_responsibilities(jd_text: str) -> List[str]:
    verbs = r"(own|lead|drive|design|build|ship|develop|maintain|scale|optimiz|evaluate|deploy|integrate|collaborate|partner)"
    lines = re.split(r"[\n;]|(?<=[.!?])\s+", jd_text or "")
    reps = []
    for ln in lines:
        s = ln.strip()
        if not s:
            continue
        if re.search(rf"^\s*-?\s*{verbs}\b", s, re.IGNORECASE) or re.search(rf"\b{verbs}\b", s, re.IGNORECASE):
            s = re.sub(r"^\s*[-*â€¢]\s*", "", s)
            s = re.sub(r"\s{2,}", " ", s).strip().rstrip(";")
            if 6 <= len(s.split()) <= 25:
                reps.append(s)
    # dedupe preserving order
    seen = set()
    out = []
    for r in reps:
        k = r.lower()
        if k not in seen:
            seen.add(k)
            out.append(r)
    return out[:12]

def _overlap_priority(jd_terms: set[str], resume_terms: set[str]) -> tuple[List[str], List[str], List[str]]:
    jd_low, res_low = {t.lower(): t for t in jd_terms}, {t.lower(): t for t in resume_terms}
    shared = [jd_low[k] for k in jd_low.keys() & res_low.keys()]
    jd_only = [jd_low[k] for k in jd_low.keys() - res_low.keys()]
    res_only = [res_low[k] for k in res_low.keys() - jd_low.keys()]
    shared.sort(key=str.lower); jd_only.sort(key=str.lower); res_only.sort(key=str.lower)
    return shared, jd_only, res_only

def _dedupe_verbs_local(text: str) -> str:
    sentences = _SENT_SPLIT.split(text.strip())
    seen = set()
    for i, s in enumerate(sentences):
        m = re.match(r"^([A-Za-z]+)\b", s.strip())
        if not m:
            continue
        v = m.group(1).lower()
        if v in seen and v in _VERB_VARIANTS:
            idx = _VERB_VARIANTS.index(v)
            newv = _VERB_VARIANTS[(idx + 3) % len(_VERB_VARIANTS)]
            sentences[i] = re.sub(r"^[A-Za-z]+\b", newv.capitalize(), s.strip(), count=1)
        seen.add(v)
    return " ".join(sentences)

def _clean_text_local(s: str, banned_phrases: Optional[List[str]] = None) -> str:
    txt = _norm(s)
    txt = re.sub(r"^\s*(?:[#`>\-\*â€¢]|\d+[.)])\s+", "", txt, flags=re.MULTILINE)
    banned = set((banned_phrases or []) + _BUZZ_DEFAULT)
    for b in sorted(banned, key=len, reverse=True):
        txt = re.sub(rf"\b{re.escape(b)}\b", "", txt, flags=re.IGNORECASE)
    txt = re.sub(r"\bGPA\b[:\s]?\d+(\.\d+)?", "", txt, flags=re.IGNORECASE)
    txt = re.sub(r"\bcourse(s|work)?\b.*?(completed|including|such as).*$", "", txt, flags=re.IGNORECASE)
    txt = re.sub(r"\b(visa|relocation|sponsorship|available from)\b.*?$", "", txt, flags=re.IGNORECASE)
    txt = re.sub(r"[\[\]\{\}]+", "", txt)
    txt = re.sub(r"\s+,", ",", txt)
    txt = re.sub(r"\s{2,}", " ", txt).strip()
    return txt

def _enforce_word_band_local(text: str, length: str) -> str:
    lo, hi = _LENGTH_BANDS.get(length, (180, 280))
    words = text.split()
    if len(words) <= hi and len(words) >= lo:
        return text
    sentences = _SENT_SPLIT.split(text.strip())
    out = []
    for s in sentences:
        candidate = " ".join(out + [s])
        if len(candidate.split()) <= hi:
            out.append(s)
        else:
            break
    trimmed = " ".join(out).strip()
    if len(trimmed.split()) < lo:
        extra = [x for x in sentences[len(out):] if x.strip()]
        if extra:
            trimmed = (trimmed + " " + extra[0]).strip()
    return trimmed

def _sentence_stats(text: str) -> List[int]:
    return [len(s.split()) for s in _SENT_SPLIT.split(text.strip()) if s.strip()]

def _enforce_sentence_band(text: str) -> str:
    sentences = [s.strip() for s in _SENT_SPLIT.split(text.strip()) if s.strip()]
    fixed = []
    i = 0
    while i < len(sentences):
        s = sentences[i]
        n = len(s.split())
        if n > _SENT_MAX:
            split = re.split(r"; |, and |, ", s, maxsplit=1)
            if len(split) == 2:
                a, b = split
                if a and b:
                    sentences.insert(i+1, b.strip().capitalize())
                    s = a.strip()
                    n = len(s.split())
        if n < _SENT_MIN and i + 1 < len(sentences):
            s = (s.rstrip(",") + ", " + sentences[i+1].lstrip().lower())
            i += 1
        fixed.append(s)
        i += 1
    return " ".join(fixed)

def _shape_paragraphs(text: str, mode: str) -> str:
    sents = [s.strip() for s in _SENT_SPLIT.split(text.strip()) if s.strip()]
    if not sents:
        return text.strip()
    if mode == "short":
        cut = max(1, min(len(sents)-1, len(sents)//3))
        return (" ".join(sents[:cut]).strip() + "\n\n" + " ".join(sents[cut:]).strip())
    n = len(sents)
    i1 = max(1, min(n-2, n//5))
    i2 = max(i1+1, min(n-1, (n*4)//5))
    return (" ".join(sents[:i1]).strip()
            + "\n\n" + " ".join(sents[i1:i2]).strip()
            + "\n\n" + " ".join(sents[i2:]).strip())

def _keywords_in_text(text: str, keywords: List[str]) -> Dict[str, int]:
    counts: Dict[str, int] = {}
    low = text.lower()
    for k in keywords:
        if not k:
            continue
        pat = re.escape(k.lower())
        c = len(re.findall(rf"\b{pat}\b", low))
        if c:
            counts[k] = c
    return counts

def _per_sentence_kw_counts(text: str, keywords: List[str]) -> List[int]:
    sents = [s.strip() for s in _SENT_SPLIT.split(text.strip()) if s.strip()]
    res = []
    for s in sents:
        c = sum(1 for k in keywords if re.search(rf"\b{re.escape(k.lower())}\b", s.lower()))
        res.append(c)
    return res

def _allowed_terms(jd_terms: set[str], resume_terms: set[str]) -> set[str]:
    return set(t.lower() for t in jd_terms | resume_terms | set(_tokenize(" ".join(jd_terms | resume_terms))))

def _oov_terms(text: str, allowed: set[str]) -> set[str]:
    cand = set()
    for tok in set(_WORD.findall(text)):
        low = tok.lower()
        if low in _STOPWORDS:
            continue
        if _TOOLISH.search(tok) or _CAMEL.search(tok) or "-" in tok or "." in tok or "/" in tok:
            if low not in allowed:
                cand.add(tok)
    return cand

def _choose_kw_list(shared: List[str], jd_only: List[str], max_n: int = 14) -> List[str]:
    out = shared[:]
    for t in jd_only:
        if t not in out:
            out.append(t)
        if len(out) >= max_n:
            break
    return out

def _has_specific_company_detail(text: str, facts: List[str]) -> bool:
    if not facts:
        return True
    low = text.lower()
    for f in facts:
        f = (f or "").strip()
        if not f:
            continue
        parts = [p for p in re.split(r"\s+", f.lower()) if p and p not in _STOPWORDS]
        if len(parts) >= 2 and " ".join(parts) in low:
            return True
        if any(len(p) > 5 and re.search(rf"\b{re.escape(p)}\b", low) for p in parts):
            return True
    return False

def _has_why_now(text: str) -> bool:
    return bool(re.search(r"\b(recent|recently|now|right now|this quarter|this year|launch|launched|rollout|roadmap|momentum)\b", text.lower()))

async def draft_cover_body(
    jd_text: str,
    resume_text: str,
    company: str,
    role: str,
    tone: str,
    length: str,
    company_facts: Optional[List[str]] = None,
    first_30_day_ideas: Optional[List[str]] = None,
    banned_phrases: Optional[List[str]] = None,
) -> str:
    """
    BODY only; enforced structure + grounding:
      - Extract JD keywords & responsibilities; prefer JDâ€“resume overlap
      - Enforce 5â€“9 total keywords, â‰¤2 per sentence
      - Sentence length band; paragraph shaping by length
      - OOV grounding against JDâˆªresume terms; fuzzy repair
      - Validate specific company detail and a â€œwhy nowâ€ reason
    """
    try:
        tone = (tone or "balanced").strip().lower()
        length = (length or "standard").strip().lower()
        if length not in _LENGTH_BANDS:
            length = "standard"

        # Pre-extraction and grounding
        jd_terms = _extract_terms(jd_text or "")
        resume_terms = _extract_terms(resume_text or "")
        shared, jd_only, _ = _overlap_priority(jd_terms, resume_terms)
        preferred_kw_pool = _choose_kw_list(shared, jd_only, max_n=20)

        if len(preferred_kw_pool) < _TOTAL_KW_MIN:
            toks = [t for t in _tokenize(jd_text) if t not in _STOPWORDS and len(t) > 3]
            freq: Dict[str, int] = {}
            for t in toks:
                freq[t] = freq.get(t, 0) + 1
            for t, _n in sorted(freq.items(), key=lambda x: (-x[1], x[0])):
                if t not in preferred_kw_pool:
                    preferred_kw_pool.append(t)
                if len(preferred_kw_pool) >= 20:
                    break

        jd_resps = _extract_responsibilities(jd_text or "")
        allowed = _allowed_terms(jd_terms, resume_terms)

        # Prompt with JSON scaffold and strict shape
        facts_str = ""
        if company_facts:
            slim = [f.strip() for f in company_facts if f and f.strip()]
            if slim:
                facts_str = "Company facts (may reference 1): " + " | ".join(slim[:3])

        ideas_str = ""
        if first_30_day_ideas:
            slim = [f.strip() for f in first_30_day_ideas if f and f.strip()]
            if slim:
                ideas_str = "Seed first-30-day ideas (optional, choose at most 1): " + " | ".join(slim[:3])

        length_hint = {
            "short": "Limit to ~120â€“180 words; 2 paragraphs.",
            "standard": "Target ~180â€“280 words; 3 paragraphs.",
            "long": "Allow ~300â€“400 words; 3 paragraphs.",
        }[length]

        sys_prompt = f"""
You will produce TWO outputs in order:

(1) A STRICT JSON object:
{{
 "thesis": "1â€“2 sentences that name {company} and {role} and a 'why now'",
 "story": "4â€“6 sentences: challenge â†’ actions (tools) â†’ outcome (numbers)",
 "mapping": ["exact JD responsibility 1", "exact JD responsibility 2", "optional 3"],
 "plan": "1â€“2 sentences: realistic 30â€“60 day experiment using their stack",
 "ask": "1 sentence interview ask",
 "keywords_used": ["k1","k2","..."]  // 5â€“9 items chosen ONLY from the ALLOWED_KEYWORDS list below
}}

(2) Then plain BODY paragraphs only (no bullets, no headings, no code), merging the JSON fields into:
- {('2 paragraphs' if length=='short' else '3 paragraphs')} with the shape:
  P1 thesis, P2 story+mapping, {('P3 omitted for short' if length=='short' else 'P3 plan+ask')}

Rules:
- First-person singular; energetic, not hypey. Plain English. No clichÃ©s.
- Ground everything strictly in the JD and resume. Do NOT invent employers, tools, results, or dates.
- Use "and" instead of "&". No em/en dashes; use commas or conjunctions.
- No GPA, course lists, relocation/visa/availability notes.
- Weave exactly 5â€“9 keywords from ALLOWED_KEYWORDS total, max 2 per sentence.
- Explicitly reuse 2â€“3 responsibilities from JD_RESPONSIBILITIES in "mapping".
- {length_hint}
- After the JSON, output only the paragraphs (no JSON, no commentary).

ALLOWED_KEYWORDS (prioritize overlap with resume):
{preferred_kw_pool}

JD_RESPONSIBILITIES (pick 2â€“3 to map):
{jd_resps}

Quality checks (you must satisfy):
- Names company, role, and a â€œwhy nowâ€ tied to a specific detail if possible.
- One measurable outcome in the story (%, #, $, latency, adoption, risk).
- Keyword total 5â€“9; â‰¤2 per sentence.
- Paragraph count as specified; no bullets or step numbers.

{facts_str}
{ideas_str}
""".strip()

        user_prompt = (
            f"JOB DESCRIPTION (<=4k):\n{(jd_text or '')[:4000]}\n\n"
            f"RESUME (raw; may contain LaTeX, <=4k):\n{(resume_text or '')[:4000]}\n\n"
            "Return JSON first, then the body paragraphs."
        )

        draft = await chat_text(sys_prompt, user_prompt, model=_DRAFT_MODEL)

        # remove initial JSON blob if present
        body_v1 = re.sub(r"^\s*\{.*?\}\s*", "", draft.strip(), flags=re.DOTALL)
        body_v1 = _clean_text_local(body_v1, banned_phrases=banned_phrases)

        def _stats(text: str) -> Dict[str, object]:
            kw_counts = _keywords_in_text(text, preferred_kw_pool)
            per_sent = _per_sentence_kw_counts(text, preferred_kw_pool)
            oov = _oov_terms(text, allowed)
            has_detail = _has_specific_company_detail(text, company_facts or [])
            has_why = _has_why_now(text)
            s_lens = _sentence_stats(text)
            return {
                "total_keywords_used": len([k for k,v in kw_counts.items() if v > 0]),
                "kw_counts": kw_counts,
                "kw_per_sentence": per_sent,
                "oov_terms": list(sorted(oov)),
                "has_company_detail": has_detail,
                "has_why_now": has_why,
                "sentence_lengths": s_lens,
                "paragraphs": len([p for p in text.split("\n") if p.strip()]),
            }

        stats = _stats(body_v1)

        async def _repair(reason: str, text_in: str,
                          enforce_add: List[str] = None,
                          enforce_drop: List[str] = None,
                          target_paras: int = 3) -> str:
            enforce_add = enforce_add or []
            enforce_drop = enforce_drop or []
            add_str = ", ".join(enforce_add) if enforce_add else "none"
            drop_str = ", ".join(enforce_drop) if enforce_drop else "none"
            r_prompt = f"""
Rewrite the cover-letter BODY for "{role}" at "{company}" to FIX: {reason}.
Constraints:
- Keep all facts grounded in JD and resume (no new employers/tools/results/dates).
- Keep tone first-person, plain English, no clichÃ©s, no bullets or lists.
- Use EXACTLY {_TOTAL_KW_MIN}-{_TOTAL_KW_MAX} keywords total from this list, max {_PER_SENT_KW_MAX} per sentence:
  {preferred_kw_pool}
- Prefer overlap keywords; avoid overstuffing any sentence.
- Paragraphs: {target_paras} total ({'P1 thesis, P2 story+mapping' if target_paras==2 else 'P1 thesis, P2 story+mapping, P3 plan+ask'}).
- Add these keywords if missing (optional): {add_str}
- Remove or replace these terms if present: {drop_str}
Return only the body paragraphs.
"""
            revised = await chat_text(r_prompt, f"JD:\n{(jd_text or '')[:3000]}\n\nResume:\n{(resume_text or '')[:3000]}\n\nDraft:\n{text_in}", model=_DRAFT_MODEL)
            return _clean_text_local(revised, banned_phrases=banned_phrases)

        # OOV grounding
        if stats["oov_terms"]:
            replacements = {}
            for bad in stats["oov_terms"]:
                cand = difflib.get_close_matches(bad.lower(), [t.lower() for t in preferred_kw_pool], n=1, cutoff=0.72)
                if cand:
                    replacements[bad] = cand[0]
            if replacements:
                body_v1 = re.sub(
                    "|".join(map(re.escape, replacements.keys())),
                    lambda m: replacements[m.group(0)],
                    body_v1
                )
            else:
                body_v1 = await _repair("remove or replace out-of-vocabulary tool names with JD/resume tools", body_v1, target_paras=(2 if length=="short" else 3))
            stats = _stats(body_v1)

        # Total keyword count 5â€“9
        total_kw_used = stats["total_keywords_used"]
        if total_kw_used < _TOTAL_KW_MIN:
            missing = [k for k in preferred_kw_pool if k not in stats["kw_counts"]][:(_TOTAL_KW_MIN - total_kw_used)]
            body_v1 = await _repair(f"raise total distinct keyword count to between {_TOTAL_KW_MIN} and {_TOTAL_KW_MAX}", body_v1, enforce_add=missing, target_paras=(2 if length=="short" else 3))
            stats = _stats(body_v1)
        elif total_kw_used > _TOTAL_KW_MAX:
            extras = sorted(stats["kw_counts"], key=lambda k: (k.lower() not in [s.lower() for s in shared], stats["kw_counts"][k]), reverse=True)
            drop = extras[:max(0, total_kw_used - _TOTAL_KW_MAX)]
            body_v1 = await _repair(f"reduce total distinct keyword count to at most {_TOTAL_KW_MAX}", body_v1, enforce_drop=drop, target_paras=(2 if length=="short" else 3))
            stats = _stats(body_v1)

        # Per-sentence density cap â‰¤ 2
        if any(c > _PER_SENT_KW_MAX for c in stats["kw_per_sentence"]):
            body_v1 = await _repair(f"reduce keyword density to â‰¤{_PER_SENT_KW_MAX} per sentence", body_v1, target_paras=(2 if length=="short" else 3))
            stats = _stats(body_v1)

        # Sentence length band
        if any(n < _SENT_MIN or n > _SENT_MAX for n in stats["sentence_lengths"]):
            body_v1 = _enforce_sentence_band(body_v1)
            stats = _stats(body_v1)

        # Specific company detail + why-now
        if not _has_specific_company_detail(body_v1, company_facts or []):
            body_v1 = await _repair("include one specific company detail from facts and tie it to 'why now'", body_v1, target_paras=(2 if length=="short" else 3))
            stats = _stats(body_v1)
        if not _has_why_now(body_v1):
            body_v1 = await _repair("add a concise 'why now' reason anchored to a product, launch, metric, or initiative", body_v1, target_paras=(2 if length=="short" else 3))
            stats = _stats(body_v1)

        # Paragraph count/shape control
        paras_target = 2 if length == "short" else 3
        body_v1 = _shape_paragraphs(body_v1, length)
        par_count = len([p for p in body_v1.split("\n") if p.strip()])
        if par_count != paras_target:
            body_v1 = await _repair(f"ensure exactly {paras_target} paragraphs with the required shape", body_v1, target_paras=paras_target)
            body_v1 = _shape_paragraphs(body_v1, length)

        # Final local post-processing
        body_v1 = _dedupe_verbs_local(body_v1)
        body_v1 = _enforce_word_band_local(body_v1, length)
        body_v1 = re.sub(r"(^|\n)\s*(?:[-*â€¢]|\d+[.)])\s+", " ", body_v1)

        return _postprocess_body(body_v1)

    except Exception as e:
        log_event("coverletter_draft_fail", {"error": str(e)})
        raise HTTPException(status_code=500, detail=f"Body generation failed: {e}")


# -----------------------------
# Humanize via internal service
# -----------------------------
async def humanize_text(body_text: str, tone: str) -> str:
    api_base = (getattr(config, "API_BASE_URL", "") or "").rstrip("/") or "http://127.0.0.1:8000"
    url = f"{api_base}/api/superhuman/rewrite"
    payload = {"text": body_text, "mode": "coverletter", "tone": tone, "latex_safe": True}
    try:
        async with httpx.AsyncClient(timeout=120.0) as client:
            r = await client.post(url, json=payload)
        r.raise_for_status()
        data = r.json()
        return data.get("rewritten") or data.get("text") or body_text
    except Exception as e:
        log_event("superhuman_handoff_fail", {"error": str(e)})
        return body_text


# -----------------------------
# Header placeholder filling
# -----------------------------
def _fill_header_fields(
    tex: str,
    *,
    company: str,
    role: str,
    candidate: str,
    date_str: str,
    email: str = "",
    phone: str = "",
    citystate: str = "",
) -> str:
    def esc(v: str) -> str:
        return _latex_escape_light(secure_tex_input(v or ""))

    subst = {
        "COMPANY": company,
        "ROLE": role,
        "CANDIDATE_NAME": candidate,
        "NAME": candidate,
        "DATE": date_str,
        "EMAIL": email,
        "PHONE": phone,
        "CITYSTATE": citystate,
    }
    for k, v in subst.items():
        tex = tex.replace(f"{{{{{k}}}}}", esc(v))
        tex = tex.replace(f"%<<{k}>>%", esc(v))

    patterns = {
        r"(\\def\\Company\{)(.*?)(\})": company,
        r"(\\def\\Role\{)(.*?)(\})": role,
        r"(\\def\\CandidateName\{)(.*?)(\})": candidate,
        r"(\\def\\Date\{)(.*?)(\})": date_str,
    }
    for pat, val in patterns.items():
        tex = re.sub(pat, lambda m: f"{m.group(1)}{esc(val)}{m.group(3)}", tex, flags=re.I)

    tex = re.sub(r"(^|\n)\s*Company\s*$", lambda m: f"{m.group(1)}{esc(company)}", tex, flags=re.M)
    tex = re.sub(r"(^|\n)\s*Your Name\s*$", lambda m: f"{m.group(1)}{esc(candidate)}", tex, flags=re.M)
    return tex


# -----------------------------
# Precise body injection helpers
# -----------------------------
def _inject_between_salutation_and_signoff(base_tex: str, body_tex: str) -> Optional[str]:
    pat = r"(Dear[^\n]*?,\s*\n)([\s\S]*?)(\n\s*Sincerely,\s*\\\\[\s\S]*?$)"
    if re.search(pat, base_tex, flags=re.I):
        return re.sub(pat, lambda m: f"{m.group(1)}{body_tex}\n{m.group(3)}", base_tex, flags=re.I)
    return None


# -----------------------------
# Inject Body into LaTeX Template
# -----------------------------
def inject_body_into_template(base_tex: str, body_tex: str) -> str:
    swapped = _inject_between_salutation_and_signoff(base_tex, body_tex)
    if swapped is not None:
        return swapped

    if _shared_inject and not _DISABLE_SHARED_INJECTOR:
        try:
            return _shared_inject(base_tex, body_tex)
        except Exception as e:  # pragma: no cover
            log_event("shared_inject_fail", {"error": str(e)})

    safe_body = re.sub(r"\\documentclass[\s\S]*?\\begin\{document\}", "", body_tex or "", flags=re.I)
    safe_body = re.sub(r"\\end\{document\}\s*$", "", safe_body, flags=re.I).strip()

    anchor_pat = r"(%-+BODY-START-+%)(.*?)(%-+BODY-END-+%)"
    if re.search(anchor_pat, base_tex, flags=re.S):
        return re.sub(anchor_pat, lambda m: f"{m.group(1)}\n{safe_body}\n{m.group(3)}", base_tex, flags=re.S)

    if re.search(r"\\end\{document\}\s*$", base_tex, flags=re.I):
        return re.sub(
            r"\\end\{document\}\s*$",
            lambda m: f"\n% (Auto-inserted by HIREX)\n{safe_body}\n\\end{{document}}\n",
            base_tex,
            flags=re.I,
        )

    return base_tex.rstrip() + f"\n\n% (Auto-inserted by HIREX)\n{safe_body}\n\\end{{document}}\n"


# -----------------------------
# Main Endpoint
@router.post("")
async def generate_coverletter(
    jd_text: str = Form(...),
    resume_tex: str = Form(""),
    use_humanize: bool = Form(True),
    tone: str = Form("balanced"),
    length: str = Form("standard"),
):
    if not (config.OPENAI_API_KEY or "").strip():
        raise HTTPException(status_code=400, detail="OPENAI_API_KEY missing in environment.")
    if not (jd_text or "").strip():
        raise HTTPException(status_code=400, detail="jd_text is required.")

    company, role = await extract_company_role(jd_text)

    company_slug = safe_filename(company)
    role_slug = safe_filename(role)
    context_key = f"{company_slug}__{role_slug}"

    body_text = await draft_cover_body(
        jd_text, resume_tex, company, role, tone, length
    )

    if use_humanize:
        body_text = await humanize_text(body_text, tone)
        body_text = _postprocess_body(body_text)

    base_path = config.BASE_COVERLETTER_PATH
    try:
        with open(base_path, encoding="utf-8") as f:
            base_tex = f.read()
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail=f"Base cover-letter template not found at {base_path}")

    today_str = datetime.now().strftime("%B %d, %Y")
    candidate = getattr(config, "CANDIDATE_NAME", "Sri Akash Kadali")
    applicant_email = getattr(config, "APPLICANT_EMAIL", "kadali18@umd.edu")
    applicant_phone = getattr(config, "APPLICANT_PHONE", "+1 240-726-9356")
    applicant_city = getattr(config, "APPLICANT_CITYSTATE", "College Park, Maryland")

    base_tex = _fill_header_fields(
        base_tex,
        company=company,
        role=role,
        candidate=candidate,
        date_str=today_str,
        email=applicant_email,
        phone=applicant_phone,
        citystate=applicant_city,
    )

    try:
        injected = inject_body_into_template(base_tex, body_text)
    except re.error as e:
        log_event("inject_regex_error", {"error": str(e)})
        injected = f"{base_tex}\n\n% (Fallback inject due to regex error)\n{body_text}\n"
        if not injected.strip().endswith("\\end{document}"):
            injected += "\n\\end{document}\n"

    final_tex = render_final_tex(injected)
    pdf_bytes = compile_latex_safely(final_tex) or b""
    pdf_b64 = base64.b64encode(pdf_bytes).decode("utf-8")

    # Save final PDF into samples path per your spec
    out_pdf_path = config.get_sample_coverletter_pdf_path(company, role)
    ensure_dir(out_pdf_path.parent)
    if pdf_bytes:
        out_pdf_path.write_bytes(pdf_bytes)

    # Persist into stable context (dedup by Company__Role)
    ctx_dir = config.get_contexts_dir()
    ensure_dir(ctx_dir)
    ctx_path = ctx_dir / f"{context_key}.json"

    existing: Dict[str, Any] = {}
    if ctx_path.exists():
        try:
            existing = json.loads(ctx_path.read_text(encoding="utf-8"))
        except Exception:
            existing = {}

    context_payload: Dict[str, Any] = {
        **existing,
        "key": context_key,
        "title_for_memory": f"{company_slug}_{role_slug}_{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}",
        "company": company,
        "role": role,
        "jd_text": jd_text or existing.get("jd_text", ""),
        "cover_letter": {
            **(existing.get("cover_letter") or {}),
            "tex": final_tex,
            "pdf_path": str(out_pdf_path),
            "pdf_b64": pdf_b64,
            "tone": tone,
            "length": length,
            "humanized": bool(use_humanize),
        },
        "updated_at": datetime.utcnow().isoformat() + "Z",
    }
    ctx_path.write_text(json.dumps(context_payload, ensure_ascii=False, indent=2), encoding="utf-8")

    log_event(
        "coverletter_saved",
        {
            "company": company,
            "role": role,
            "tone": tone,
            "use_humanize": use_humanize,
            "length": length,
            "chars": len(body_text or ""),
            "pdf_path": str(out_pdf_path),
            "context_path": str(ctx_path),
            "context_key": context_key,
        },
    )

    return JSONResponse(
        {
            "company": company,
            "role": role,
            "tone": tone,
            "use_humanize": use_humanize,
            "tex_string": final_tex,
            "pdf_base64": pdf_b64,
            "pdf_path": str(out_pdf_path),
            "context_key": context_key,
            "context_path": str(ctx_path),
            "id": context_key,
            "memory_id": context_key,
        }
    )
-e 


-e # ===== FILE: ./api/render_tex.py =====

"""
HIREX â€¢ api/render_tex.py
Template renderer for final LaTeX resume output.
Simplified for direct LaTeX input (no JSON parsing).
Author: Sri Akash Kadali
"""

import re
from backend.core.utils import log_event


# ============================================================
# ðŸ§  Direct LaTeX Renderer
# ============================================================
def render_final_tex(final_tex: str) -> str:
    """
    Returns the LaTeX text exactly as received from GPT/Humanize,
    after performing minimal safety cleanup and normalization.

    Args:
        final_tex (str): Full LaTeX document string (already formatted)
    Returns:
        str: Safe LaTeX text ready for compilation
    """
    if not isinstance(final_tex, str):
        raise ValueError("render_final_tex() expects a LaTeX string input.")

    # --- Trim and clean any code-fence artifacts ---
    cleaned = (
        final_tex.replace("```latex", "")
        .replace("```", "")
        .strip()
    )

    # --- Normalize line endings ---
    cleaned = cleaned.replace("\r\n", "\n").replace("\r", "\n")

    # --- Remove stray leading/trailing blank lines ---
    cleaned = re.sub(r"^\s*\n", "", cleaned)
    cleaned = re.sub(r"\n\s*$", "\n", cleaned)

    # --- Basic validation warnings ---
    if not re.search(r"\\documentclass", cleaned):
        log_event("âš ï¸ [RENDER] Missing \\documentclass header.")
    if "\\begin{document}" not in cleaned:
        log_event("âš ï¸ [RENDER] Missing \\begin{document} block.")

    # --- Ensure proper closing tag ---
    if not cleaned.strip().endswith("\\end{document}"):
        cleaned += "\n\\end{document}\n"

    # --- Collapse excessive blank lines for cleanliness ---
    cleaned = re.sub(r"\n{3,}", "\n\n", cleaned)

    log_event("âœ… [RENDER] Final LaTeX render complete and safe for compilation.")
    return cleaned


# ============================================================
# ðŸ§ª Local Test
# ============================================================
if __name__ == "__main__":
    sample_tex = r"""
\documentclass{article}
\begin{document}
Hello World

\section*{Education}
University of Maryland, College Park

\end{document}
"""
    print(render_final_tex(sample_tex))-e 


-e # ===== FILE: ./api/dashboard.py =====

# ============================================================
#  ASTRA v2.1.2 â€” Dashboard Analytics & History Endpoint (FINAL)
#  ------------------------------------------------------------
#  Provides:
#   â€¢ Aggregated event summaries (counts, not clones)
#   â€¢ Tone/mode analytics
#   â€¢ Weekly trend data (Mon..Sun)
#   â€¢ Recent history listing (deduped per Company__Role)
#   â€¢ Event type registry
#   â€¢ Robust log reading & safe JSONL parsing
#  Author: Sri Akash Kadali
# ============================================================

from __future__ import annotations

import json
from collections import Counter
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from fastapi import APIRouter, Query
from fastapi.responses import JSONResponse

from backend.core import config

router = APIRouter(prefix="/api/dashboard", tags=["dashboard"])

# ============================================================
# ðŸ“ Paths (read from config only; directories auto-created)
# ============================================================
LOG_PATH = Path(config.LOG_PATH)
HISTORY_PATH = Path(config.HISTORY_PATH)

for p in (LOG_PATH.parent, HISTORY_PATH.parent):
    p.mkdir(parents=True, exist_ok=True)


# ============================================================
# ðŸ§© Helpers: Time / JSONL / Normalization / Dedupe
# ============================================================
def _now_iso() -> str:
    """UTC now in ISO-8601 with trailing Z."""
    return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")


def _read_jsonl(path: Path, limit: int = 500) -> List[Dict[str, Any]]:
    """Safely read the last N lines of a JSONL file (newest first)."""
    if not path.exists():
        return []
    try:
        with open(path, "r", encoding="utf-8") as f:
            lines = f.readlines()[-limit:]
        records: List[Dict[str, Any]] = []
        # Reverse so newest first
        for line in reversed(lines):
            try:
                obj = json.loads(line)
                if isinstance(obj, dict):
                    records.append(obj)
            except json.JSONDecodeError:
                continue
        return records
    except Exception:
        return []


def _iso(ts: Optional[str]) -> str:
    """Coerce to ISO timestamp string (safe fallback to now)."""
    if not ts:
        return _now_iso()
    try:
        _ = datetime.fromisoformat(ts.replace("Z", "+00:00"))
        return ts
    except Exception:
        return _now_iso()


def _event_name(e: Dict[str, Any]) -> str:
    """Normalize event/type name."""
    return (e.get("event") or e.get("type") or "unknown").lower()


def _company_role_from_meta(e: Dict[str, Any]) -> Tuple[str, str]:
    """Extract (company, role) from common locations."""
    m = e.get("meta") or {}
    company = (m.get("company") or e.get("company") or "").strip()
    role = (m.get("role") or e.get("role") or "").strip()
    return company, role


def _ts_value(e: Dict[str, Any]) -> float:
    """Timestamp (epoch seconds) for ordering/dedupe."""
    ts_raw = e.get("timestamp") or e.get("time") or ""
    try:
        return datetime.fromisoformat(str(ts_raw).replace("Z", "+00:00")).timestamp()
    except Exception:
        return 0.0


def _dedupe_company_role(records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Collapse multiple records for the same (company, role) combo,
    keeping the most recent one. This prevents dashboard tables
    from exploding with clones of similar actions for the same job.
    """
    best: Dict[Tuple[str, str], Dict[str, Any]] = {}
    for r in records:
        company, role = _company_role_from_meta(r)
        if not (company and role):
            # keep entries with no company/role (diagnostics) â€” single latest
            key = ("", "")
            prev = best.get(key)
            if prev is None or _ts_value(r) > _ts_value(prev):
                best[key] = r
            continue

        k = (company, role)
        prev = best.get(k)
        if prev is None or _ts_value(r) > _ts_value(prev):
            best[k] = r

    # Keep deterministic newest-first order
    out = sorted(best.values(), key=_ts_value, reverse=True)
    return out


# ============================================================
# ðŸ“Š Aggregations
# ============================================================
def summarize_events(events: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Generate analytic aggregates for dashboard visualizations."""
    summary: Dict[str, Any] = {
        "total_events": len(events),
        "optimize_runs": 0,
        "coverletters": 0,
        "superhuman_calls": 0,
        "talk_queries": 0,
        "mastermind_chats": 0,
        "tones": Counter(),
        "modes": Counter(),
        "avg_resume_length": 0.0,
        "distinct_company_roles": 0,
    }

    # Distinct (company, role) counter for high-level dedup metric
    distinct_pairs = set()

    total_len = 0
    len_count = 0
    for e in events:
        evt = _event_name(e)
        meta = e.get("meta", {}) or {}

        if "optimize" in evt:
            summary["optimize_runs"] += 1
        if "coverletter" in evt:
            summary["coverletters"] += 1
        if "superhuman" in evt or "humanize" in evt:
            summary["superhuman_calls"] += 1
        if "talk" in evt:
            summary["talk_queries"] += 1
        if "mastermind" in evt:
            summary["mastermind_chats"] += 1

        tone = str(meta.get("tone", "balanced")).lower()
        mode = str(meta.get("mode", "general")).lower()
        if tone:
            summary["tones"][tone] += 1
        if mode:
            summary["modes"][mode] += 1

        try:
            rl = int(meta.get("resume_len") or 0)
            if rl > 0:
                total_len += rl
                len_count += 1
        except Exception:
            pass

        c, r = _company_role_from_meta(e)
        if c or r:
            distinct_pairs.add((c, r))

    # Average only over entries that actually reported resume_len
    denom = max(len_count, 1)
    summary["avg_resume_length"] = round(total_len / denom, 2)
    summary["distinct_company_roles"] = len(distinct_pairs)

    # Convert counters to plain dicts for JSON
    summary["tones"] = dict(summary["tones"])
    summary["modes"] = dict(summary["modes"])
    return summary


def summarize_history(records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Extract recent high-level activity (for dashboard table)."""
    out: List[Dict[str, Any]] = []
    for h in records:
        meta = h.get("meta", {}) or {}
        ts = _iso(h.get("timestamp") or h.get("time"))
        evt = _event_name(h)
        out.append(
            {
                "timestamp": ts,
                "event": evt,
                "company": meta.get("company", ""),
                "role": meta.get("role", ""),
                "tone": meta.get("tone", "balanced"),
                "score": meta.get("fit_score", None),
                "length": meta.get("resume_len", None),
                "source": h.get("origin", "system"),
            }
        )
    return out


def weekly_trend(records: List[Dict[str, Any]]) -> Dict[str, List[int]]:
    """
    Build Mon..Sun trend counts per category.
    """
    buckets = {
        "optimizations": [0] * 7,
        "coverletters": [0] * 7,
        "superhuman": [0] * 7,
        "mastermind": [0] * 7,
        "talk": [0] * 7,
    }

    def _dow(ts: str) -> int:
        try:
            d = datetime.fromisoformat(ts.replace("Z", "+00:00"))
            # Monday=0
            return d.weekday()
        except Exception:
            return 0

    for r in records:
        evt = _event_name(r)
        ts = _iso(r.get("timestamp") or r.get("time"))
        i = _dow(ts)
        if "optimize" in evt:
            buckets["optimizations"][i] += 1
        elif "coverletter" in evt:
            buckets["coverletters"][i] += 1
        elif "superhuman" in evt or "humanize" in evt:
            buckets["superhuman"][i] += 1
        elif "mastermind" in evt:
            buckets["mastermind"][i] += 1
        elif "talk" in evt:
            buckets["talk"][i] += 1

    return buckets


# ============================================================
# ðŸš€ Root: Combined payload (summary + trend + history)
# ============================================================
@router.get("")
@router.get("/")  # compatibility
async def dashboard_root(
    limit: int = Query(300, ge=1, le=2000),
    dedupe: bool = Query(True, description="Collapse multiple actions per (Company,Role) to the newest one"),
):
    events = _read_jsonl(LOG_PATH, limit)
    history = _read_jsonl(HISTORY_PATH, limit)
    records = history or events

    if dedupe:
        records = _dedupe_company_role(records)

    if not records:
        return {"summary": {}, "trend": {}, "history": [], "updated": _now_iso()}

    return {
        "summary": summarize_events(records),
        "trend": weekly_trend(records),
        "history": summarize_history(records)[:100],
        "updated": _now_iso(),
    }


# ============================================================
# ðŸš€ Endpoint: /summary
# ============================================================
@router.get("/summary")
async def get_summary(
    limit: int = Query(300, ge=1, le=2000),
    dedupe: bool = Query(True),
):
    """
    Aggregated dashboard summary used for top metrics and charts.
    Combines analytics from events.jsonl and history.jsonl.
    """
    events = _read_jsonl(LOG_PATH, limit)
    history = _read_jsonl(HISTORY_PATH, limit)

    if not events and not history:
        return JSONResponse({"message": "No analytics available.", "summary": {}, "recent": []})

    # Prefer history when present
    records = history or events
    if dedupe:
        records = _dedupe_company_role(records)

    summary = summarize_events(records)
    hist_data = summarize_history(records)

    return {"summary": summary, "recent": hist_data[:100], "updated": _now_iso()}


# ============================================================
# ðŸš€ Endpoint: /trend
# ============================================================
@router.get("/trend")
async def get_trend(
    limit: int = Query(300, ge=1, le=2000),
    dedupe: bool = Query(True),
):
    """Weekly Mon..Sun trend counts by category."""
    history = _read_jsonl(HISTORY_PATH, limit) or _read_jsonl(LOG_PATH, limit)
    if dedupe:
        history = _dedupe_company_role(history)
    return {"trend": weekly_trend(history), "updated": _now_iso()}


# ============================================================
# ðŸš€ Endpoint: /recent
# ============================================================
@router.get("/recent")
async def get_recent(
    limit: int = Query(100, ge=1, le=1000),
    dedupe: bool = Query(True),
):
    """Returns a chronological list of recent user-visible actions."""
    history = _read_jsonl(HISTORY_PATH, limit) or _read_jsonl(LOG_PATH, limit)
    if dedupe:
        history = _dedupe_company_role(history)
    return {"events": summarize_history(history)}


# ============================================================
# ðŸš€ Endpoint: /types
# ============================================================
@router.get("/types")
async def list_event_types():
    """Returns a deduplicated list of event types for frontend filters."""
    # Combine both sources for a more complete registry
    events = _read_jsonl(LOG_PATH, 1000) + _read_jsonl(HISTORY_PATH, 1000)
    types = sorted(
        {
            (e.get("event") or e.get("type") or "").lower()
            for e in events
            if (e.get("event") or e.get("type"))
        }
    )
    return {"types": types}


# ============================================================
# ðŸ§  Endpoint: /metrics
# ============================================================
@router.get("/metrics")
async def metrics_summary(
    limit: int = Query(500, ge=1, le=3000),
    dedupe: bool = Query(True),
):
    """Returns lightweight numeric insights (for quick dashboard cards)."""
    events = _read_jsonl(LOG_PATH, limit)
    if not events:
        return {
            "optimize": 0,
            "coverletters": 0,
            "superhuman": 0,
            "talk": 0,
            "mastermind": 0,
            "distinct_company_roles": 0,
            "updated": _now_iso(),
        }

    records = _dedupe_company_role(events) if dedupe else events
    summary = summarize_events(records)
    return {
        "optimize": summary["optimize_runs"],
        "coverletters": summary["coverletters"],
        "superhuman": summary["superhuman_calls"],
        "talk": summary["talk_queries"],
        "mastermind": summary["mastermind_chats"],
        "distinct_company_roles": summary["distinct_company_roles"],
        "updated": _now_iso(),
    }


# ============================================================
# ðŸ§¾ Endpoint: /raw
# ============================================================
@router.get("/raw")
async def raw_dump(limit: int = Query(100, ge=1, le=2000)):
    """
    Developer-only diagnostic endpoint: returns raw JSON lines.
    Use for backend debugging or analytics export.
    """
    events = _read_jsonl(LOG_PATH, limit)
    return {"count": len(events), "events": events}
-e 


-e # ===== FILE: ./api/debug.py =====

"""
============================================================
 HIREX v2.1.0 â€” api/debug.py
 ------------------------------------------------------------
 Lightweight diagnostic endpoint for frontend â†’ backend logs.

  â€¢ Accepts any POSTed JSON payload (dict or list) or raw text
  â€¢ Prints to console in readable, truncated format
  â€¢ Persists structured event via log_event()
  â€¢ Auto-tags origin, page, level, and timestamps
  â€¢ Never crashes on malformed or non-JSON payloads

 Author: Sri Akash Kadali
============================================================
"""

from __future__ import annotations

import json
from datetime import datetime
from typing import Any, Dict

from fastapi import APIRouter, Request
from fastapi.responses import JSONResponse

from backend.core.utils import log_event


# IMPORTANT:
#  - NO `/api` here
#  - `/api` is owned by main.py
router = APIRouter(prefix="/api/debug", tags=["debug"])


def _now_iso() -> str:
    return datetime.utcnow().isoformat()


def _truncate(obj: Any, limit: int = 800) -> str:
    """Truncate a JSON-serialized preview to avoid spammy console logs."""
    try:
        s = json.dumps(obj, ensure_ascii=False, indent=2)
    except Exception:
        s = str(obj)
    return (s[:limit] + "â€¦") if len(s) > limit else s


@router.get("/ping")
async def debug_ping():
    """Simple liveness check for the debug router."""
    return {"ok": True, "router": "debug", "time": _now_iso()}


# ============================================================
# ðŸ§  Frontend â†’ Backend Debug / Analytics Logger
# ============================================================
@router.post("/log")
async def debug_log(request: Request):
    """
    Receives arbitrary frontend debug or analytics payloads
    and logs them both to console and persistent JSONL.
    """

    # 1ï¸âƒ£ Parse body (JSON or raw text)
    payload: Dict[str, Any]
    try:
        body = await request.json()
        if isinstance(body, dict):
            payload = body
        else:
            payload = {"data": body, "non_dict_json": True}
    except Exception:
        raw = (await request.body()).decode("utf-8", "ignore")
        payload = {"raw": raw, "format_error": True}

    # 2ï¸âƒ£ Inject metadata
    headers = request.headers
    client_ip = request.client.host if request.client else "unknown"

    payload.setdefault("received_at", _now_iso())
    payload.setdefault("origin", client_ip)
    payload.setdefault("page", payload.get("page", "unknown"))
    payload.setdefault("level", payload.get("level", "debug"))
    payload.setdefault("user_agent", headers.get("user-agent", ""))
    payload.setdefault("referer", headers.get("referer", ""))
    payload.setdefault("timestamp", payload.get("received_at"))

    # 3ï¸âƒ£ Console output (safe + truncated)
    msg = payload.get("msg", "(no message)")
    page = payload.get("page", "?")
    print(f"[FE DEBUG] ({page}) {msg}")
    print("  â””â”€", _truncate({k: v for k, v in payload.items() if k != "raw"}))

    # 4ï¸âƒ£ Persist event
    try:
        log_event("frontend_debug", payload)
    except Exception as e:
        print(f"[WARN] Failed to persist debug event: {e}")

    # 5ï¸âƒ£ Response
    return JSONResponse(
        {
            "ok": True,
            "logged": True,
            "timestamp": payload["received_at"],
        }
    )-e 


-e # ===== FILE: ./api/talk.py =====

"""
============================================================
 HIREX v2.1.3 â€” talk.py
 ------------------------------------------------------------
 "Talk to HIREX" conversational endpoint.
 Answers job-application or interview questions using
 JD + resume (+ cover letter if available) from the stable
 (Company__Role) context files saved in /api/context.

 Author: Sri Akash Kadali
============================================================
"""

from __future__ import annotations

import json
import re
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional, Dict, Any, Tuple

import httpx
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

# OpenAI SDK (lazy/defensive import so the router still loads even if SDK is missing)
try:
    from openai import AsyncOpenAI  # type: ignore
except Exception:  # pragma: no cover
    AsyncOpenAI = None  # type: ignore

from backend.core import config
from backend.core.utils import log_event, safe_filename, ensure_dir
from backend.core.security import secure_tex_input

# Prefix keeps both /api/talk (default) and /api/talk/answer (alias) working
router = APIRouter(prefix="/api/talk", tags=["talk"])

# Initialize OpenAI client if SDK is available; defer credential checks to request-time
openai_client = AsyncOpenAI(api_key=getattr(config, "OPENAI_API_KEY", "")) if AsyncOpenAI else None

# Stable location where /api/context saves contexts
CONTEXT_DIR: Path = config.get_contexts_dir()
ensure_dir(CONTEXT_DIR)

# Cheap, reliable summarizer / answer model (override-able from env)
SUMMARIZER_MODEL = getattr(config, "TALK_SUMMARY_MODEL", "gpt-5-mini")
ANSWER_MODEL = getattr(config, "TALK_ANSWER_MODEL", getattr(config, "DEFAULT_MODEL", "gpt-5-mini"))

# Chat-safe default if a responses-only or image model is requested
CHAT_SAFE_DEFAULT = getattr(config, "DEFAULT_MODEL", "gpt-4o-mini")

# Regex hints for models that are not served by chat.completions
RESPONSES_ONLY_HINTS = (
    re.compile(r"^gpt-image", re.I),
    re.compile(r"^dall[- ]?e", re.I),
    re.compile(r"^whisper", re.I),
)


# ------------------------------------------------------------
# Small helper to be resilient to different secure_tex_input signatures
# ------------------------------------------------------------
def _tex_safe(s: str) -> str:
    """
    Ensure returned text is safe to embed into TeX (no macros injected).
    Works whether secure_tex_input accepts (text) or (filename, text).
    """
    try:
        return secure_tex_input(s)  # type: ignore[arg-type]
    except TypeError:
        return secure_tex_input("inline.txt", s)  # type: ignore[misc]


def _is_responses_only_model(name: str) -> bool:
    if not name:
        return False
    return any(rx.search(name) for rx in RESPONSES_ONLY_HINTS)


def _is_image_family(name: str) -> bool:
    if not name:
        return False
    return bool(re.match(r"^(gpt-image|dall[- ]?e)", name, flags=re.I))


# ============================================================
# ðŸ§  REQUEST MODEL (back-compat + new stable key)
# ============================================================
class TalkReq(BaseModel):
    # Primary inputs (optional if pulling from context)
    jd_text: str = ""
    question: str
    resume_tex: Optional[str] = None
    resume_plain: Optional[str] = None

    # Behavior
    tone: str = "balanced"
    humanize: bool = True
    model: str = ANSWER_MODEL

    # Context lookup (new: stable key) â€” fallbacks preserved
    context_key: Optional[str] = None         # NEW: "{company}__{role}"
    context_id: Optional[str] = None          # legacy "title"/"id" filename stem
    title: Optional[str] = None               # alias for legacy id
    use_latest: bool = True                   # fallback to newest if nothing provided


# ============================================================
# ðŸ§© CONTEXT HELPERS (stable-key aware)
# ============================================================
def _path_for_key(key: str) -> Path:
    return CONTEXT_DIR / f"{safe_filename(key)}.json"


def _latest_path() -> Optional[Path]:
    files = sorted(CONTEXT_DIR.glob("*.json"), key=lambda p: p.stat().st_mtime, reverse=True)
    return files[0] if files else None


def _read_json(path: Optional[Path]) -> Dict[str, Any]:
    if not path or not path.exists():
        return {}
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {}


def _coerce_key_from_ctx(ctx: Dict[str, Any], fallback_path: Optional[Path]) -> str:
    if ctx.get("key"):
        return str(ctx["key"]).strip()
    c, r = (ctx.get("company") or "").strip(), (ctx.get("role") or "").strip()
    if c and r:
        return f"{safe_filename(c)}__{safe_filename(r)}"
    return fallback_path.stem if fallback_path else ""


def _pick_resume_from_ctx(ctx: Dict[str, Any]) -> str:
    """
    Prefer modern nested structure, then legacy flat.
    """
    # modern nested
    h = (ctx.get("humanized") or {})
    o = (ctx.get("optimized") or {})
    for candidate in (h.get("tex"), o.get("tex"), ctx.get("humanized_tex"), ctx.get("resume_tex")):
        if isinstance(candidate, str) and candidate.strip():
            return candidate
    return ""


def _pick_coverletter_from_ctx(ctx: Dict[str, Any]) -> str:
    cl = (ctx.get("cover_letter") or {})
    v = cl.get("tex")
    return v.strip() if isinstance(v, str) else ""


def _load_context(req: TalkReq) -> Tuple[Dict[str, Any], Optional[Path]]:
    """
    Load context in priority order:
      1) context_key (stable)
      2) legacy id/title
      3) latest
    """
    path: Optional[Path] = None
    if (req.context_key or "").strip():
        path = _path_for_key(req.context_key.strip())
    elif (req.context_id or req.title or "").strip():
        # legacy files can still be addressed directly by their filename stem
        stem = safe_filename((req.context_id or req.title or "").strip())
        path = CONTEXT_DIR / f"{stem}.json"
    elif req.use_latest:
        path = _latest_path()

    ctx = _read_json(path)
    if ctx:
        meta = {
            "key": _coerce_key_from_ctx(ctx, path),
            "company": ctx.get("company"),
            "role": ctx.get("role"),
            "updated_at": ctx.get("updated_at") or ctx.get("saved_at"),
            "title_for_memory": ctx.get("title_for_memory") or ctx.get("title"),
        }
        log_event("talk_context_used", meta)
    return ctx, path


# ============================================================
# ðŸ§© OPENAI HELPERS â€” smart routing (Responses vs Chat)
# ============================================================
async def _gen_text_smart(system: str, user: str, model: str) -> str:
    """
    Generate text using the appropriate OpenAI endpoint.
    - If an image/responses-only model is selected, route safely:
        â€¢ Image-family models â†’ map to CHAT_SAFE_DEFAULT (text-only task)
        â€¢ Other responses-only models â†’ call Responses API
    - Otherwise, use Chat Completions.
    Also retries via Responses API if we detect a "responses-only" server error.
    """
    if not openai_client:
        raise HTTPException(status_code=500, detail="OpenAI SDK not installed.")
    if not (getattr(config, "OPENAI_API_KEY", "") or "").strip():
        raise HTTPException(status_code=400, detail="OPENAI_API_KEY missing in environment.")

    requested_model = (model or "").strip() or CHAT_SAFE_DEFAULT

    # If clearly an image model, map to chat-safe text model (this is a text Q&A endpoint)
    if _is_image_family(requested_model):
        mapped = CHAT_SAFE_DEFAULT
        log_event("talk_model_mapped", {"from": requested_model, "to": mapped, "reason": "image_model_not_text"})
        requested_model = mapped

    # If it's responses-only (but not an image model), prefer Responses API
    if _is_responses_only_model(requested_model):
        try:
            resp = await openai_client.responses.create(
                model=requested_model,
                input=[{"role": "system", "content": system}, {"role": "user", "content": user}],
            )
            txt = getattr(resp, "output_text", "") or ""
            if not txt:
                # Best-effort extraction if SDK shape changes
                try:
                    out = []
                    for blk in getattr(resp, "output", []) or []:
                        if getattr(blk, "type", "") == "message":
                            for c in getattr(blk, "content", []) or []:
                                if getattr(c, "type", "") == "text":
                                    out.append(getattr(c, "text", "") or "")
                    txt = "\n".join(out).strip()
                except Exception:
                    txt = ""
            return (txt or "").strip()
        except Exception as e:
            # If Responses fails, last-resort: fall back to chat-safe default on chat API
            fallback = CHAT_SAFE_DEFAULT
            log_event("talk_responses_fail_fallback_chat", {"model": requested_model, "error": str(e), "fallback": fallback})
            r = await openai_client.chat.completions.create(
                model=fallback,
                messages=[{"role": "system", "content": system}, {"role": "user", "content": user}],
            )
            return (r.choices[0].message.content or "").strip()

    # Normal path: Chat Completions
    try:
        r = await openai_client.chat.completions.create(
            model=requested_model,
            messages=[{"role": "system", "content": system}, {"role": "user", "content": user}],
        )
        return (r.choices[0].message.content or "").strip()
    except Exception as e:
        # If server tells us to use Responses, retry there once
        msg = str(getattr(e, "message", None) or e)
        if "only supported in v1/responses" in msg.lower():
            log_event("talk_retry_responses_api", {"model": requested_model})
            try:
                resp = await openai_client.responses.create(
                    model=requested_model,
                    input=[{"role": "system", "content": system}, {"role": "user", "content": user}],
                )
                txt = getattr(resp, "output_text", "") or ""
                if not txt:
                    try:
                        out = []
                        for blk in getattr(resp, "output", []) or []:
                            if getattr(blk, "type", "") == "message":
                                for c in getattr(blk, "content", []) or []:
                                    if getattr(c, "type", "") == "text":
                                        out.append(getattr(c, "text", "") or "")
                        txt = "\n".join(out).strip()
                    except Exception:
                        txt = ""
                return (txt or "").strip()
            except Exception as e2:
                # Final fallback to chat-safe default
                fallback = CHAT_SAFE_DEFAULT
                log_event("talk_responses_retry_fail_fallback_chat", {"model": requested_model, "error": str(e2), "fallback": fallback})
                r2 = await openai_client.chat.completions.create(
                    model=fallback,
                    messages=[{"role": "system", "content": system}, {"role": "user", "content": user}],
                )
                return (r2.choices[0].message.content or "").strip()

        # Unknown error â€” surface
        raise


# ============================================================
# ðŸ”Ž RESUME SUMMARY
# ============================================================
async def extract_resume_summary(resume_tex: Optional[str], resume_plain: Optional[str]) -> str:
    """
    Compress resume content into factual bullet points.
    Strips formatting, avoids hallucination.
    """
    if not (resume_tex or resume_plain):
        return "No resume text provided."

    text_input = (resume_plain or resume_tex or "").strip()[:3500]
    sys_prompt = (
        "Summarize this resume into 6â€“10 concise factual bullet points "
        "about key skills, technologies, and experiences. "
        "Do NOT fabricate or guess. Output plain-text bullets."
    )

    try:
        summary_text = await _gen_text_smart(sys_prompt, text_input, model=SUMMARIZER_MODEL)
        return _tex_safe(summary_text)  # keep LaTeX-safe if UI renders into TeX
    except Exception as e:
        log_event("talk_resume_summary_fail", {"error": str(e)})
        # Fallback: provide trimmed raw text (still TeX-safe)
        return _tex_safe(text_input[:1200])


# ============================================================
# ðŸ’¬ ANSWER GENERATION â€” exactly two short, customized paragraphs
# ============================================================
async def generate_answer(
    jd_text: str,
    resume_summary: str,
    question: str,
    model: str,
    cover_letter: str = "",
) -> str:
    """
    Produce a two-paragraph, first-person answer (short but rich).
    Para 1 = proof from resume; Para 2 = how that maps to THIS JD/company.
    Ground ONLY in JD, resume summary, and (if present) cover letter.
    No headings, no bullets, no fluff, no claims beyond the sources.
    """
    sys_prompt = (
        "You are HIREX Assistant, an AI recruiter co-pilot. "
        "Write like a candidate who genuinely did the work and was born to do this job. "
        "STRICT FORMAT: Output EXACTLY TWO PARAGRAPHS, no headings or bullets. "
        'STYLE: First-person, confident, specific, natural; never say "as an AI"; avoid buzzwords and clichÃ©s. '
        "GROUNDING: Use ONLY facts present in the Job Description (JD), Resume Summary, and (if provided) Cover Letter. "
        "TRUTH: If a detail isnâ€™t in those sources, donâ€™t invent it; prefer a brief gap-statement instead. "
        "CUSTOMIZATION: Weave 3â€“6 concrete JD keywords naturally (stack, methods, domains). "
        "LENGTH: Keep it tightâ€”roughly 90â€“150 words total across both paragraphs."
    )

    cl_block = f"\n\n[Cover Letter]\n{cover_letter[:3000]}" if cover_letter else ""
    user_prompt = (
        "You will answer an application or interview question using the provided sources.\n\n"
        "[Job Description]\n"
        f"{jd_text[:6000]}\n\n"
        "[Resume Summary]\n"
        f"{resume_summary[:3000]}{cl_block}\n\n"
        "[Question]\n"
        f"{question.strip()}\n\n"
        "[Role Context]\n"
        "Respond as the applicant described above, aligning tone and evidence to that company and position.\n\n"
        "CONTENT PLAN:\n"
        "Paragraph 1 â€” Proof of past impact: choose 2â€“3 resume-backed accomplishments most relevant to the JD; "
        "quantify outcomes (%/#/$/time), name concrete artifacts (models, datasets, systems), and mention tools/frameworks exactly as written.\n"
        "Paragraph 2 â€” Forward alignment: map those wins to the companyâ€™s JD and mission; mirror 3â€“4 explicit JD needs "
        "(model types, infrastructure, metrics, or domain), outline your contribution plan, and tie to user or business impact.\n\n"
        "HARD RULES:\n"
        "- Exactly two paragraphs (no more, no less).\n"
        "- 90â€“150 words total.\n"
        "- No bullet points, lists, or headings.\n"
        "- No invented facts; only use data from the sources.\n"
        "- Include 3â€“5 exact JD nouns naturally.\n"
        "- Avoid copying JD sentences verbatim.\n"
        "- Use confident, natural tone â€” no buzzwords or fluff.\n"
        "- Past tense for proof; present/future for mapping."
    )


    start = time.time()
    answer = await _gen_text_smart(sys_prompt, user_prompt, model=model)
    latency = round(time.time() - start, 2)
    tokens = len(answer.split())
    log_event("talk_answer_raw", {"latency": latency, "tokens": tokens, "model": model})

    return _tex_safe(answer)


# ============================================================
# âœ¨ HUMANIZE (SuperHuman)
# ============================================================
async def humanize_text(answer_text: str, tone: str) -> Tuple[str, bool]:
    """
    Refine the tone and flow via SuperHuman rewrite API.
    Falls back gracefully if unavailable.

    Returns:
        (final_text, was_humanized)
    """
    api_base = (getattr(config, "API_BASE_URL", "") or "").rstrip("/") or "http://127.0.0.1:8000"
    url = f"{api_base}/api/superhuman/rewrite"
    payload = {
        "text": "Rewrite while preserving EXACTLY TWO PARAGRAPHS and ~90â€“150 words total.\n\n" + answer_text,
        "mode": "paragraph",
        "tone": tone,
        "latex_safe": True,
    }

    try:
        async with httpx.AsyncClient(timeout=45.0) as client:
            r = await client.post(url, json=payload)
        r.raise_for_status()
        data = r.json()
        rewritten = data.get("rewritten") or answer_text
        was_humanized = isinstance(rewritten, str) and rewritten.strip() and (rewritten.strip() != answer_text.strip())
        return _tex_safe(rewritten), was_humanized
    except Exception as e:
        log_event("talk_superhuman_fail", {"error": str(e)})
        return answer_text, False


# ============================================================
# ðŸŸ¢ HEALTH CHECK (compact; returns both epoch and ISO)
# ============================================================
@router.get("/ping")
async def ping():
    now = datetime.now(tz=timezone.utc)
    return {"ok": True, "service": "talk", "epoch": time.time(), "iso": now.isoformat()}


# ============================================================
# ðŸš€ MAIN ENDPOINT
# ============================================================
@router.post("/answer")
@router.post("")  # compatibility for POST /api/talk
async def talk_to_hirex(req: TalkReq):
    """
    Generate a contextual, factual, optionally humanized answer for
    job-application or interview questions.

    Behavior:
      â€¢ If jd_text / resume not provided, pulls from the latest (or specified)
        saved context created by /api/context/save (stable key: {company}__{role}).
      â€¢ Prefer humanizedâ†’optimizedâ†’legacy resume text in that order.
      â€¢ If a cover letter exists in context, it is included for added grounding.
      â€¢ Returns both 'answer' (final) and 'draft_answer' (pre-humanize).
    """
    # Pull context if needed
    jd_text = (req.jd_text or "").strip()
    resume_tex = (req.resume_tex or "").strip()
    cover_letter_tex = ""
    used_key = ""
    used_company = ""
    used_role = ""
    used_title_for_memory = ""
    used_updated_at = ""

    # Load context if any required piece is missing
    if (not jd_text) or (not resume_tex and not (req.resume_plain or "").strip()):
        ctx, ctx_path = _load_context(req)
        if ctx:
            jd_text = jd_text or (ctx.get("jd_text") or "")
            resume_tex = resume_tex or _pick_resume_from_ctx(ctx)
            cover_letter_tex = _pick_coverletter_from_ctx(ctx)
            used_key = _coerce_key_from_ctx(ctx, ctx_path)
            used_company = (ctx.get("company") or "").strip()
            used_role = (ctx.get("role") or "").strip()
            used_title_for_memory = (ctx.get("title_for_memory") or ctx.get("title") or "").strip()
            used_updated_at = (ctx.get("updated_at") or ctx.get("saved_at") or "").strip()

    if not jd_text.strip():
        raise HTTPException(status_code=400, detail="Job Description missing. Provide jd_text or save a context first.")
    if not (resume_tex or (req.resume_plain or "").strip()):
        raise HTTPException(status_code=400, detail="Resume text missing. Provide resume_tex/plain or save a context first.")

    # 1) Resume summary
    resume_summary = await extract_resume_summary(resume_tex, req.resume_plain)

    # 2) Raw answer generation
    model = (req.model or ANSWER_MODEL).strip() or ANSWER_MODEL
    draft_answer = await generate_answer(jd_text, resume_summary, req.question, model=model, cover_letter=cover_letter_tex)

    # 3) Optional humanization (and truthy flag only if it actually changed)
    if req.humanize:
        final_answer, was_humanized = await humanize_text(draft_answer, req.tone)
    else:
        final_answer, was_humanized = draft_answer, False

    # 4) Log metadata
    log_event(
        "talk_to_hirex",
        {
            "question": req.question,
            "tone": req.tone,
            "humanize_requested": req.humanize,
            "humanize_applied": was_humanized,
            "jd_len": len(jd_text),
            "resume_len": len(resume_tex or req.resume_plain or ""),
            "model": model,
            "context_used": bool(used_key),
            "context_key": used_key,
            "company": used_company,
            "role": used_role,
        },
    )

    # 5) Structured return (include aliases for frontend)
    return {
        "question": req.question.strip(),
        "resume_summary": resume_summary,
        "draft_answer": draft_answer,
        "final_text": final_answer,
        "answer": final_answer,                 # alias for UI compatibility
        "tone": req.tone,
        "humanized": was_humanized,             # true only if rewrite changed output
        "model": model,
        # context meta for UI panes / breadcrumbs
        "context": {
            "key": used_key,
            "company": used_company,
            "role": used_role,
            "title_for_memory": used_title_for_memory,
            "updated_at": used_updated_at,
            "has_cover_letter": bool(cover_letter_tex),
        },
    }
-e 


-e # ===== FILE: ./api/context_store.py =====

# ============================================================
#  HIREX v2.1.0 â€” context_store.py (FINAL, stable-key + dedupe)
#  Store & retrieve JD + (optimized/humanized) resume + cover-letter.
#  File-per-(Company,Role) using key:  {safe_company}__{safe_role}.json
#  "title_for_memory" keeps a timestamped title for UI memory only.
#  Back-compat:
#   â€¢ /api/context/list returns items with both key and id (id===key)
#   â€¢ /api/context/get accepts key=..., id_or_title=..., latest=true
#   â€¢ /api/context/get flattens nested fields for older UIs
# ============================================================

from __future__ import annotations

import json
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple

from fastapi import APIRouter, HTTPException, Form, Query

from backend.core import config
from backend.core.utils import safe_filename, log_event, ensure_dir

router = APIRouter(prefix="/api/context", tags=["context"])

# ---- Resolve contexts directory with robust fallback ----
def _default_contexts_dir() -> Path:
    # Conservative default inside backend data; ensures writeable path exists
    here = Path(__file__).resolve().parent
    root = here.parents[2] if len(here.parents) >= 3 else here
    return (root / "backend" / "data" / "contexts")

_get_dir = getattr(config, "get_contexts_dir", None)
try:
    CONTEXT_DIR: Path = Path(_get_dir()) if callable(_get_dir) else _default_contexts_dir()
except Exception:
    CONTEXT_DIR: Path = _default_contexts_dir()

ensure_dir(CONTEXT_DIR)


# ---------------------- internal helpers ----------------------

def _nowstamp() -> str:
    return datetime.utcnow().strftime("%Y%m%d-%H%M%S")


def _iso_utc() -> str:
    # ISO 8601 with 'Z' suffix for UTC
    return datetime.utcnow().isoformat(timespec="seconds") + "Z"


def _make_key(company: str, role: str) -> str:
    return f"{safe_filename(company)}__{safe_filename(role)}"


def _path_for_key(key: str) -> Path:
    return CONTEXT_DIR / f"{safe_filename(key)}.json"


def _read(path: Path) -> Dict[str, Any]:
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {}


def _write(path: Path, payload: Dict[str, Any]) -> None:
    ensure_dir(path.parent)
    path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")


def _coerce_key(d: Dict[str, Any], path: Optional[Path] = None) -> str:
    # Prefer explicit key in file; else derive from company/role; else from filename (legacy)
    if "key" in d and str(d["key"]).strip():
        return str(d["key"]).strip()
    company = str(d.get("company") or "").strip()
    role = str(d.get("role") or "").strip()
    if company and role:
        return _make_key(company, role)
    if path is not None:
        return path.stem
    return ""


def _updated_at(d: Dict[str, Any], default_ts: float = 0.0) -> float:
    try:
        ts = d.get("updated_at") or d.get("saved_at") or ""
        if isinstance(ts, str) and ts:
            # accept '...Z' or bare ISO
            iso = ts.rstrip("Z")
            return datetime.fromisoformat(iso).timestamp()
    except Exception:
        pass
    return default_ts


def _compact_meta(d: Dict[str, Any], key: str) -> Dict[str, Any]:
    """Return a compact row for list(), compatible with old UIs."""
    title = d.get("title_for_memory") or d.get("title") or f"{d.get('company','')} â€” {d.get('role','')}"
    has_opt = bool(((d.get("optimized") or {}).get("tex")) or d.get("resume_tex"))
    has_hum = bool(((d.get("humanized") or {}).get("tex")) or d.get("humanized_tex"))
    cl = d.get("cover_letter") or {}
    has_cl = bool(cl.get("tex") or cl.get("pdf_b64") or d.get("cover_letter_tex") or d.get("cover_letter_pdf_b64"))
    return {
        "key": key,
        "id": key,                     # back-compat for frontends expecting .id
        "title": title,
        "company": d.get("company"),
        "role": d.get("role"),
        "updated_at": d.get("updated_at") or d.get("saved_at"),
        "has_optimized": has_opt,
        "has_humanized": has_hum,
        "has_cover_letter": has_cl,
        "model": d.get("model"),
        "fit_score": d.get("fit_score"),
    }


def _load_all_contexts() -> List[Tuple[str, Path, Dict[str, Any]]]:
    """Return list of (key, path, data)."""
    items: List[Tuple[str, Path, Dict[str, Any]]] = []
    for p in CONTEXT_DIR.glob("*.json"):
        data = _read(p)
        key = _coerce_key(data, p) or p.stem
        items.append((key, p, data))
    return items


def _merge_update(existing: Dict[str, Any], patch: Dict[str, Any]) -> Dict[str, Any]:
    """
    Merge PATCH into existing:
      - Only overwrite nested blobs if new fields are non-empty.
      - Maintain 'optimized', 'humanized', 'cover_letter' subobjects.
    """
    out = dict(existing or {})
    # shallow updates
    for k in ["key", "company", "role", "jd_text", "model", "fit_score"]:
        v = patch.get(k, None)
        if v is not None and (not isinstance(v, str) or v.strip() != ""):
            out[k] = v

    # maintain timestamped memory title
    if "title_for_memory" in patch and patch["title_for_memory"]:
        out["title_for_memory"] = patch["title_for_memory"]

    # legacy flat fields -> normalize into subobjects
    if "resume_tex" in patch or "pdf_base64" in patch:
        opt = dict(out.get("optimized") or {})
        if str(patch.get("resume_tex", "")).strip():
            opt["tex"] = patch["resume_tex"]
        if str(patch.get("pdf_base64", "")).strip():
            opt["pdf_b64"] = patch["pdf_base64"]
        out["optimized"] = opt

    if "humanized_tex" in patch or "pdf_base64_humanized" in patch:
        hum = dict(out.get("humanized") or {})
        if str(patch.get("humanized_tex", "")).strip():
            hum["tex"] = patch["humanized_tex"]
        if str(patch.get("pdf_base64_humanized", "")).strip():
            hum["pdf_b64"] = patch["pdf_base64_humanized"]
        out["humanized"] = hum

    # modern subobjects (prefer these if provided)
    if "optimized" in patch and isinstance(patch["optimized"], dict):
        base = dict(out.get("optimized") or {})
        for k, v in patch["optimized"].items():
            if v is not None and (not isinstance(v, str) or v.strip() != ""):
                base[k] = v
        out["optimized"] = base

    if "humanized" in patch and isinstance(patch["humanized"], dict):
        base = dict(out.get("humanized") or {})
        for k, v in patch["humanized"].items():
            if v is not None and (not isinstance(v, str) or v.strip() != ""):
                base[k] = v
        out["humanized"] = base

    if "cover_letter" in patch and isinstance(patch["cover_letter"], dict):
        base = dict(out.get("cover_letter") or {})
        for k, v in patch["cover_letter"].items():
            if v is not None and (not isinstance(v, str) or v.strip() != ""):
                base[k] = v
        out["cover_letter"] = base

    out["updated_at"] = _iso_utc()
    return out


def _flatten_for_frontend(d: Dict[str, Any]) -> Dict[str, Any]:
    """
    Provide flat aliases expected by older front-ends:
      resume_tex, pdf_base64, humanized_tex, pdf_base64_humanized,
      cover_letter_tex, cover_letter_pdf_b64
    """
    out = dict(d)
    opt = d.get("optimized") or {}
    hum = d.get("humanized") or {}
    cl  = d.get("cover_letter") or {}

    # Resume (optimized)
    out["resume_tex"] = out.get("resume_tex") or opt.get("tex") or ""
    out["pdf_base64"] = out.get("pdf_base64") or opt.get("pdf_b64") or ""

    # Humanized
    out["humanized_tex"] = out.get("humanized_tex") or hum.get("tex") or ""
    out["pdf_base64_humanized"] = out.get("pdf_base64_humanized") or hum.get("pdf_b64") or ""

    # Cover letter
    out["cover_letter_tex"] = out.get("cover_letter_tex") or cl.get("tex") or ""
    out["cover_letter_pdf_b64"] = out.get("cover_letter_pdf_b64") or cl.get("pdf_b64") or ""

    # Convenience â€” mirror some names used by UIs
    out["company_name"] = out.get("company")
    out["role_name"] = out.get("role")
    out["title"] = out.get("title_for_memory") or out.get("title") or f"{out.get('company','')} â€” {out.get('role','')}"

    return out


# -------------------------- routes ----------------------------

@router.post("/save")
async def save_context(
    company: str = Form(...),
    role: str = Form(...),
    jd_text: str = Form(""),

    # Legacy flat payloads from older UI (will be normalized)
    resume_tex: str = Form(""),
    humanized_tex: str = Form(""),
    pdf_base64: str = Form(""),
    pdf_base64_humanized: str = Form(""),

    # Optional modern nested payloads (optimized/humanized/cover_letter)
    optimized: str = Form("", description="JSON string of optimized block (tex, pdf_b64, pdf_path)"),
    humanized: str = Form("", description="JSON string of humanized block (tex, pdf_b64, pdf_path, enabled)"),
    cover_letter: str = Form("", description="JSON string of cover_letter block (tex, pdf_b64, pdf_path, tone, length)"),

    model: str = Form(""),
    fit_score: str = Form(""),
):
    """
    Persist context under a STABLE key per (Company, Role).
    Overwrites previous file with the same key (dedup by design).
    Keeps a 'title_for_memory' that includes a timestamp for UI history labels.
    """
    key = _make_key(company, role)
    path = _path_for_key(key)

    # Parse optional JSON blocks if provided
    def _parse_json_str(s: str) -> Dict[str, Any]:
        try:
            return json.loads(s) if s and s.strip().startswith("{") else {}
        except Exception:
            return {}

    patch: Dict[str, Any] = {
        "key": key,
        "company": company,
        "role": role,
        "jd_text": jd_text,
        "model": model or getattr(config, "DEFAULT_MODEL", "gpt-4o-mini"),
        "fit_score": fit_score,
        "title_for_memory": f"{safe_filename(company)}_{safe_filename(role)}_{_nowstamp()}",
    }

    # legacy flat fields
    if resume_tex or pdf_base64:
        patch.update({"resume_tex": resume_tex, "pdf_base64": pdf_base64})
    if humanized_tex or pdf_base64_humanized:
        patch.update({"humanized_tex": humanized_tex or resume_tex, "pdf_base64_humanized": pdf_base64_humanized})

    # modern nested overrides (preferred)
    opt_obj = _parse_json_str(optimized)
    hum_obj = _parse_json_str(humanized)
    cl_obj = _parse_json_str(cover_letter)
    if opt_obj: patch["optimized"] = opt_obj
    if hum_obj: patch["humanized"] = hum_obj
    if cl_obj: patch["cover_letter"] = cl_obj

    existing = _read(path) if path.exists() else {}
    merged = _merge_update(existing, patch)
    _write(path, merged)

    log_event("context_saved", {"key": key, "company": company, "role": role, "path": str(path)})
    return {"ok": True, "key": key, "path": str(path), "updated_at": merged["updated_at"]}


@router.get("/list")
async def list_contexts(
    limit: int = Query(50, ge=1, le=500),
    dedupe: bool = Query(True),
):
    """
    List recent contexts (newest first). If legacy timestamped files exist,
    `dedupe=True` collapses multiple files to the single latest per key.
    """
    entries = _load_all_contexts()

    # Build (key -> newest (path,data))
    # If dedupe=False we pseudo-namespace keys to keep all files distinct.
    by_key: Dict[str, Tuple[Path, Dict[str, Any], float]] = {}
    for key, p, d in entries:
        ts = _updated_at(d, p.stat().st_mtime)
        idx_key = key if dedupe else f"{key}::{p.name}"
        cur = by_key.get(idx_key)
        if (cur is None) or (ts > cur[2]):
            by_key[idx_key] = (p, d, ts)

    # Sort newest first and compact
    rows = sorted(by_key.items(), key=lambda kv: kv[1][2], reverse=True)[:limit]
    items = [_compact_meta(d, _coerce_key(d, p)) for (_k, (p, d, _)) in rows]
    return {"items": items}


@router.get("/get")
async def get_context(
    key: str = Query("", description="Stable key: {company}__{role}"),
    id_or_title: str = Query("", description="(Back-compat) Either a stable key or a title_for_memory string."),
    latest: bool = Query(False),
):
    """
    Fetch full saved context by:
      â€¢ latest=true (newest item overall), OR
      â€¢ key=stable_key, OR
      â€¢ id_or_title=(stable_key or title_for_memory) â€” back-compat

    Response includes flattened fields for older UIs:
      resume_tex, pdf_base64, humanized_tex, pdf_base64_humanized,
      cover_letter_tex, cover_letter_pdf_b64
    """
    # Case 1: newest overall
    # Case 1: newest overall
    if latest or (not key.strip() and not id_or_title.strip()):
        newest_path: Optional[Path] = None
        newest_ts = -1.0
        for k, p, d in _load_all_contexts():
            ts = _updated_at(d, p.stat().st_mtime)
            if ts > newest_ts:
                newest_ts, newest_path = ts, p
        if not newest_path:
            raise HTTPException(status_code=404, detail="Context not found")
        return _flatten_for_frontend(_read(newest_path))

    # Case 2: explicit stable key
    if key.strip():
        path = _path_for_key(key)
        if not path.exists():
            raise HTTPException(status_code=404, detail="Context not found")
        return _flatten_for_frontend(_read(path))

    # Case 3: back-compat lookup via id_or_title (try key match, then title_for_memory)
    wanted = id_or_title.strip()
    # Try as key first
    candidate = _path_for_key(wanted)
    if candidate.exists():
        return _flatten_for_frontend(_read(candidate))

    # Fallback: scan for title_for_memory match (exact, then loose)
    best: Optional[Dict[str, Any]] = None
    best_ts = -1.0
    for k, p, d in _load_all_contexts():
        title = (d.get("title_for_memory") or d.get("title") or "").strip()
        if title == wanted or safe_filename(title) == safe_filename(wanted):
            ts = _updated_at(d, p.stat().st_mtime)
            if ts > best_ts:
                best, best_ts = d, ts
    if best is not None:
        return _flatten_for_frontend(best)

    raise HTTPException(status_code=404, detail="Context not found")


@router.delete("/delete")
async def delete_context(
    key: str = Query(..., description="Stable key: {company}__{role}")
):
    """
    Delete the single context file for the given (Company, Role) key.
    """
    path = _path_for_key(key)
    if not path.exists():
        raise HTTPException(status_code=404, detail="Context not found")
    path.unlink()
    log_event("context_deleted", {"key": key})
    return {"deleted": True, "key": key}
-e 


-e # ===== FILE: ./api/superhuman.py =====

"""
============================================================
 HIREX v2.1.2 â€” superhuman.py
 ------------------------------------------------------------
 SuperHuman Humanizer API
  â€¢ Rewrites text for clarity, flow, and tone
  â€¢ Preserves factual integrity and metrics
  â€¢ Supports resume, coverletter, paragraph, sentence modes
  â€¢ LaTeX-safe output for integration with HIREX optimizer
  â€¢ Powered by AI Humanize API (https://aihumanize.io)
  â€¢ Local fallback available (tagless) if explicitly enabled

 Author: Sri Akash Kadali
============================================================
"""

from __future__ import annotations

import asyncio
import json
import re
import time
from typing import List, Union, Dict, Any, Optional

import httpx
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field

from backend.core import config
from backend.core.utils import log_event
from backend.core.security import secure_tex_input


# ============================================================
# ðŸ”§ Setup
# ============================================================

router = APIRouter(prefix="/api/superhuman", tags=["superhuman"])

# Vendor endpoints used by HIREX.
HUMANIZE_REWRITE_URL = "https://aihumanize.io/api/v1/rewrite"
HUMANIZE_BALANCE_URL = "https://aihumanize.io/api/v1/surplus"

# Credentials (see core/config.py)
HUMANIZE_API_KEY: str = getattr(config, "HUMANIZE_API_KEY", "") or ""
HUMANIZE_MAIL: str = getattr(config, "HUMANIZE_MAIL", "") or ""

# Behavior toggles and limits
_MAX_ITEMS = 25
_TIMEOUT_S = 120
_LOCAL_ENABLED = bool(getattr(config, "SUPERHUMAN_LOCAL_ENABLED", False))  # default off per new requirements
_CONCURRENCY = 6  # limit concurrent outbound calls

# Humanize always-on defaults (from config; can be overridden via .env)
_HUMANIZE_DEFAULT_ON = getattr(config, "HUMANIZE_DEFAULT_ON", True)
_HUMANIZE_MODE_DEFAULT = str(getattr(config, "HUMANIZE_MODE_DEFAULT", "balance")).lower()
_AIH_MODE_ID = getattr(config, "AIHUMANIZE_MODE_ID", {"quality": "0", "balance": "1", "enhanced": "2"})


# ============================================================
# ðŸ§  Request Model
# ============================================================

from pydantic import BaseModel, Field
from typing import List, Union, Optional

class RewriteRequest(BaseModel):
    text: Union[str, List[str]] = Field(..., description="Text or list of texts to rewrite.")
    mode: str = Field(default="resume")
    tone: str = Field(default="balanced")
    latex_safe: bool = Field(default=True)
    constraints: Dict[str, Any] = Field(
        default_factory=lambda: {"no_fabrication": True, "keep_metrics": True},
        description="Behavior guards such as no_fabrication, keep_metrics",
    )
    max_len: int = Field(1600, description="Max input chars per item (truncate beyond).")


# ============================================================
# ðŸ§© Tone/Mode â†’ AIHumanize model code
#   0: quality   1: balance   2: enhanced
# ============================================================

def _resolve_model_code(tone: str) -> int:
    t = (tone or "").lower().strip()
    if t in {"default", ""}:
        t = _HUMANIZE_MODE_DEFAULT
    if t in {"formal", "academic", "quality"}:
        return 0
    if t in {"balanced", "confident", "balance"}:
        return 1
    if t in {"conversational", "enhanced"}:
        return 2
    # Fallback to configured default mapping
    try:
        return int(_AIH_MODE_ID.get(_HUMANIZE_MODE_DEFAULT, "1"))
    except Exception:
        return 1


# ============================================================
# ðŸ§¼ Post-processing
# ============================================================

_FALLBACK_TAG_RE = re.compile(r"^\[LOCAL-FALLBACK:[^\]]+\]\s*", re.IGNORECASE)

def _clean_text(text: str, latex_safe: bool, mode: str) -> str:
    """Normalize whitespace, strip any fallback labels, and optionally LaTeX-sanitize."""
    t = (text or "").replace("\r", "")
    t = _FALLBACK_TAG_RE.sub("", t)  # never leak [LOCAL-FALLBACK:*]
    t = re.sub(r"[ \t\f\v]+", " ", t).strip()

    if mode == "resume":
        # Resume bullets should be single-line and punchy; trim trailing period
        t = t.replace("\n", " ").rstrip(" .")

    if latex_safe:
        t = secure_tex_input(t)

    return t


# ============================================================
# ðŸ§© Local fallback (offline mode) â€” TAGLESS by design
# ============================================================

def _local_rewrite_stub(text: str, tone: str, mode: str) -> str:
    """
    Offline fallback when no API connectivity or creds are invalid.
    Keep light-touch edits and NEVER prefix any label (tagless).
    """
    t = (text or "")
    # basic normalization + a couple of clarity nips
    t = re.sub(r"\bu\b", "you", t, flags=re.IGNORECASE)
    t = re.sub(r"\br\b", "are", t, flags=re.IGNORECASE)
    t = re.sub(r"\bim\b", "I am", t, flags=re.IGNORECASE)
    if (tone or "").lower().strip() in {"formal", "academic"}:
        t = (t.replace("I'm", "I am")
               .replace("don't", "do not")
               .replace("can't", "cannot"))
    return t.strip()


# ============================================================
# ðŸ” Creds / Headers helpers
# ============================================================

def _require_api_creds() -> None:
    """
    Ensure Humanize is enabled by default and credentials are present.
    If fallback is enabled explicitly, we don't raise here (but still prefer API).
    """
    if not _HUMANIZE_DEFAULT_ON:
        raise HTTPException(status_code=503, detail="Humanize disabled by configuration.")
    if not HUMANIZE_API_KEY or not HUMANIZE_MAIL:
        if _LOCAL_ENABLED:
            log_event("âš ï¸ superhuman_local_fallback", {"reason": "missing_credentials"})
            return
        raise HTTPException(
            status_code=503,
            detail="Humanize API credentials missing. "
                   "Set HUMANIZE_API_KEY and HUMANIZE_MAIL in your .env.",
        )


def _header_variants(key: str) -> List[Dict[str, str]]:
    """
    Build header variants to maximize compatibility and reduce CF challenges.
    """
    key = (key or "").strip()
    base = {
        "Content-Type": "application/json",
        "Accept": "application/json",
        "User-Agent": "Mozilla/5.0",
    }
    return [
        dict(base, **{"Authorization": key}),
        dict(base, **{"Authorization": f"Bearer {key}"}),
        dict(base, **{"X-API-KEY": key}),
    ]


def _payload_with_aliases(model_code: int, mail: str, text: str) -> Dict[str, Any]:
    """
    Include common field aliases used by different Humanize integrations.
    Canonical fields:
      - model: 0|1|2
      - mail: registered email
      - data: text to rewrite
    Aliases:
      - email: same as mail
      - text: same as data
      - token: some gateways expect token in body (rare)
    """
    return {
        "model": model_code,
        "mail": mail,
        "email": mail,
        "data": text,
        "text": text,
        "token": HUMANIZE_API_KEY,  # harmless if ignored
    }


# ============================================================
# âš™ï¸ Core Rewrite
# ============================================================

async def _call_humanize(text: str, tone: str, mode: str) -> str:
    """
    Low-level caller that tries multiple header styles automatically.
    Raises HTTPException on hard failures.
    """
    model_code = _resolve_model_code(tone)
    payload = _payload_with_aliases(model_code, HUMANIZE_MAIL, text)

    last_err: Optional[str] = None
    start_time = time.time()

    async with httpx.AsyncClient(timeout=_TIMEOUT_S, headers={"User-Agent": "Mozilla/5.0", "Accept": "application/json"}) as client:
        for hdr in _header_variants(HUMANIZE_API_KEY):
            try:
                r = await client.post(HUMANIZE_REWRITE_URL, headers=hdr, json=payload)
                # Try to parse JSON; if not JSON, surface an HTTP error
                try:
                    data = r.json()
                except json.JSONDecodeError:
                    r.raise_for_status()
                    raise HTTPException(status_code=502, detail="Humanize API returned invalid JSON.")

                code = int(data.get("code", r.status_code))
                msg = str(data.get("msg") or "")
                if code != 200:
                    # Common: 1003 Invalid API Key
                    last_err = f"Humanize error ({code}): {msg or 'unknown'}; hdr={list(hdr.keys())[0]}"
                    # try next header variant
                    continue

                rewritten_raw = data.get("data") or ""
                if not isinstance(rewritten_raw, str):
                    rewritten_raw = str(rewritten_raw)

                latency = round(time.time() - start_time, 2)
                log_event(
                    "superhuman_rewrite_http_success",
                    {"mode": mode, "tone": tone, "chars": len(text), "latency_s": latency, "hdr": list(hdr.keys())[0]},
                )
                return rewritten_raw

            except Exception as e:
                last_err = f"{type(e).__name__}: {e}; hdr={list(hdr.keys())[0]}"
                # try next header variant

    # If we got here, all header variants failed
    raise HTTPException(status_code=502, detail=last_err or "Humanize call failed.")


async def rewrite_single(
    text: str,
    mode: str,
    tone: str,
    constraints: dict,  # reserved for future guarding/filtering
    latex_safe: bool,
    max_len: int,
) -> str:
    """Rewrite a single block of text using the AIHumanize API or (optional) fallback."""
    t_in = (text or "").strip()[: max_len]
    if not t_in:
        return ""

    # Prefer API if creds present; otherwise use optional fallback
    if HUMANIZE_API_KEY and HUMANIZE_MAIL:
        try:
            rewritten_raw = await _call_humanize(t_in, tone, mode)
            return _clean_text(rewritten_raw, latex_safe, mode)
        except HTTPException as e:
            log_event("âš ï¸ superhuman_rewrite_error", {"error": str(e)})
            if _LOCAL_ENABLED:
                log_event("âš™ï¸ using_local_rewrite_fallback", {"text_len": len(t_in)})
                return _clean_text(_local_rewrite_stub(t_in, tone, mode), latex_safe, mode)
            raise
    else:
        if _LOCAL_ENABLED:
            log_event("âš™ï¸ using_local_rewrite_fallback", {"reason": "missing_credentials", "text_len": len(t_in)})
            return _clean_text(_local_rewrite_stub(t_in, tone, mode), latex_safe, mode)
        raise HTTPException(status_code=503, detail="Humanize credentials missing and local fallback disabled.")


# ============================================================
# ðŸš€ Main Endpoint
# ============================================================

@router.post("/rewrite")
async def rewrite_text(req: RewriteRequest):
    """
    SuperHuman rewrite engine â€” transforms one or more text inputs via AIHumanize API.
    Fails fast if Humanize is disabled or credentials are missing (unless explicit local fallback is enabled).
    """
    _require_api_creds()

    if not req.text:
        raise HTTPException(status_code=400, detail="No text provided.")

    items = req.text if isinstance(req.text, list) else [req.text]
    if len(items) > _MAX_ITEMS:
        raise HTTPException(status_code=413, detail=f"Too many items (max {_MAX_ITEMS}).")

    sem = asyncio.Semaphore(_CONCURRENCY)

    async def _bounded_rewrite(t: str):
        async with sem:
            return await rewrite_single(t, req.mode, req.tone, req.constraints, req.latex_safe, req.max_len)

    results = await asyncio.gather(*[_bounded_rewrite(t) for t in items], return_exceptions=True)

    # Propagate first error if any
    for r in results:
        if isinstance(r, Exception):
            raise r

    log_event(
        "superhuman_batch_complete",
        {
            "count": len(items),
            "mode": req.mode,
            "tone": req.tone,
            "latex_safe": req.latex_safe,
            "creds_present": bool(HUMANIZE_API_KEY and HUMANIZE_MAIL),
            "local_enabled": _LOCAL_ENABLED,
        },
    )

    return {"rewritten": results if isinstance(req.text, list) else results[0]}


# ============================================================
# ðŸ’° Balance Check Endpoint
# ============================================================

@router.get("/balance")
async def check_balance():
    """Return remaining words balance from AIHumanize API (or local indicator)."""
    if not (HUMANIZE_API_KEY and HUMANIZE_MAIL):
        if _LOCAL_ENABLED:
            return {"remaining_words": "âˆž (local mode)"}
        raise HTTPException(status_code=503, detail="Humanize API credentials missing.")

    payload = {"mail": HUMANIZE_MAIL, "email": HUMANIZE_MAIL, "token": HUMANIZE_API_KEY}

    async with httpx.AsyncClient(timeout=_TIMEOUT_S, headers={"User-Agent": "Mozilla/5.0", "Accept": "application/json"}) as client:
        last_err: Optional[str] = None
        for hdr in _header_variants(HUMANIZE_API_KEY):
            try:
                r = await client.post(HUMANIZE_BALANCE_URL, headers=hdr, json=payload)
                try:
                    data = r.json()
                except json.JSONDecodeError:
                    r.raise_for_status()
                    raise HTTPException(status_code=502, detail="Humanize balance returned invalid JSON.")

                code = int(data.get("code", r.status_code))
                if code != 200:
                    last_err = f"Humanize balance error ({code}): {data.get('msg') or 'unknown'}; hdr={list(hdr.keys())[0]}"
                    continue

                remaining = data.get("data") or 0
                try:
                    remaining_int = int(remaining)
                except Exception:
                    remaining_int = remaining
                return {"remaining_words": remaining_int}
            except Exception as e:
                last_err = f"{type(e).__name__}: {e}; hdr={list(hdr.keys())[0]}"

    log_event("âš ï¸ superhuman_balance_error", {"error": last_err or "unknown"})
    if _LOCAL_ENABLED:
        return {"remaining_words": "unknown (local fallback)"}
    raise HTTPException(status_code=502, detail=last_err or "Balance check failed.")


# ============================================================
# ðŸ”Ž Health / Debug
# ============================================================

@router.get("/health")
async def health():
    """Simple health check for the superhuman service."""
    return {
        "ok": True,
        "default_on": _HUMANIZE_DEFAULT_ON,
        "local_fallback": _LOCAL_ENABLED,
        "has_api_key": bool(HUMANIZE_API_KEY),
        "has_mail": bool(HUMANIZE_MAIL),
        "endpoint_rewrite": HUMANIZE_REWRITE_URL,
        "endpoint_balance": HUMANIZE_BALANCE_URL,
        "mode_default": _HUMANIZE_MODE_DEFAULT,
    }
-e 


